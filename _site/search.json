[
  {
    "objectID": "wk5.html",
    "href": "wk5.html",
    "title": "Possion regression",
    "section": "",
    "text": "Reading - Ch 4.1,4.2.1 - 4.2.3,5.2 - Skim sections 5.1, 5.2.3,5.3,.5.4"
  },
  {
    "objectID": "wk5.html#class-announcements",
    "href": "wk5.html#class-announcements",
    "title": "Possion regression",
    "section": "Class Announcements",
    "text": "Class Announcements\nNo HW this week\nLab-1 due in 1 week"
  },
  {
    "objectID": "wk5.html#roadmap",
    "href": "wk5.html#roadmap",
    "title": "Possion regression",
    "section": "Roadmap",
    "text": "Roadmap\nRearview Mirror\n\nModel unordered and ordered categorical response\n\nToday\n\nPoisson probability model\nPoisson regression model, estimation, and statistical inference\nModel Comparison Criteria, Model Assessment, Goodness of Fit\n\nLooking Ahead\n\nUnivariate and multivariate time-series\nNotion of dependency and stationarity"
  },
  {
    "objectID": "wk5.html#parameter",
    "href": "wk5.html#parameter",
    "title": "Possion regression",
    "section": "Parameter",
    "text": "Parameter\nEqual Mean and Variance and Overdispersion\nThis assumption of equal mean and variance is often not met when actually fitting to data, which results in what is known as overdispersion where the variance in the data is larger than the variance fit in the model.\nThis usually can be remedied by adding more X variables into the model to improve the fit.\nAnother option is to fit what are known as a quasi poisson model or negative binomial regression model. In both cases, we relax the equal mean and variance assumption by adding an additional parameter to the variance of the response variable, allowing it to be larger than the mean.\nIn quasi poisson regression we set $\n\\text{V}[\\hat{y_i}]=\\theta\\lambda_i and in negative binomial regression we set\n\\text{V}[\\hat{y_i}]=\\lambda_i+\\kappa\\lambda_i^2"
  },
  {
    "objectID": "wk5.html#target",
    "href": "wk5.html#target",
    "title": "Possion regression",
    "section": "Target",
    "text": "Target\n\nThe number of awards earned by students based on the type of programs students were enrolled in using historical admission data.\nnum_awards: the number of awards earned by students at a high school in a year\nmath: students’ scores on their final math exam\nprog: the type of program in which the students were enrolled\n\n1 = General\n2 = Academic\n3 = Vocational"
  },
  {
    "objectID": "wk5.html#eda",
    "href": "wk5.html#eda",
    "title": "Possion regression",
    "section": "EDA",
    "text": "EDA\n\nWhat is the number of observations?\nWhat is the number of variables?\nAre there any redundant variables?\nAre there any missing information?\nAre there any duplicated records?\nAre there any values in each of the variables that seem unreasonable?\n\n\n\n\nModeling the Number of Awards\n\n\nX\nid\nnum_awards\nprog\nmath\n\n\n\n\n1\n45\n0\nVocational\n41\n\n\n2\n108\n0\nGeneral\n41\n\n\n3\n15\n0\nVocational\n44\n\n\n4\n67\n0\nVocational\n42\n\n\n5\n153\n0\nVocational\n40\n\n\n6\n51\n0\nGeneral\n42\n\n\n\n\n\n\n\n         X         id num_awards       prog       math \n         0          0          0          0          0 \n\n\n\n\nUnivariate Analysis\n\nUse a frequency table and a bar plot to explore the distribution of the response variable(num_awards). What do you learn?\n\n\n\n\nModeling the Number of Awards\n\n\nNumber of awards\nN\nProportion\n\n\n\n\n0\n124\n0.62\n\n\n1\n49\n0.24\n\n\n2\n13\n0.06\n\n\n3\n9\n0.04\n\n\n4\n2\n0.01\n\n\n5\n2\n0.01\n\n\n6\n1\n0.00\n\n\n\n\n\n\n\nWarning: The dot-dot notation (`..prop..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(prop)` instead.\n\n\n\n\n\n\n\n\n\n\nThe prog is the committee’s key explanatory variable of interest. It has three levels: academic, general, and vocational. Use a frequency table and a bar plot to examine its distribution. What do you discover?\n\n\n\n\nModeling the Number of Awards\n\n\nType of program\nN\nProportion\n\n\n\n\nAcademic\n105\n0.52\n\n\nGeneral\n45\n0.22\n\n\nVocational\n50\n0.25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot the distribution of math scores. What are the range and average math scores?\n\n\n\n\n\n\n\n\n\n\n\n\nBivariate Analysis\n\nExamine the associations between the number of awards and program and math scores.\nThe graph below shows the distribution of the number of awards by program types. How are awards distributed among different programs?\n\n\n\n\n\n\n\n\n\n\n\nThe graph below shows the distribution of the number of awards and students’ math scores. Is there any clear relationship between them?\n\n\n\n\n\n\n\n\n\n\n\nUse summary_factorlist() function from the finalfit package to tabulate data. What do you learn from the EDA?\n\n\n\n\nModeling the Number of Awards\n\n\nDependent: num_awards\n\nunit\nvalue\n\n\n\n\nprog\nAcademic\nMean (sd)\n1.0 (1.3)\n\n\n\nGeneral\nMean (sd)\n0.2 (0.4)\n\n\n\nVocational\nMean (sd)\n0.2 (0.5)\n\n\nmath\n[33.0,75.0]\nMean (sd)\n0.6 (1.1)"
  },
  {
    "objectID": "wk5.html#coefficients",
    "href": "wk5.html#coefficients",
    "title": "Possion regression",
    "section": "Coefficients",
    "text": "Coefficients\n\npoisson.mod.1 &lt;- poisson.mod.1 &lt;- glm(num_awards ~ prog + math, \n                                      data = df, \n                                      family = poisson)\n\nsummary(poisson.mod.1)\n\n\nCall:\nglm(formula = num_awards ~ prog + math, family = poisson, data = df)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.2043  -0.8436  -0.5106   0.2558   2.6796  \n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    -4.16327    0.66288  -6.281 3.37e-10 ***\nprogGeneral    -1.08386    0.35825  -3.025  0.00248 ** \nprogVocational -0.71405    0.32001  -2.231  0.02566 *  \nmath            0.07015    0.01060   6.619 3.63e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 287.67  on 199  degrees of freedom\nResidual deviance: 189.45  on 196  degrees of freedom\nAIC: 373.5\n\nNumber of Fisher Scoring iterations: 6\n\n\n\nThe negative coefficients of the general and vocational programs indicate that the number of wards is lower in these two programs.\nThe positive coefficients of math score indicates that the number of awards is increasing as the math score increases.\n\n\n(exp(coef(poisson.mod.1)) - 1) * 100\n\n   (Intercept)    progGeneral progVocational           math \n    -98.444332     -66.171250     -51.034289       7.267164 \n\n\nTo have a more convenient way to interpret these coefficients, we compute and use percentage changes - Holding program type constant, 1 unit increase in math score increase the mean number of awards by 7%.\n\nHold math score constant;\n\na student in the general program, on average, receives 66% fewer awards than students in the academic program\na student in the vocational program, on average, receives 51% fewer awards than students in the academic program\n\n\n\nAnova()\n\nTest the overall effect of prog using Anova():\n\n\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: num_awards\n     LR Chisq Df Pr(&gt;Chisq)    \nprog   14.572  2  0.0006852 ***\nmath   45.010  1   1.96e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nBased on the p-values, the prog, taken together, is a statistically significant predictor of the number of awards."
  },
  {
    "objectID": "wk5.html#plot-the-result",
    "href": "wk5.html#plot-the-result",
    "title": "Possion regression",
    "section": "Plot the result",
    "text": "Plot the result\n\nPlot the fitted values across the three programs and discuss how the number of awards is associated with math scores.\n\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\nThis graph indicates that the most awards are earned by students in the academic program, especially if the student has a high math score.\nStudents in the general program earn the lowest number of awards"
  },
  {
    "objectID": "wk5.html#ci-of-cofficients",
    "href": "wk5.html#ci-of-cofficients",
    "title": "Possion regression",
    "section": "CI of cofficients",
    "text": "CI of cofficients\n\nConstruct and interpret the confidence intervals for each variable using confint().\n\n\n\nWaiting for profiling to be done...\n\n\n\nbeta interval\n\n\n\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n-5.4857279\n-2.8857230\n\n\nprogGeneral\n-1.8558691\n-0.4350439\n\n\nprogVocational\n-1.3905775\n-0.1261624\n\n\nmath\n0.0494967\n0.0910770\n\n\n\n\n\n\n\n\nbeta interval\n\n\n\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n-99.585448\n-94.418558\n\n\nprogGeneral\n-84.368296\n-35.276377\n\n\nprogVocational\n-75.106849\n-11.852828\n\n\nmath\n5.074217\n9.535332\n\n\n\n\n\n\n\n\nat \\alpha = 0.05, Compared to the students in academic program, the mean number of awards that students in General program decreases by 35% to 84% holding the math score constant.\nWith 95% confidence, the mean number of awards decrease by 11% to 75% for the student in vocational program vs. student in academic programs, holding the math score constant.\n95% confidence interval of a the effect of 1-unit increase in math score on the mean number of award holding everything else constant is 5.07% to 9.53% increase"
  },
  {
    "objectID": "wk5.html#aic",
    "href": "wk5.html#aic",
    "title": "Possion regression",
    "section": "AIC",
    "text": "AIC\nAIC = IC(2) = -2log(L(\\hat{\\beta}|y_1,.....,y_n))+2r"
  },
  {
    "objectID": "wk5.html#aic_c",
    "href": "wk5.html#aic_c",
    "title": "Possion regression",
    "section": "AIC_c",
    "text": "AIC_c\nAIC_c = IC(\\frac{2n}{n-r-1}) = -2log(L(\\hat{\\beta}|y_1,.....,y_n))+r\\frac{2n}{n-r-1}=AIC+\\frac{2r(r+1)}{n-r-1}"
  },
  {
    "objectID": "wk5.html#bic",
    "href": "wk5.html#bic",
    "title": "Possion regression",
    "section": "BIC",
    "text": "BIC\nBIC = IC(log(n)) = -2log(L(\\hat{\\beta}|y_1,.....,y_n))+rlog(n)"
  },
  {
    "objectID": "wk5.html#example",
    "href": "wk5.html#example",
    "title": "Possion regression",
    "section": "Example",
    "text": "Example\n\nCompute these three information criteria for the following three models and then rank the models based on each criterion using AIC(), BIC(), and AICc().\n\n\\text{mod.1:   }\\log(mean\\_num\\_awards) = \\beta_0 + \\beta_1 prog + u   \\text{mod.2:   }\\log(mean\\_num\\_awards) = \\beta_0 + \\beta_1 math + u\n\\text{mod.3:   }\\log(mean\\_num\\_awards) = \\beta_0 + \\beta_1 prog + \\beta_2 math + u\n\n\n     mod.1    mod.2    mod.3\n1 384.0762 416.5149 373.5045\n\n\n     mod.1    mod.2    mod.3\n1 384.1371 416.6373 373.7096\n\n\n     mod.1    mod.2    mod.3\n1 390.6728 426.4098 386.6978\n\n\n\nThe model with the lowest AIC, corrected AIC, or BIC score is preferred.\n\nThe absolute values of these scores do not matter.\n\nThese scores can be negative or positive.\n\nBased on all these three criteria, the third model with both math and program is the best, and the second model with the only program is the worst model"
  },
  {
    "objectID": "wk5.html#the-pearson-statistics",
    "href": "wk5.html#the-pearson-statistics",
    "title": "Possion regression",
    "section": "The Pearson Statistics",
    "text": "The Pearson Statistics\n\\chi^2=\\sum_{i=1}^{n}\\frac{(y_{i}-\\exp\\{\\textbf{X}_{i}\\hat{\\beta}\\})^{2}}{\\exp\\{\\textbf{X}_{i}\\hat{\\beta}\\}}\n\n# Calculate Pearson statistic residuals\npearson_stat &lt;- sum(residuals(poisson.mod.1, type = \"pearson\")^2)\npearson_stat\n\n[1] 212.1437\n\n# Get p value associated with the pearson statistic\npearson_p.value &lt;- pchisq(pearson_stat, poisson.mod.1$df.residual,\nlower.tail = FALSE)\npearson_p.value\n\n[1] 0.203986"
  },
  {
    "objectID": "wk5.html#residual-deviance",
    "href": "wk5.html#residual-deviance",
    "title": "Possion regression",
    "section": "Residual Deviance",
    "text": "Residual Deviance\nD=2\\sum_{i=1}^{n}\\biggl[y_{i}\\log\\biggl(\\frac{y_{i}}{\\exp\\{\\textbf{X}_{i}\\hat{\\beta}\\}}\\biggr)-(y_{i}-\\exp\\{\\textbf{X}_{i}\\hat{\\beta}\\})\\biggr]\n\n\n[1] 0.6182274\n\n\n\nGoodness-of-fit statistics test is a more objective measure of the overall fit.\nThe null hypothesis is that the model is correct against the alternative that it is not.\nWe can use both Pearson statistic or the residual deviance to perform this test\nHere, the two non significant p-values indicate that we fail to reject the null hypothesis that the model is correct"
  },
  {
    "objectID": "wk5.html#reminders",
    "href": "wk5.html#reminders",
    "title": "Possion regression",
    "section": "Reminders",
    "text": "Reminders\n\nBefore next live session:\n\nComplete and turn in the Lab-1\nComplete all videos and reading for unit 6"
  },
  {
    "objectID": "wk4P2.html",
    "href": "wk4P2.html",
    "title": "Nominol response regression model",
    "section": "",
    "text": "Reading\n\nCh.3.3"
  },
  {
    "objectID": "wk4P2.html#functional-form",
    "href": "wk4P2.html#functional-form",
    "title": "Nominol response regression model",
    "section": "Functional form",
    "text": "Functional form\n\\text{log}(\\frac{\\text{probility of getting some color}}{\\text{probability of getting blue}}) = \\beta_{\\text{some color }0} + \\beta_{\\text{some color }1}x_1 .. + \\beta_{\\text{some color }p}x_p\n\\text{log}(\\frac{\\pi_j}{\\pi_1}) = \\beta_{j0} + \\beta_{j1}x_1 .. + \\beta_{jp}x_p\n\nusing the multinomial regression model, we can estimate \\pi_1..\\pi_J based on sample, but finding their confidence interval is hard such that we only focus on Wald Confidence interval here."
  },
  {
    "objectID": "wk4P2.html#fit-the-model-and-evaluate",
    "href": "wk4P2.html#fit-the-model-and-evaluate",
    "title": "Nominol response regression model",
    "section": "Fit the model and evaluate",
    "text": "Fit the model and evaluate\n\nEDA\n\nwheat &lt;- read.csv(here(\"data\",\"Wheat.csv\"), stringsAsFactors = TRUE)\n\nhrw &lt;- wheat %&gt;% filter(class == \"hrw\")\n\np &lt;- ggparcoord(data = hrw, \n                columns = c(2:6),\n                groupColumn = 7,\n                showPoints = TRUE, \n                title = \"Parallel Coordinate Plot for the Iris Data\",\n                alphaLines = 0.9)\n\nggplotly(p)\n\n\n\n\n\n\n\nFit the model\n\n#the first is the baseline\nlevels(wheat$type)\n\n[1] \"Healthy\" \"Scab\"    \"Sprout\" \n\n#library(package = nnet)\nmod.fit &lt;- multinom(formula = type ~ class + density + hardness\n+ size + weight + moisture , data = wheat)\n\n# weights:  24 (14 variable)\ninitial  value 302.118379 \niter  10 value 234.991271\niter  20 value 192.127549\nfinal  value 192.112352 \nconverged\n\n\n\n\nEvaluation\n\n\nCall:\nmultinom(formula = type ~ class + density + hardness + size + \n    weight + moisture, data = wheat)\n\nCoefficients:\n       (Intercept)   classsrw   density    hardness      size     weight\nScab      30.54650 -0.6481277 -21.59715 -0.01590741 1.0691139 -0.2896482\nSprout    19.16857 -0.2247384 -15.11667 -0.02102047 0.8756135 -0.0473169\n          moisture\nScab    0.10956505\nSprout -0.04299695\n\nStd. Errors:\n       (Intercept)  classsrw  density    hardness      size     weight\nScab      4.289865 0.6630948 3.116174 0.010274587 0.7722862 0.06170252\nSprout    3.767214 0.5009199 2.764306 0.008105748 0.5409317 0.03697493\n        moisture\nScab   0.1548407\nSprout 0.1127188\n\nResidual Deviance: 384.2247 \nAIC: 412.2247 \n\n\n\nThe first parameter class has two values hrw and srw\nThis model output is hard to see.\n\n\n# library(equatiomatic)\n# model &lt;- lm(formula = type ~ class + density + hardness\n# + size + weight + moisture , data = wheat)\n# latex_equation &lt;- extract_eq(model)\n# print(latex_equation)\n\n\nThe estimated coefficient for classrw, -0.648 is part of the following equation\n\n\n\\operatorname{log\\frac{\\hat{\\pi}_{scab}}{\\hat{\\pi}_{Healthy}}} = 30.55 -0.65\\cdot(\\operatorname{class}_{\\operatorname{srw}}) + \\beta_{2}(\\operatorname{density}) + \\beta_{3}(\\operatorname{hardness}) + \\beta_{4}(\\operatorname{size}) + \\beta_{5}(\\operatorname{weight}) + \\beta_{6}(\\operatorname{moisture}) + \\hat{\\epsilon}\n\n\ntest statistic is -0.978 and its corresponding p-value is 0.32 - there is not sufficient evidence that hard and soft red winter wheat have different effects on the scab or healthy status of the kernels given the other explanatory variables are in the model\nLet’s print the output again.\n\n\n\n\nScab\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nz\np\nResponse\n\n\n\n\n(Intercept)\n30.5464951\n4.2898646\n0.95\n22.1385150\n38.9544752\n7.1206199\n0.0000000\nScab\n\n\nclasssrw\n-0.6481277\n0.6630948\n0.95\n-1.9477696\n0.6515141\n-0.9774285\n0.3283570\nScab\n\n\ndensity\n-21.5971549\n3.1161741\n0.95\n-27.7047438\n-15.4895660\n-6.9306639\n0.0000000\nScab\n\n\nhardness\n-0.0159074\n0.0102746\n0.95\n-0.0360452\n0.0042304\n-1.5482286\n0.1215673\nScab\n\n\nsize\n1.0691139\n0.7722862\n0.95\n-0.4445393\n2.5827670\n1.3843493\n0.1662515\nScab\n\n\nweight\n-0.2896482\n0.0617025\n0.95\n-0.4105829\n-0.1687135\n-4.6942690\n0.0000027\nScab\n\n\nmoisture\n0.1095650\n0.1548407\n0.95\n-0.1939172\n0.4130473\n0.7075983\n0.4791947\nScab\n\n\n\n\n\n\n\n\nSprout\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nz\np\nResponse\n\n\n\n\n8\n(Intercept)\n19.1685687\n3.7672143\n0.95\n11.7849643\n26.5521732\n5.0882607\n0.0000004\nSprout\n\n\n9\nclasssrw\n-0.2247384\n0.5009199\n0.95\n-1.2065233\n0.7570465\n-0.4486513\n0.6536832\nSprout\n\n\n10\ndensity\n-15.1166714\n2.7643059\n0.95\n-20.5346114\n-9.6987314\n-5.4685234\n0.0000000\nSprout\n\n\n11\nhardness\n-0.0210205\n0.0081057\n0.95\n-0.0369074\n-0.0051335\n-2.5932793\n0.0095066\nSprout\n\n\n12\nsize\n0.8756135\n0.5409317\n0.95\n-0.1845931\n1.9358201\n1.6187138\n0.1055089\nSprout\n\n\n13\nweight\n-0.0473169\n0.0369749\n0.95\n-0.1197864\n0.0251526\n-1.2797020\n0.2006500\nSprout\n\n\n14\nmoisture\n-0.0429970\n0.1127188\n0.95\n-0.2639218\n0.1779279\n-0.3814532\n0.7028670\nSprout\n\n\n\n\n\n\n\n\n\nDoes anything have any predictive power?\n\nSo many numbers to keep track of…\n\n\n\n\n\n\nH_0: \\beta_{21} = \\beta_{31} = 0 H_0: \\beta_{21} \\not = 0 \\text{ and/or } \\beta_{31} \\not= 0\n\n\nAnova()\nAnova() test performs LR test. - LR Chisq is the transformed test statistic related to class variable,-2\\text{log}(\\lambda)=0.964 - Because of the large p-value, there is not sufficient evidence to indicate that the class of wheat is important given that the other variables are in the model.\n\nAnova(mod.fit) %&gt;% kable(\"html\", caption = \"Anova()\") %&gt;% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\nAnova()\n\n\n\nLR Chisq\nDf\nPr(&gt;Chisq)\n\n\n\n\nclass\n0.9641189\n2\n0.6175104\n\n\ndensity\n90.5551631\n2\n0.0000000\n\n\nhardness\n7.0736934\n2\n0.0291050\n\n\nsize\n3.2106326\n2\n0.2008260\n\n\nweight\n28.2302354\n2\n0.0000007\n\n\nmoisture\n1.1933402\n2\n0.5506422\n\n\n\n\n\n\n\n\nSeparate tests for density, hardness, and weight in the output all indicate at least marginal evidence of importance for these explanatory variables.\n\n\npi.hat &lt;- predict(object = mod.fit , newdata = wheat , type =\n\"probs\")\n\npi.hat %&gt;% head() %&gt;% kable(\"html\", caption = \"pi.hat\") %&gt;% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\npi.hat\n\n\nHealthy\nScab\nSprout\n\n\n\n\n0.8552110\n0.0463968\n0.0983922\n\n\n0.7492553\n0.0215722\n0.2291726\n\n\n0.5172800\n0.0689799\n0.4137401\n\n\n0.8982064\n0.0067407\n0.0950529\n\n\n0.5103245\n0.1762608\n0.3134147\n\n\n0.7924907\n0.0153041\n0.1922052\n\n\n\n\n\n\n\n\nCheck accuracy\n\n\n\n[1] 0.72\n\n\n\n\nChanges in probability\n\nWith respect to changes in density when the function has only density as its parameter\n\n\n\n# weights:  9 (4 variable)\ninitial  value 302.118379 \niter  10 value 229.769334\niter  20 value 229.712304\nfinal  value 229.712290 \nconverged"
  },
  {
    "objectID": "wk4P2.html#odds-ratios",
    "href": "wk4P2.html#odds-ratios",
    "title": "Nominol response regression model",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nsee page 160\nThe log-odds are modeled directly in a multinomial regression model, odds ratios are useful for interpreting an explanatory variable’s relationship with the response\nOdds ratios for numerical explanatory variables represent the change in odds correspodning to a c-unit increase in a particular explanatory variable.\nThe odds of category j response vs a category 1 response (the baseline) are \\text{exp}(\\beta_{j0}+\\beta_{j1}x_1 + ... + \\beta_{jp}x_p)\nThe odds in Wheat proble are constructed as\n\n\\frac{P(Y=j)}{P(Y=1)}\nwhere j = 2 (scab) and 3 (sprout), and 1 (healthy)\nand we had 6 features (see table below)\n\nmod.fit &lt;- multinom(formula = type ~ class + density + hardness\n+ size + weight + moisture , data = wheat)\n\n# weights:  24 (14 variable)\ninitial  value 302.118379 \niter  10 value 234.991271\niter  20 value 192.127549\nfinal  value 192.112352 \nconverged\n\ncoefficients(mod.fit) %&gt;% kable(\"html\", caption = \"mod.fit\") %&gt;% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\nmod.fit\n\n\n\n(Intercept)\nclasssrw\ndensity\nhardness\nsize\nweight\nmoisture\n\n\n\n\nScab\n30.54650\n-0.6481277\n-21.59715\n-0.0159074\n1.0691139\n-0.2896482\n0.109565\n\n\nSprout\n19.16857\n-0.2247384\n-15.11667\n-0.0210205\n0.8756135\n-0.0473169\n-0.042997\n\n\n\n\n\n\n\n\nchanges in OR\n\nwhen you change the value of the feature by 1 sd\n\n\nsd.wheat &lt;- apply(X = wheat[,-c(1,7,8)], MARGIN = 2, FUN = sd)\nsd.wheat\n\n   density   hardness       size     weight   moisture \n 0.1313021 27.3561563  0.4906125  7.9154398  2.0332132 \n\nc.value &lt;- c(1, sd.wheat)\nc.value\n\n              density   hardness       size     weight   moisture \n 1.0000000  0.1313021 27.3561563  0.4906125  7.9154398  2.0332132 \n\n\n\n\n          density hardness     size   weight moisture \n    0.52     0.06     0.65     1.69     0.10     1.25 \n\n\n          density hardness     size   weight moisture \n    1.91    17.04     1.55     0.59     9.90     0.80 \n\n\n\nThe estimated odds of a scab vs. a healthy kernel change by 0.06 times for a 0.13 increase in the density holding the other variables constant.\nEquivalently, we can say that the estimated odds of a scab vs. a healthy kernel change by 17.04 times for a 0.13 decrease in the density holding the other variables constant.\nThe estimated odds of a scab vs. healthy kernel change by 9.90 times for a 7.92 decrease in the weight holding the other variables constant.\n\n\n\n          density hardness     size   weight moisture \n    0.80     0.14     0.56     1.54     0.69     0.92 \n\n\n\nThe estimated odds of a sprout vs. ahealthy kernel change by 7.28 times for a 0.13 decrease in the density holding the other variables constant\nThe estimated odds of a sprout vs. healthy kernel change by 1.45 times for a 7.92 decrease in the weight holding the other variables constant.\nWe see that the larger the density and weight, the more likely a kernel is healthy."
  },
  {
    "objectID": "wk4P2.html#confidence-interval",
    "href": "wk4P2.html#confidence-interval",
    "title": "Nominol response regression model",
    "section": "Confidence interval",
    "text": "Confidence interval\n\nGet the Wald intervals for the odd ratios\n\n\n# Wald intervals\nconf.beta &lt;- confint(object = mod.fit , level = 0.95)\n\nconf.beta\n\n, , Scab\n\n                   2.5 %        97.5 %\n(Intercept)  22.13851497  38.954475222\nclasssrw     -1.94776958   0.651514098\ndensity     -27.70474380 -15.489565975\nhardness     -0.03604523   0.004230411\nsize         -0.44453927   2.582767006\nweight       -0.41058295  -0.168713512\nmoisture     -0.19391723   0.413047326\n\n, , Sprout\n\n                   2.5 %       97.5 %\n(Intercept)  11.78496433 26.552173165\nclasssrw     -1.20652328  0.757046542\ndensity     -20.53461137 -9.698731394\nhardness     -0.03690744 -0.005133494\nsize         -0.18459306  1.935820104\nweight       -0.11978643  0.025152642\nmoisture     -0.26392179  0.177927888"
  },
  {
    "objectID": "wk2.html",
    "href": "wk2.html",
    "title": "logit",
    "section": "",
    "text": "Reading Christopher R. Bilder and Thomas M. Loughin. Analysis of Categorical Data with R. CRC Press. 2015. - CH 2.1, 2.2.1-2.2.4 (page 61- 94) - Ch. 2.2.5 – 2.2.8, 2.3 (page 94)"
  },
  {
    "objectID": "wk2.html#link-function",
    "href": "wk2.html#link-function",
    "title": "logit",
    "section": "Link function",
    "text": "Link function\n\\text{logit}(\\pi_i)=\\text{log}(\\frac{\\pi_i}{1-\\pi_i}) = \\beta_0 + \\beta_1x_{i,1}+...++ \\beta_px_{i,p}\n\nUnfortunately, there are only a few simple cases where these parameter estimates have closed-form solutions; i.e., we cannot generally write out the parameter estimates in terms of the observed data like we could for the single probability estimate \\pi in Section 1.1.2.\nInstead, we use iterative numerical procedures, as described in Appendix B.3.2, to successively find estimates of the regression parameters that increase the log-likelihood function"
  },
  {
    "objectID": "wk2.html#mod.fit",
    "href": "wk2.html#mod.fit",
    "title": "logit",
    "section": "Mod.fit()",
    "text": "Mod.fit()\n\n\\log\\left[ \\frac { P( \\operatorname{good} = \\operatorname{1} ) }{ 1 - P( \\operatorname{good} = \\operatorname{1} ) } \\right] = 5.812 +- 0.115\\cdot(\\operatorname{distance})\n\n\nYou can get confidence interval this way\n\n\nconfint(mod.fit)\n\nWaiting for profiling to be done...\n\n\n                 2.5 %      97.5 %\n(Intercept)  5.1958841  6.47715198\ndistance    -0.1318144 -0.09907103\n\n\n\nalso this way\n\n\nsummary(mod.fit)\n\n\nCall:\nglm(formula = good ~ distance, family = binomial(link = logit), \n    data = placekick)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.7441   0.2425   0.2425   0.3801   1.6092  \n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  5.812080   0.326277   17.81   &lt;2e-16 ***\ndistance    -0.115027   0.008339  -13.79   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1013.43  on 1424  degrees of freedom\nResidual deviance:  775.75  on 1423  degrees of freedom\nAIC: 779.75\n\nNumber of Fisher Scoring iterations: 6\n\n\npage74\n\nHypothesis tests for regression parameters\n\n(see page 56)\nWald test, which involves calculating \\Z_0 and using standard normal distribution often suffer from bias.\nLRT typically performs better than the Wald Test\n\nH_0: \\text{logit}(\\pi) =\n\\log\\left[ \\frac { P( \\operatorname{good} = \\operatorname{1} ) }{ 1 - P( \\operatorname{good} = \\operatorname{1} ) } \\right] = \\alpha + \\beta_{1}\\cdot\\operatorname{distance}\n\nH_A: \\text{logit}(\\pi) =\n\\log\\left[ \\frac { P( \\operatorname{good} = \\operatorname{1} ) }{ 1 - P( \\operatorname{good} = \\operatorname{1} ) } \\right] = \\alpha + \\beta_{1}\\cdot\\operatorname{distance} + \\beta_2\\cdot\\text{another_feature}\n\n\n\nLRT\n\n\n\n\n\n\\Lambda = \\frac{\\text{ML under }H_0 }{\\text{ML under }H_A}\n\n\nTransformed LRT\n\n\n\n\n\n-2\\text{log}{(\\Lambda)} \\approx \\chi^2"
  },
  {
    "objectID": "wk2.html#saturated-model",
    "href": "wk2.html#saturated-model",
    "title": "logit",
    "section": "Saturated model",
    "text": "Saturated model\n\nsaturated model, which has a different coefficent for each data point, leading to perfect prediction, a likelihood of one, and a log likelihood of zero."
  },
  {
    "objectID": "wk2.html#null-deviance",
    "href": "wk2.html#null-deviance",
    "title": "logit",
    "section": "Null deviance",
    "text": "Null deviance\n\nThe null deviance measures the performance of the worst model using only an intercept, providing a benchmark.\n\n\n\\text{Null Deviance}= -2 \\text{log}(L(\\hat{\\beta_0}|y_1,..., y_n))"
  },
  {
    "objectID": "wk2.html#residual-deviance",
    "href": "wk2.html#residual-deviance",
    "title": "logit",
    "section": "Residual deviance",
    "text": "Residual deviance\n\nThe residual deviance is the deviance of our fitted model.\nIt is always greater than zero unless it is the saturated model which explains the data perfectly.\n\n\n\\text{Residual Deviance}= -2 \\text{log}(L(\\hat{\\beta}|y_1,..., y_n))\n\n\nTherefore, how much better (smaller) our residual deviance is compared to the null deviance and how close it is to zero is a measure of model fit.\nSometimes people will compute an R^2 for logistic regression using 1-\\frac{\\text{Residual Deviance}}{\\text{Null Deviance}} since it is bounded between 0 (residual deviance = null deviance) and 1 (residual deviance = saturated model = 0).\nNote that we can compute deviance of two separate models by substracting the null model residual deviance and the alternative model residual deviance from separate logistic regression fits.\nSee page 81 for more information\n\n\nmod.fit2 &lt;- glm(formula = good ~ change + distance , family =\nbinomial(link = logit), data = placekick)\n\n#one at a time test\nAnova(mod.fit2, test = \"LR\")\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: good\n         LR Chisq Df Pr(&gt;Chisq)    \nchange      5.246  1      0.022 *  \ndistance  218.650  1     &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mod.fit, mod.fit2, test = \"LR\")\n\nAnalysis of Deviance Table\n\nModel 1: good ~ distance\nModel 2: good ~ change + distance\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)  \n1      1423     775.75                       \n2      1422     770.50  1   5.2455    0.022 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "wk2.html#confidence-interval-of-the-pi-estimate",
    "href": "wk2.html#confidence-interval-of-the-pi-estimate",
    "title": "logit",
    "section": "confidence interval of the \\pi estimate",
    "text": "confidence interval of the \\pi estimate\n\nsee page 90\nWe can get the \\hat{\\pi}\n\n\npredict(mod.fit, newdata = new_data, type = \"response\", se = TRUE)\n\n$fit\n        1 \n0.9710145 \n\n$se.fit\n         1 \n0.00487676 \n\n$residual.scale\n[1] 1\n\n\n\n#create a function to get the confidence interval of pi\nci.pi &lt;- function(newdata , mod.fit.obj , alpha){\n  linear.pred &lt;- predict(object = mod.fit.obj , \n                         newdata =newdata , \n                         type = \"link\", se = TRUE)\n  \n  CI.lin.pred.lower &lt;- linear.pred$fit - qnorm(p =1-alpha/2)*linear.pred$se\n  CI.lin.pred.upper &lt;- linear.pred$fit + qnorm(p =1-alpha/2)*linear.pred$se\n  \n  #get pi\n  CI.pi.lower &lt;- exp(CI.lin.pred.lower) / (1 +exp(CI.lin.pred.lower))\n  CI.pi.upper &lt;- exp(CI.lin.pred.upper) / (1 +exp(CI.lin.pred.upper))\n\n  list(lower = CI.pi.lower , upper = CI.pi.upper)\n}\n\n\nci.pi(newdata = data.frame(distance = 20), mod.fit.obj = mod.fit , alpha = 0.05)\n\n$lower\n        1 \n0.9597647 \n\n$upper\n        1 \n0.9791871 \n\n\n\nx &lt;- 5:70\n\npredicted &lt;- predict(object = mod.fit , newdata =\ndata.frame(distance = x), type = \"response\")\n\nlower &lt;- ci.pi(newdata = data.frame(distance = x),\nmod.fit.obj = mod.fit , alpha = 0.05)$lower\n\nupper &lt;- ci.pi(newdata = data.frame(distance = x),\nmod.fit.obj = mod.fit , alpha = 0.05)$upper\n\ndf &lt;- as.data.frame(cbind(x,lower,upper,predicted)) %&gt;% pivot_longer(-x)\n\np &lt;- df %&gt;% ggplot(aes(x=x,y = value, color = name)) + geom_line()\n\nggplotly(p)"
  },
  {
    "objectID": "wk2.html#odd-ratio",
    "href": "wk2.html#odd-ratio",
    "title": "logit",
    "section": "ODD Ratio",
    "text": "ODD Ratio\n\\text{OR} = \\frac{\\text{Odds}_{x+c}}{\\text{Odds}_{x}}= \\text{exp}(c\\beta_1) -\n\nIncrease in odd is OR &gt; 1,\ndecrease in odd if OR &lt; 1,\n\n\\hat{\\text{OR}} = \\text{exp}(c\\hat{\\beta}_1) - So, it will have estimated variance. - As you have guessed by now, Wald CI does not work well when sample size small.\n\nuse transformed LRT statistic.\n\n\n#finds LR confidence interval\nmod.fit\n\n\nCall:  glm(formula = good ~ distance, family = binomial(link = logit), \n    data = placekick)\n\nCoefficients:\n(Intercept)     distance  \n      5.812       -0.115  \n\nDegrees of Freedom: 1424 Total (i.e. Null);  1423 Residual\nNull Deviance:      1013 \nResidual Deviance: 775.7    AIC: 779.7\n\nbeta.ci &lt;- confint(mod.fit, parm = \"distance\", level = 0.95)\n\nWaiting for profiling to be done...\n\nbeta.ci\n\n      2.5 %      97.5 % \n-0.13181435 -0.09907103 \n\n\n\nUnderstanding the changes in odd\n\n10-yard decrease in distance increases the odds of a successful placekick.\n\n\n\n  97.5 %    2.5 % \n2.693147 3.736478 \n\n\n\n\n[1] 2.682701 3.719946\n\n\n\nHas similar interval due to large sample size\n\n\n\nExample of magin coin\n\nYou can convert log odd to probability\nYour aunt offers a service in which she weights coins to make them unfair.\nYou give her a coin and tell her how much you want the log-odds to change.\n\nShe returns the modified coin.\n\nFor each of the following orders, use your function to compute the resulting probability of heads:\n\nfair coin, increase log-odds by 1.\nfair coin, increase log-odds by 2.\nfair coin, increase log-odds by 10.\nfair coin, decrease log-odds by 1.\nfair coin, decrease log-odds by 2.\nfair coin, decrease log-odds by 10.\n\nWrite an R function that computes the probability of heads, given log-odds.\n\n\n\n[1] 0.5\n\n\n\n\n\n\n\nlog_odds\nprobability\n\n\n\n\n10\n1.000\n\n\n2\n0.881\n\n\n1\n0.731\n\n\n0\n0.500\n\n\n-1\n0.269\n\n\n-2\n0.119\n\n\n-10\n0.000\n\n\n\n\n\n\n\n\nIn you own words, describe how changes in log-odds translate to changes in probability\n\n\n\n\n\n\n\n\n\n\n\nYou can see in this plot, As log-odds increase, the probability of success increases relative to the probability of failure, and it approaches one. As log-odds decrease probability of success decrease and converges to zero.\nIf you get log-odds values that are very very small like -10 the probability of success is almost zero, and if you get log-odds values that are very big like 10 or the probability of success is almost one.\nThe relationship between log-odd and probability is not linear, but of s-curve type, and log odds ratios ranging from -5 to +5 create probabilities that range from just above 0 to very close to 1."
  },
  {
    "objectID": "wk2.html#background",
    "href": "wk2.html#background",
    "title": "logit",
    "section": "Background",
    "text": "Background\n\nTarget: Probability of getting coronary heart disease, chd\nFeature:\n\nHigh blood pressure, high LDL cholesterol, diabetes, smoking, secondhand smoke exposure, obesity, an unhealthy diet, and physical inactivity are among the leading risk factors for heart disease.\n\nData Source: Source: Rousseauw, J., du Plessis, J., Benade, A., Jordaan, P., Kotze, J. and Ferreira, J. (1983). Coronary risk factor screening in three rural communities, South African Medical Journal 64: 430–436.\nData Description\n\nsbp: systolic blood pressure\ntobacco: cumulative tobacco use (kg)\nldl: low density lipoprotein cholesterol (‘bad’ cholestrol)\nadiposity: Body adiposity index determines body fat percentage(calculated as (HC / (HM)1.5) - 18, where HC = Hip Circumference in Centimetres and HM = Height in meters)\nfamhist: family history of heart disease\ntypea: A personality type that could raise one’s chances of developing coronary heart disease\nobesity: Body Mass Index (BMI) (kg/m^2)\nalcohol: current alcohol consumption\nage: age at onset\nchd: coronary heart disease (target)"
  },
  {
    "objectID": "wk2.html#task",
    "href": "wk2.html#task",
    "title": "logit",
    "section": "Task",
    "text": "Task\n\nUsing blood pressure, smoking, cholesterol,and age.\nLoad the data and answer the following questions:\n\nWhat are the number of variables and number of observations?\nWhat is the type of each variable? Do we need to change it?\nAre there any missing values (in each of the variables)?\nAre there any abnormal values in each of the variables in the     raw data?\n\n\n\n\n\n\n\ntobacco\nldl\nsbp\nage\nchd\nobesity\n\n\n\n\n12.00\n5.73\n160\n52\n1\n25.30\n\n\n0.01\n4.41\n144\n63\n1\n28.87\n\n\n0.08\n3.48\n118\n46\n0\n29.14\n\n\n7.50\n6.41\n170\n58\n1\n31.99\n\n\n13.60\n3.50\n134\n49\n1\n25.99\n\n\n6.20\n6.47\n132\n45\n0\n30.77"
  },
  {
    "objectID": "wk2.html#eda",
    "href": "wk2.html#eda",
    "title": "logit",
    "section": "EDA",
    "text": "EDA\n\nUnivariate analysis\n\nThe response (or dependent) variable of interest, Heart disease, is a binary variable taking the type factor.\nUse a bar chart to explore the distribution of the response variable (chd). What do you learn?\n\n\n\n\n\n\n\nHeart disease\nN\nProportion\n\n\n\n\n0\n302\n0.65\n\n\n1\n160\n0.35\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor metric variables, a density plot or histogram allows us to determine the shape of the distribution and look for outliers.\n\nUse a density plot to explore the distribution of explanatory variables. What do you discover?\n\n\n\n\n\n\n\n\n\n\n\nBivariate Analysis\n\nPrior to moving on to the fully specified model, it is advisable to first examine the simple associations between the response and each explanatory variable.\nBox plots are useful for exploring the association between a categorical variable and a variable measured on an interval scale.\nUse a boxplot to examine how the explanatory variables are correlated with the response variable (chd)?\n\nThe coord_flip() function is used to keep the dependent variable on the y-axis.\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse the convenient summary_factorlist() function from the finalfit package to tabulate data.\n\n\n\n\n\n\nDependent: chd\n\n0\n1\np\n\n\n\n\nldl\nMean (SD)\n4.3 (1.9)\n5.5 (2.2)\n&lt;0.001\n\n\nsbp\nMean (SD)\n135.5 (18.0)\n143.7 (23.7)\n&lt;0.001\n\n\ntobacco\nMean (SD)\n2.6 (3.6)\n5.5 (5.6)\n&lt;0.001\n\n\nage\nMean (SD)\n38.9 (14.9)\n50.3 (10.6)\n&lt;0.001\n\n\n\n\n\n\n\n\nAccording to the plots and the tables, What variable is most important for explaining heart disease? How is that variable correlated with heart disease?"
  },
  {
    "objectID": "wk2.html#mrl",
    "href": "wk2.html#mrl",
    "title": "logit",
    "section": "MRL",
    "text": "MRL\n\nIs the linear probability model an appropriate choice to study the relationship between heart disease and risk factors?\nEstimate the following linear probability model and interpret the model results.\n\n\\text{chd} = \\beta_0 + \\beta_1 \\cdot \\text{ldl} + \\beta_2 \\cdot \\text{sbp} + + \\beta_3 \\cdot \\text{tobacco} + \\beta_4 \\cdot \\text{age} + \\epsilon\n\n\\operatorname{\\hat{chd}} = -0.35 + 0.036\\cdot\\operatorname{ldl} + 0.00097\\cdot\\operatorname{sbp} + 0.0165 \\cdot\\operatorname{tobacco} + 0.007 \\cdot \\operatorname{age} + \\hat{\\epsilon}\n\n\n\n\nCall:\nlm(formula = chd ~ ldl + sbp + tobacco + age, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.8439 -0.3405 -0.1250  0.4365  1.0172 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.3493578  0.1405912  -2.485 0.013315 *  \nldl          0.0362419  0.0102322   3.542 0.000438 ***\nsbp          0.0009739  0.0010670   0.913 0.361839    \ntobacco      0.0165577  0.0049101   3.372 0.000809 ***\nage          0.0076831  0.0016886   4.550 6.89e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4318 on 457 degrees of freedom\nMultiple R-squared:  0.1853,    Adjusted R-squared:  0.1781 \nF-statistic: 25.98 on 4 and 457 DF,  p-value: &lt; 2.2e-16\n\n\n\nWhat are the advantages and disadvantages of the linear probability model?"
  },
  {
    "objectID": "wk2.html#glm",
    "href": "wk2.html#glm",
    "title": "logit",
    "section": "GLM",
    "text": "GLM\n\nGeneralized linear model\nEstimate the following logistic regression model and interpret the model results.\n\n \\text{logit}(\\pi_i) =\\beta_0 + \\beta_1 \\cdot \\text{ldl} + \\beta_2 \\cdot \\text{sbp} + + \\beta_3 \\cdot \\text{tobacco} + \\beta_4 \\cdot \\text{age} +  \\epsilon\n\n\\log\\left[ \\frac { P( \\operatorname{chd} = \\operatorname{1} ) }{ 1 - P( \\operatorname{chd} = \\operatorname{1} ) } \\right] = -4.54 + 0.018\\cdot \\operatorname{ldl} + 0.004\\cdot \\operatorname{sbp} + 0.075\\cdot \\operatorname{tobacco} + 0.04\\cdot \\operatorname{age}\n\n\nOdd ratio\n\nDo the raw coefficient estimates directionally make sense?\n\n\n\n\nCall:\nglm(formula = chd ~ ldl + sbp + tobacco + age, family = binomial(link = logit), \n    data = df)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.9457  -0.8595  -0.4999   1.0238   2.3906  \n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -4.535524   0.781360  -5.805 6.45e-09 ***\nldl          0.185131   0.054121   3.421 0.000625 ***\nsbp          0.004307   0.005394   0.798 0.424623    \ntobacco      0.075982   0.025616   2.966 0.003016 ** \nage          0.046264   0.009852   4.696 2.66e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 596.11  on 461  degrees of freedom\nResidual deviance: 502.19  on 457  degrees of freedom\nAIC: 512.19\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\nAgain, all of the explanatory variables except blood pressure are statistically significant and positively correlated with the probability of heart disease, same as the linear probability model.\n\nRecall that (page 83)\n\n\n\\text{OR} = \\frac{\\text{Odds}_{x_k+c}}{\\text{Odds}_{x_k}}=exp(c \\beta_k)\n\n\nThe odd of a success change by exp(c\\beta_k) times for every c-unit increase in x\nCompute and interpret the estimated odds ratio for a 10-unit increase in each explanatory variable.\n\n\n# Replace with your code\nround(exp(10*coef(mod.logit.h0)),2)\n\n(Intercept)         ldl         sbp     tobacco         age \n       0.00        6.37        1.04        2.14        1.59 \n\n\n\nThe estimated odds of success or having a heart disease change by 6.37 times for every 10-unit increase in LDL or ‘bad’ cholesterol.\n\n\nInterestingly, the odds of having a heart disease is almost 1 for every 10-unit increase in blood pressure, which means an increase in blood pressure doesn’t change the odds of having heart disease, and it’s consistent with its insignificant coefficient.\n\n\n\nHypothesis Test\n\nUsing the likelihood ratio test (LRT) for hypothesis testing is a common practice in a logistic regression model.\nUse LRT to test whether (obesity) is associated with heart disease.\n\nH_0: \\beta_{obesity} = 0\nH_a: \\beta_{obesity} \\ne 0\n\n\nUse both Anova() or anova() functions.\n\n#mod.logit.ha &lt;- # uncomment and replace with your code\nmod.logit.ha &lt;- glm(chd ~ ldl + sbp +tobacco + age + obesity, family = binomial(link = logit), data = df)\n\n#anova()\nanova(mod.logit.h0, mod.logit.ha, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: chd ~ ldl + sbp + tobacco + age\nModel 2: chd ~ ldl + sbp + tobacco + age + obesity\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)\n1       457     502.19                     \n2       456     501.07  1   1.1191   0.2901\n\n#Anova()\nAnova(mod.logit.ha, test = \"LR\")\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: chd\n        LR Chisq Df Pr(&gt;Chisq)    \nldl      13.3932  1  0.0002525 ***\nsbp       0.8640  1  0.3526279    \ntobacco   9.4670  1  0.0020920 ** \nage      24.3447  1  8.055e-07 ***\nobesity   1.1191  1  0.2901078    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\ndeviance refers to the amount that a particular model deviates from another model as measured by -2\\text{log}(\\Lambda).\nWhat are the null deviance and residual deviance in the model summary?\n\nFor null and residual deviance, the alternative model we use is the saturated model, which has a different coefficent for each data point, leading to perfect prediction, a likelihood of one, and a log likelihood of zero.\n\nThe null deviance measures the performance of the worst model using only an intercept, providing a benchmark.\n\n\n\\text{Null Deviance}= -2 \\text{log}(L(\\hat{\\beta_0}|y_1,..., y_n))\n\n\nThe residual deviance is the deviance of our fitted model.\n\nIt is always greater than zero unless it is the saturated model / explains the data perfectly.\n\n\n\n\\text{Residual Deviance}= -2 \\text{log}(L(\\hat{\\beta}|y_1,..., y_n))\n\n\nTherefore, how much better (smaller) our residual deviance is compared to the null deviance and how close it is to zero is a measure of model fit.\nSometimes people will compute an R^2 for logistic regression using 1-\\frac{\\text{Residual Deviance}}{\\text{Null Deviance}} since it is bounded between 0 (residual deviance = null deviance) and 1 (residual deviance = saturated model = 0).\nNote that we can compute deviance of two separate models by substracting the null model residual deviance and the alternative model residual deviance from separate logistic regression fits. (Why is this?)\nUsing deviance, test whether (obesity) is associated with heart disease.\n\nH_0: \\beta_{obesity} = 0\nH_a: \\beta_{obesity} \\ne 0\n\n\n\nWe get a p-value of 0.29, the same as what we got from both anova() and Anova() functions, and again we fail to reject the null hypothesis that obesity is not correlated with heart disease given this data set.\n\n\n\nConfidence Interval\n\nConfidence Interval for odds ratio\n\nWald Confidence:\n\nc*\\hat{\\beta_k} \\pm c*Z_{1-\\alpha/2} \\sqrt{\\widehat{Var}(\\hat{\\beta}_k)}\n\n\nexp \\left(c*\\hat{\\beta_k} \\pm c*Z_{1-\\alpha/2} \\sqrt{\\widehat{Var}(\\hat{\\beta}_k)} \\right)\n\n\nCalculate Wald CI for odds ratio of 10-unit increase in LDL cholesterol based on the above formula:\n\n\n\n             (Intercept)           ldl           sbp       tobacco\n(Intercept)  0.610523787 -9.955527e-03 -3.315082e-03  1.122271e-03\nldl         -0.009955527  2.929029e-03 -1.336470e-05 -4.923675e-06\nsbp         -0.003315082 -1.336470e-05  2.909849e-05 -1.506782e-06\ntobacco      0.001122271 -4.923675e-06 -1.506782e-06  6.562050e-04\nage         -0.001828353 -6.178475e-05 -1.503794e-05 -7.699037e-05\n                      age\n(Intercept) -1.828353e-03\nldl         -6.178475e-05\nsbp         -1.503794e-05\ntobacco     -7.699037e-05\nage          9.706566e-05\n\n\n[1]  2.20 18.39\n\n\n\nWith 95% confidence, the odds of having a heart disease change between 2.20 to 18.4 times for every 10-unit increase in LDL or ‘bad’ cholesterol.\n\n\nWhat is the main concern with Wald CI?\n\n\nWald confidence interval has a true confidence level close to the 95% only when we have large samples. When the sample size is not large, profile LR confidence intervals generally perform better.\n\n\nNow calculate the profile likelihood ratio (LR) confidence interval using the confint function.\n\n\n# Replace with your code\nbeta_ci &lt;- confint(mod.logit.h0)\n\nWaiting for profiling to be done...\n\nodds_ci &lt;- exp(10*beta_ci)\n\nround(cbind(odds_ci ),2)\n\n            2.5 % 97.5 %\n(Intercept)  0.00   0.00\nldl          2.24  18.84\nsbp          0.94   1.16\ntobacco      1.31   3.59\nage          1.31   1.93\n\n\n\nSince we have a large sample, 462 observations, the profile likelihood ratio (LR) confidence interval is pretty close to the Wald CI.\n\n\nConfidence Interval for the Probability of Success\nRecall that the estimated probability of success is \n\\hat{\\pi} = \\frac{exp \\left( \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\dots + \\hat{\\beta}_K x_k \\right)}{1+exp \\left(\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\dots + \\hat{\\beta}_K x_k \\right)}\n\n\nWhile backing out the estimated probability of success is straightforward, obtaining its confidence interval is not, as it involves many parameters.\nWald Confidence Interval\n\n\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\dots + \\hat{\\beta}_K x_K \\pm Z_{1-\\alpha/2} \\sqrt{\\widehat{Var}(\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\dots + \\hat{\\beta}_K x_K)}\n\nwhere\n\n\\widehat{Var}(\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\dots + \\hat{\\beta}_K x_K) = \\sum_{i=0}^K x_i^2 \\widehat{Var}(\\hat{\\beta_i}) + 2 \\sum_{i=0}^{K-1} \\sum_{j=i+1}^{K} x_i x_j \\widehat{Cov}(\\hat{\\beta}_i,\\hat{\\beta}_j)\n\nSo, the Wald Interval for \\pi is\n\n\\frac{exp \\left( \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\dots + \\hat{\\beta}_K x_k \\pm \\sqrt{\\sum_{i=0}^K x_i^2 \\widehat{Var}(\\hat{\\beta_i}) + 2 \\sum_{i=0}^{K-1} \\sum_{j=i+1}^{K} x_i x_j \\widehat{Cov}(\\hat{\\beta}_i,\\hat{\\beta}_j)}  \\right)}{1+exp \\left(\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\dots + \\hat{\\beta}_K x_k \\right)  \\pm \\sqrt{\\sum_{i=0}^K x_i^2 \\widehat{Var}(\\hat{\\beta_i}) + 2 \\sum_{i=0}^{K-1} \\sum_{j=i+1}^{K} x_i x_j \\widehat{Cov}(\\hat{\\beta}_i,\\hat{\\beta}_j)}}\n\n\nFor an average value of all explanatory variables, compute the Confidence Interval for the Probability of Success given the formula above\n\n\n\n  pi.hat  lower  upper\n1 0.3089 0.2925 0.3259"
  },
  {
    "objectID": "wk2.html#final-visualization",
    "href": "wk2.html#final-visualization",
    "title": "logit",
    "section": "Final Visualization",
    "text": "Final Visualization\n\nUsing both the linear probability and logistic regression models, plot the estimated probability of heart disease for different values of cholesterol, holding other variables constant at their average level.\nDiscuss which one can better explain this relationship."
  },
  {
    "objectID": "wk2.html#final-report",
    "href": "wk2.html#final-report",
    "title": "logit",
    "section": "Final Report",
    "text": "Final Report\n\nDisplay both estimated linear and logistic models in a regression table. Is there any significant difference between their results?\n\n\n\n\nTable 1: The estimated relationship between heart disease and risk factors\n==================================================\n                         Dependent variable:      \n                    ------------------------------\n                                 chd              \n                           OLS          logistic  \n                           (1)             (2)    \n--------------------------------------------------\nldl                      0.036***       0.185***  \n                         (0.010)         (0.054)  \n                                                  \nsbp                       0.001           0.004   \n                         (0.001)         (0.005)  \n                                                  \ntobacco                  0.017***        0.076**  \n                         (0.005)         (0.026)  \n                                                  \nage                      0.008***       0.046***  \n                         (0.002)         (0.010)  \n                                                  \nConstant                 -0.349*        -4.536*** \n                         (0.141)         (0.781)  \n                                                  \n--------------------------------------------------\nObservations               462             462    \nR2                        0.185                   \nAdjusted R2               0.178                   \nLog Likelihood                          -251.093  \nAkaike Inf. Crit.                        512.187  \nResidual Std. Error  0.432 (df = 457)             \n==================================================\nNote:                *p&lt;0.05; **p&lt;0.01; ***p&lt;0.001\n\n\n\nIn both models, all the coefficients except blood pressure are statistically significant and positively associated with the probability of having heart disease. Also, LDL is the most correlated variable with the probability of heart disease in both models."
  },
  {
    "objectID": "wk2.html#background-1",
    "href": "wk2.html#background-1",
    "title": "logit",
    "section": "Background",
    "text": "Background\n\nIn osteoporosis, bones become weak and brittle, so weak that even bending over or coughing can fracture them. Hip, wrist, and spine fractures are the most common osteoporosis-related fractures.\nAll races of people are at risk for osteoporosis.\nHowever, white and Asian women, particularly those that are post menopause, are at the greatest risk.\nA healthy diet, weight-bearing exercises, and medications can strengthen weak bones or prevent their loss. (Mayo Clinic)\n\nHere, Our goal is description of the data:\n\nHow factors such as age and weight are related to the fracture rates among older women?\n\nThis sample comes from the Global Longitudinal Study of Osteoporosis in Women (GLOW).\nThe data set includes information on 500 subjects enrolled in this study.\nInstall and load the aplore3 library to use the glow500 dataset and understand the structure dataset.\nWe summarize some of the variables that we will use:\n\nPRIORFRAC: History of prior fracture\nAGE: Age at enrollment\nWEIGHT: Weight at enrollment (Kilograms)\nHEIGHT: Height at enrollment (Centimeters)\nBMI: Body mass index (kg/m^2)\nPREMENO: Menopause before age 45\nFRACTURE: Any fracture in first year of follow up (target)\nRATERISK: Self-reported risk of fracture\nSMOKE: Former or current smoker"
  },
  {
    "objectID": "wk2.html#task-1",
    "href": "wk2.html#task-1",
    "title": "logit",
    "section": "Task",
    "text": "Task\n\nPredict the probability of have a fracture"
  },
  {
    "objectID": "wk2.html#eda-1",
    "href": "wk2.html#eda-1",
    "title": "logit",
    "section": "EDA",
    "text": "EDA\n\ndf &lt;- glow500 %&gt;%\n  dplyr::select(fracture, age, priorfrac, premeno, raterisk, smoke, bmi)\n\n\ndf %&gt;% count(fracture) %&gt;%\n  mutate(prop = round(prop.table(n),2)) %&gt;%\n  kable(format = \"html\",col.names = c('Fracture', 'N', \"Proportion\")) %&gt;% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\nFracture\nN\nProportion\n\n\n\n\nNo\n375\n0.75\n\n\nYes\n125\n0.25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge has a higher age in women with fractures than women without fractures. BMI distributions have almost the same mean and same variance in both groups with and without fracture, so probably BMI is not a useful variable to classify these two groups\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom these box plots, we can see the women who suffered from a fracture are older, but both groups have the same distribution of BMI.\nFrom the plots above, we see that the women with a history of prior fracture, and a high self-reported risk of fracture, have a higher probability of having a fracture in the first year of study. But, smokers and no smokers and women with or without menopause before 45 have the same probability of having a fracture. so smokers and menopause do not help classify these two groups, and we’re not going to use them for modeling\n\n\n\n\n\nDependent: fracture\n\nNo\nYes\n\n\n\n\nbmi\nMean (SD)\n27.5 (6.0)\n27.7 (5.9)\n\n\nage\nMean (SD)\n67.5 (8.7)\n71.8 (9.1)\n\n\npriorfrac\nNo\n301 (80.3)\n73 (58.4)\n\n\n\nYes\n74 (19.7)\n52 (41.6)\n\n\npremeno\nNo\n303 (80.8)\n100 (80.0)\n\n\n\nYes\n72 (19.2)\n25 (20.0)\n\n\nraterisk\nLess\n139 (37.1)\n28 (22.4)\n\n\n\nSame\n138 (36.8)\n48 (38.4)\n\n\n\nGreater\n98 (26.1)\n49 (39.2)\n\n\nsmoke\nNo\n347 (92.5)\n118 (94.4)\n\n\n\nYes\n28 (7.5)\n7 (5.6)"
  },
  {
    "objectID": "wk2.html#glm-1",
    "href": "wk2.html#glm-1",
    "title": "logit",
    "section": "GLM",
    "text": "GLM\n\nsimple model\n\n\n\nCall:\nglm(formula = fracture ~ bmi + age, family = binomial(link = logit), \n    data = df)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-1.21426  -0.77408  -0.62995  -0.07905   2.02854  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.83441    1.10792  -5.266 1.39e-07 ***\nbmi          0.02692    0.01817   1.482    0.138    \nage          0.05736    0.01211   4.735 2.20e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 562.34  on 499  degrees of freedom\nResidual deviance: 538.89  on 497  degrees of freedom\nAIC: 544.89\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\\log\\left[ \\frac { P( \\operatorname{fracture} = \\operatorname{Yes} ) }{ 1 - P( \\operatorname{fracture} = \\operatorname{Yes} ) } \\right] = \\alpha + \\beta_{1}\\cdot\\operatorname{bmi} + \\beta_{2}\\cdot \\operatorname{age}\n\n\nRecall:\n\n\nOR = \\frac{Odds_{x_k+c}}{Odds_{x_k}}=exp(c \\beta_k)\n\n\nFind and interpret the estimated odds ratios for a 10-unit increase in age.\n\n\n\n    [,1]\nage 1.77\n\n\nThe estimated odds of having a fracture change by 1.77 times for every 10-year increase in age, or it’s 77% higher\n\n\nmodel with categorical feature\n\n\n[1] \"No\"  \"Yes\"\n\n\n[1] \"Less\"    \"Same\"    \"Greater\"\n\n\n\n\\log\\left[ \\frac { P( \\operatorname{fracture} = \\operatorname{Yes} ) }{ 1 - P( \\operatorname{fracture} = \\operatorname{Yes} ) } \\right] = \\alpha + \\beta_{1}\\cdot \\operatorname{bmi} + \\beta_{2}\\cdot \\operatorname{age} + \\beta_{3}\\cdot \\operatorname{priorfrac}_{\\operatorname{Yes}} + \\beta_{4}\\cdot \\operatorname{raterisk}_{\\operatorname{Same}} + \\beta_{5}\\cdot \\operatorname{raterisk}_{\\operatorname{Greater}}"
  },
  {
    "objectID": "terms.html",
    "href": "terms.html",
    "title": "terms",
    "section": "",
    "text": "Terms\n\n\n\n\nTable 1: First 10 Penguins\n\n\n\n\n\n\nwk\nTerm\nDescription\n$$\\text{equals to}$$\n\n\n\n\nwk1\nodd\nFor a Bernoulli random variable with parameter $p$, the odds are defined as the ratio of the probability of success to the probability of failure,\n$$\\text{Odd}=\\frac{p}{1-p}$$\n\n\nwk1\nlogit\nThis is log of odd\n$$\\text{logit}(p)=\\text{log}\\frac{p}{1-p}$$\n\n\nwk1\nodd\nfrom logit, you can get odd. This expression will be used a lot later.\n$$\\text{exp}(x) =\\frac{p}{1-p}$$\n\n\nwk1\nprobability\nthis is probability of success in binary respose case\n$$p = \\frac{exp(x)}{1+exp(x)}$$\n\n\nwk2\nsaturated model\nThis model is frequently referred as the `saturated model` because the number of parameters is equal to the number of observations, so that no additional parameters can be estimated. It refers to observed proportions of success. See p81\n$$\\text{logit}(\\pi_i) = \\gamma_i$$\n\n\nwk2\nlikelihood\nAlso known as likelihood function. It measures how well a statistical model explains observed data by calculating the probability of seeing that data under different parameter values of the model.\n\n\n\nwk2\nlikelihood ratio test\nLikelihood ratio test (LRT) is a statistic. P17. Note that this is $\\Lambda$\n$$\\Lambda = \\frac{\\text{MLR under} H_0}{\\text{MLR under} H_0 \\text{ or } H_a}$$\n\n\nwk2\ndeviance\nthe amount that a particular model deviates from another model measured by the transformed LRT,$-2log(\\Lambda)$, see p 81\n$$-2\\text{log}(\\Lambda)$$\n\n\nwk2\nnull deviance\nThe null deviance denotes how much the probabilities estimated from the model logit$(\\pi_i)=\\beta_0$ for all observation. Measured in transformed LRT, $-2log(\\Lambda)$\nThe $\\pi_i$ is estaimted to be the same value for this particular model. See p81\n\n\nwk2\nresidual deviance\nmeasures how much probabilities estimated from a model of interest deviatesfrom the `observed proportions of success`. Residual deviance statistics are often calculated as an intermediate step for performing a LRT to compare two models. Measured in transformed LRT, $-2log(\\Lambda)$\nIt is a measure of overall goodness of fit for a model. When you have two different models $H_0$ and $H_a$, you can estimate `the transformed LRT` of $H_0$ by comparing the logit($\\pi^{(0)}$ with `saturated model` and also that of $H_a$. When you subtract the transformed LRT of $H_0$ from that of $H_a$, it measures the probability of success under the $H_0$ and $H_a$. p77 and 81\n\n\nwk2\ntransformed likelihood ratio test statistic\ntransformed likelihood ratio test (LRT) statistic follows chi-square degress of freedom. Recommend using it. Better than the Wald Interval\n$$ -2log(\\Lambda) = -2log( \\frac{L(\\hat{{\\beta}}^{(0)} | y_1,..., y_n)}{L(\\hat{{\\beta}}^{(a)} | y_1,..., y_n)}) = -2\\sum y_i log( \\frac{\\hat{\\pi}_i^{(0)}}{\\hat{\\pi}_i^{(a)}}) + (1 - y_i ) log( \\frac{1- \\hat{\\pi}_i^{(0)}}{1- \\hat{\\pi}_i^{(a)}}) $$\n\n\nwk2\n$\\text{Odds}_x$\nThe odds of a success at a particular value of x. p83\n$$\\text{Odds}_x = exp(\\beta_0 + \\beta_1x) $$\n\n\nwk2\n$\\text{Odds}_{x+c}$\nThe odds of a success when x is changed by c. p83\n$$\\text{Odds}_{x+c} = exp(\\beta_0 + \\beta_1(x+c)) $$\n\n\nwk2\nodd ratio\nOR is operator. See the bottom of page 82. It should be E[X]. The odds of a success change by exp($c\\beta_1$ ) times for every c-unit increase in x.\n$$\\frac{\\text{Odds}_{x+c}}{\\text{Odds}_x}=\\frac{exp(\\beta_0 + \\beta_1(x+c))}{exp(\\beta_0 + \\beta_1x)}=exp(c\\beta_1)$$\n\n\nwk2\n$\\hat{\\text{OR}}$\nthe estimated odd ratio, and its confidence interval can be found by `confint()`\n$$\\hat{\\text{OR}} =\\text{exp}(c\\hat{\\beta})$$\n\n\nwk3\n$\\hat{\\pi}$\nprobability of succuss in logistic regression. P86"
  },
  {
    "objectID": "wk1.html",
    "href": "wk1.html",
    "title": "Binary groups",
    "section": "",
    "text": "Source"
  },
  {
    "objectID": "wk1.html#bernoulli-distribution",
    "href": "wk1.html#bernoulli-distribution",
    "title": "Binary groups",
    "section": "Bernoulli distribution",
    "text": "Bernoulli distribution\n\nparameters\n\nY is 0 or 1. Has one parameter \\pi\nE[Y] = \\pi, \\text{V}[Y]=\\pi(1-\\pi)\n\n\n\nPMF\nP(Y=y) = \\pi^y(1-\\pi)^{1-y}"
  },
  {
    "objectID": "wk1.html#binomial-distribution",
    "href": "wk1.html#binomial-distribution",
    "title": "Binary groups",
    "section": "Binomial distribution",
    "text": "Binomial distribution\n\nparameters\n\nMultiple bernoulli trials. Suppose we have n trials\nE[Y] = n\\pi, \\text{V}[Y]= n\\pi(1-\\pi)\n\n\n\nPMF\nP(W=w) = {n \\choose w} \\pi^w(1-\\pi)^{1-w}\n\n\nLikelihood Function\n\nWhat is Likelihood function?\n\n\\begin{align}\nL(\\pi|y_1,y_2,....y_n) &= P(Y_1 = y_1)\\cdot ... P(Y_n=y_n)\\\\\n&=\\pi^w(1-\\pi)^{n-w}\n\\end{align}\n\n\nMLE\n\nMaximum Likelihood Estimation\nSuppose your aunt sends you an unfair coin, but you forgot what your order was.\n\nTo figure out the probability of success, you flip the coin three times and collect the following data (we are defining heads as success here):\n\n\n\nHTH\n\n\nFor a hypothesized Bernoulli parameter \\pi, what is the likelihood of the data? Your answer should be a function of \\pi.\nlikelihood function is:\n\n\n\\begin{aligned}\n  L(\\pi|x_1,x_2,x_3) &= P(X_1 = x_1,X_1 = x_2, X_3 =  x_{3}) \\\\\n       &= \\prod_{i=1}^{3} P({X=x_i}) \\\\\n       &= \\prod_{i=1}^{3} \\pi^{x_i}(1-\\pi)^{1-x_i} \\\\\n       &= \\pi^{\\sum_{i=1}^{3} x_i} (1-\\pi)^{\\sum_{i=1}^{3}(1- x_i)} \\\\\n\\end{aligned}\n\n\nlog of the likelihood function\n\n\n\\begin{aligned}\n  \\text{Log}[L(\\pi|x_1,x_2,x_3)] &= \\left( {\\sum_{i=1}^{3} x_i} \\right)\\text{log}(\\pi) + \\left({\\sum_{i=1}^{3}(1- x_i)} \\right) \\text{log}(1-\\pi)\\\\\n\\end{aligned}\n\n\nWhat is the natural log of the likelihood of the data? Write an R function that computes the log likelihood.\n\n\nGraph your function and visually estimate what the maximum likelihood estimate for \\pi is.\n\n\n\n\n\n\n\n\n\n\n\nWe know that MLE of \\pi is: \\hat{\\pi} =\\frac{\\sum x_i}{N} = \\frac{2}{3} \nand in this question, it’s:\n\n\\hat{\\pi} = \\frac{2}{3} \n\nIn the plot, we can see that log-likelihood has a single peak at 2/3."
  },
  {
    "objectID": "wk1.html#interval",
    "href": "wk1.html#interval",
    "title": "Binary groups",
    "section": "Interval",
    "text": "Interval\n\nGiven observation, knowing distribution, we are estimating the parameter of the function.\nSince this is an estimator, it will change each time we collect sample.\nWe talked about Wald confidence interval, similar to what we talked about in W203.\n\nIt rely on the underlying normal distribution approximation for the maximum likelihood estimator. (see page 11)"
  },
  {
    "objectID": "wk1.html#hypothesis-test",
    "href": "wk1.html#hypothesis-test",
    "title": "Binary groups",
    "section": "Hypothesis test",
    "text": "Hypothesis test\n\nWhen only one simple parameter is of interest, such as \\pi here, we generally prefer confidence intervals over hypothesis tests, because the interval gives a range of possible parameter values.\nWe can typically infer that a hypothesized value for a parameter can be rejected if it does not lie within the confidence interval for the parameter.\nHowever, there are situations where a fixed known value of \\pi, say \\pi_0 , is of special interest, leading to a formal hypothesis test of\n\nH_0 : \\pi = \\pi_0 H_a : \\pi  \\not= \\pi_0\n\nSituations where a fixed known value of \\pi, say \\pi_0 , is of special interest, leading to a formal hypothesis test of\n\nH_0 : \\pi = \\pi_0 H_a : \\pi  \\not= \\pi_0\n\nSee page 11 and 12\n\n\nHypothesis testing using Wilson interval, score test statistic\n\n\n\n\n\nThis statistic is called the score test statistic\nZ_0 = \\frac{\\hat{\\pi}-\\pi_0}{\\sqrt{\\pi_0(1-\\pi_0)/n}}\n\nWhen H_0 is true, Z_0 should have approximately standard normal distribution.\nThe book recommend using the score test when performing a test for \\pi (see page 17)\n\n\n\nHypothesis testing using likelihood ratio test (LRT)\n\n\n\n\n\n\nLRT statistics look like this.\n\n\\Lambda = \\frac{\\text{Maximum of likelihood function udner } H_0}{\\text{Maximum of likelihood function udner } H_A}\n\nLRT is used to calculate confidence intervals in some more complicated contexts where better intervals are not available.\nThis interval is better than the wald interval in most problems. (see page 10)\n\n\n\nHypothesis testing using Transformed LRT\n\n\n\n\n\n-2\\text{log}(\\Lambda)\n\nAbove statistic is called transformed LRT statistisc and have an approximate \\chi^2 distribution."
  },
  {
    "objectID": "wk1.html#interval-test",
    "href": "wk1.html#interval-test",
    "title": "Binary groups",
    "section": "Interval test",
    "text": "Interval test\n\nWald Confidence interval using normal approximation.\nBook recommend using the Agresti-Caffo method\n\n\nLarry Bird’s free throw shooting\n\nc.table &lt;- array(data = c(251, 48, 34, 5), dim = c(2,2),\ndimnames = list(First = c(\"made\", \"missed\"), Second = c(\"made\", \"missed\")))\nc.table\n\n        Second\nFirst    made missed\n  made    251     34\n  missed   48      5\n\n#conditional probabilities\npi.hat.table &lt;- c.table/rowSums(c.table)\n\n#get the pi estimates\npi.hat1 &lt;- pi.hat.table[1,1] \npi.hat2 &lt;- pi.hat.table[2,1]\n\n#set type I error\nalpha &lt;- 0.05\n\n\nWald Confidence interval\n\n\n#########################\n#wald CI\n#########################\nvar.wald &lt;- pi.hat1*(1-pi.hat1) / sum(c.table[1,]) +\npi.hat2*(1-pi.hat2) / sum(c.table[2,])\n\npi.hat1 - pi.hat2 + qnorm(p = c(alpha/2, 1-alpha /2)) *\nsqrt(var.wald)\n\n[1] -0.11218742  0.06227017\n\n\n\nAgresti-Caffo confidence interval\n\n\n#########################\n# Agresti-Caffo\n#########################\npi.tilde1 &lt;- (c.table[1,1] + 1) / (sum(c.table[1,]) + 2) \npi.tilde2 &lt;- (c.table[2,1] + 1) / (sum(c.table[2,]) + 2) \nvar.AC &lt;- pi.tilde1*(1-pi.tilde1) / (sum(c.table[1,]) + 2) +\n\npi.tilde2*(1-pi.tilde2) / (sum(c.table[2,]) + 2)\npi.tilde1 - pi.tilde2 + qnorm(p = c(alpha/2, 1-alpha /2)) *sqrt(var.AC)\n\n[1] -0.10353254  0.07781192\n\n\n\nBecause these interval contains 0, we cannot reject H_0"
  },
  {
    "objectID": "wk1.html#hypothesis-test-1",
    "href": "wk1.html#hypothesis-test-1",
    "title": "Binary groups",
    "section": "Hypothesis test",
    "text": "Hypothesis test\nH_0: \\pi_1 - \\pi_2 = 0 H_0: \\pi_1 - \\pi_2 \\not= 0\n\nPerson chi-square test\n\nSo, this is the test for TWO BINARY VARIABLES.\nCreate a statistic comparing the difference between what was observed and what was predicted under H_0 that there is no difference.\nThis statistic following \\Chi^2 with n_1 and n_2 degress of freedom.\n\n\\chi^2 = \\sum_{j=1}^2\\frac{(w_j-n_j\\bar{\\pi})^2}{n_j\\bar{\\pi}(1-\\bar{\\pi})}\n\n\nLRT test\n-2\\text{log}({\\Lambda})= ...  - if -2\\text{log}({\\Lambda}) &gt; \\chi^2 with some degrees of freedom, Reject H_0\n\n\nScore test\n\nUse score statistic to perform test. Score test performs the best when the same size is small.\n\n\nprop.test(x = c.table , conf.level = 0.95, correct = FALSE)\n\n\n    2-sample test for equality of proportions without continuity correction\n\ndata:  c.table\nX-squared = 0.27274, df = 1, p-value = 0.6015\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.11218742  0.06227017\nsample estimates:\n   prop 1    prop 2 \n0.8807018 0.9056604 \n\n\n\ncorrect = FALSE argument value guarantees that the test statistic is calculated as shown Z_0.\nThe prevalence of a disease is the proportion of a population that is afflicted with that disease"
  },
  {
    "objectID": "wk1.html#relative-risk",
    "href": "wk1.html#relative-risk",
    "title": "Binary groups",
    "section": "Relative Risk",
    "text": "Relative Risk\n\nThe problem with basing inferences on \\pi_1 - \\pi_2 is that it measures a quantity whose meaning changes depending on the value of \\pi_1,\\pi_2\n\n\n\\pi_1 = 0.51 \\text{ and }  \\pi_2 = 0.5\n\\pi_1 = 0.011 \\text{ and }  \\pi_2 = 0.001\n\n\nIn both cases, 1. \\pi_1 - \\pi_2 = 0.01.\nIn case (1), this change is small compare to 1. \\pi_1\\text{ and } \\pi_2\nIn case (2), this change is 11 times the chance of \\pi_2. Suppose \\pi_2 is the chance of nonsmoking population getting a disease, then, the chance is 11 times the chance of nonsmoking population getting a disease.\nTo capture this information, we use another statistic called relative risk\n\n\\text{RR} = \\frac{\\pi_1}{\\pi_2}=\\frac{0.011}{0.001}\n\nSmokers are 11 times as likely to have the disease than nonsmokers\nSmokers are 10 times more likely to have that disease than nonsmokers. (see page 38)\n\\hat{RR} is MLE and using normal approximation is rather poor for MLE and not recommended.\n\nUse normal approximation on \\text{log}(\\hat{RR})\n\n\n\nRR of Salk vaccine clinical trial\n\nc.table &lt;- array(data = c(57, 142, 200688, 201087), dim =\nc(2,2), dimnames = list(Treatment = c(\"vaccine\", \"placebo\"), Result = c(\"polio\", \"polio free\")))\n\nc.table\n\n         Result\nTreatment polio polio free\n  vaccine    57     200688\n  placebo   142     201087\n\n#calculate conditional probability\npi.hat.table &lt;- c.table/rowSums(c.table) \npi.hat.table\n\n         Result\nTreatment        polio polio free\n  vaccine 0.0002839423  0.9997161\n  placebo 0.0007056637  0.9992943\n\n#estimated parameters to be compared\npi.hat1 &lt;- pi.hat.table[1,1] \npi.hat2 &lt;- pi.hat.table[2,1]\n\nRR &lt;- pi.hat1/pi.hat2\nRR\n\n[1] 0.4023763\n\n\n\nThe estimated probability of contracting polio is 0.4 times as likely for the vaccine group that for thh placebo group.\n\n\n#set type 1 error\nalpha &lt;- 0.05\nn1 &lt;- sum(c.table[1,]) \nn2 &lt;- sum(c.table[2,])\n\nvar.log.rr &lt;- (1-pi.hat1)/(n1*pi.hat1) + (1-pi.hat2)/(n2*pi.hat2)\n\nci &lt;- exp(log(pi.hat1/pi.hat2) + qnorm(p = c(alpha/2,\n1-alpha/2)) * sqrt(var.log.rr))\n\n\nci\n\n[1] 0.2959316 0.5471084"
  },
  {
    "objectID": "wk1.html#odds-1",
    "href": "wk1.html#odds-1",
    "title": "Binary groups",
    "section": "Odds",
    "text": "Odds\n\nOdds can also be used as a similar measure as relative risk.\nOdds are defined as the probability of a success divided by the probability of a failure.\n\n\\text{Odds} = \\frac{\\pi}{1-\\pi} = \\frac{0.1}{1-0.1}= \\frac{0.1}{0.9} - This will be referred to as 9-to-1 odds against.\n\nOdds have no upper limit unlike probabilities.\nLike RR, odds are estimated with MLE."
  },
  {
    "objectID": "wk1.html#odds-ratios",
    "href": "wk1.html#odds-ratios",
    "title": "Binary groups",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nDetermining whether or not an odds ratio is equal to 1, greater than 1 or less than 1 is often of interest.\n\n\\text{OR} = \\frac{\\text{Odds}_1}{\\text{Odds}_2}\n\nThe estimated odds of a success is \\hat{\\text{OR}} times as larag as in group 1 than in group 2\nSince OR is a statistic, each time you get a sample and estimate, you will get difference value. You can calculate CI of this estimate.\n\n\n\n\n\nTable 1: First 10 Penguins\n\n\n\n\n\n\nOdd ratio\nMeanining (page 40)\n\n\n\n\n1\nOdds are not dependent on the group (i.e., the odds of success are independent of the group designation)\n\n\n&gt;1\nThe odds of a success are higher for group 1\n\n\n1&lt;\nThe odds of a success are higher for group 2\n\n\n\n\n\n\n\n\n\n\n\nOR of Salk vaccine clinical trial\n\nOR.hat &lt;- c.table[1,1] * c.table[2,2] / (c.table[2,1] *\nc.table[1,2])\nround(OR.hat , 4)\n\n[1] 0.4022\n\n#get confidence interval\nalpha &lt;- 0.05 \n\nvar.log.or &lt;- 1/c.table[1,1] + 1/c.table[1,2] + 1/c.table[2,1] + 1/c.table[2,2]\n\nOR.CI &lt;- exp(log(OR.hat) + qnorm(p = c(alpha/2, 1-alpha /2))*sqrt(var.log.or))\n\nround(OR.CI , 2)\n\n[1] 0.30 0.55"
  },
  {
    "objectID": "wk1.html#prostate-cancer-diagnosis-procedures",
    "href": "wk1.html#prostate-cancer-diagnosis-procedures",
    "title": "Binary groups",
    "section": "Prostate cancer diagnosis procedures",
    "text": "Prostate cancer diagnosis procedures\n\n(P44) Zhou and Qin (2005) discuss a study that was used to compare the diagnostic accuracy of magnetic resonance imaging (MRI) and ultrasound in patients who had been established as having localized prostate cancer by a gold standard test.\n\n\nHypothesis testing\n\nThis is comparing marginal probabilities\n\nH_0: \\pi_{+1}=\\pi_{1+}  H_a: \\pi_{+1}\\not= \\pi_{1+} \n\nWe can use wald test statistic (the book uses Z_0 notation which was used for score statistic) or McNemar's test statistic, M which has approximately \\chi^2 distribution for large samples.\n\nReject H_0 when M &gt; \\chi^2_{..}\n\n\n\n#get marginal distribution \nn &lt;- sum(c.table) \npi.hat.plus1 &lt;- sum(c.table[,1])/n \npi.hat.1plus &lt;- sum(c.table[1,])/n\n\n#evaluate the difference by \n#subtracting the sample statistics\ndata.frame(pi.hat.plus1 , pi.hat.1plus , \n           diff = pi.hat.plus1 - pi.hat.1plus)\n\n  pi.hat.plus1 pi.hat.1plus       diff\n1 0.0004950569     0.499398 -0.4989029\n\n\n\nc.table &lt;- array(data = c(4, 3, 6, 3), dim = c(2,2), dimnames =\nlist(MRI = c(\"Localized\", \"Advanced\"), Ultrasound =\nc(\"Localized\", \"Advanced\"))) \n\nc.table\n\n           Ultrasound\nMRI         Localized Advanced\n  Localized         4        6\n  Advanced          3        3\n\nmcnemar.test(x = c.table , correct = FALSE)\n\n\n    McNemar's Chi-squared test\n\ndata:  c.table\nMcNemar's chi-squared = 1, df = 1, p-value = 0.3173\n\n\n\n\nInterval\n\nWald confidence interval\n\n\n\n\n\n\ndata:  \n\n95 percent confidence interval:\n -0.5433238  0.1683238\nsample estimates:\n[1] -0.1875\n\n\n\nAgresti-Min confidence interval\n\nYou also get difference sample esitmate.\n\n\n\n\n\n\n\ndata:  \n\n95 percent confidence interval:\n -0.5022786  0.1689453\nsample estimates:\n[1] -0.1666667"
  },
  {
    "objectID": "wk4P1.html",
    "href": "wk4P1.html",
    "title": "Multinomial",
    "section": "",
    "text": "Reading\n\nCh.3\n\nskip 3.4.3, 3.5 (page )"
  },
  {
    "objectID": "wk4P1.html#this-part-is-preparing-the-data-set-part",
    "href": "wk4P1.html#this-part-is-preparing-the-data-set-part",
    "title": "Multinomial",
    "section": "This part is preparing the data set part",
    "text": "This part is preparing the data set part\n\n#calculate conditional probability \npi.cond &lt;- pi.table/rowSums(pi.table)\npi.cond %&gt;% kable(\"html\") %&gt;% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\n1\n2\n3\n\n\n\n\n0.4\n0.4\n0.2\n\n\n0.6\n0.2\n0.2\n\n\n\n\n\n\n#simulate based on the conditional probability\n#notice the sample size is different from c.table1\n#suppose that we sampled 400 and 600.  This is just what was \n#observed or we selected this\nset.seed(8111) \nsave1 &lt;- rmultinom(n = 1, size = 400, prob = pi.cond[1,]) \nsave2 &lt;- rmultinom(n = 1, size = 600, prob = pi.cond[2,])\n\nc.table2 &lt;- array(data = c(save1[1], save2[1], save1[2],\nsave2[2], save1[3], save2 [3]), dim = c(2,3), dimnames = list(X = 1:2, Y = 1:3))\n\nc.table2 %&gt;% kable(\"html\") %&gt;% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\n1\n2\n3\n\n\n\n\n162\n159\n79\n\n\n351\n126\n123\n\n\n\n\n\n\n\n\nCan we estimate the joint from this table? is the question.\n\nAnswer is, yes\n\n\n\n\n   Y\nX       1      2      3\n  1 0.405 0.3975 0.1975\n  2 0.585 0.2100 0.2050\n\n\n\nNow, we have joint and marginal, can we test independence?"
  },
  {
    "objectID": "wk4P3.html",
    "href": "wk4P3.html",
    "title": "Ordinal response regression models",
    "section": "",
    "text": "Reading\n\nCh.3.4\n\nskip 3.4.3, 3.5"
  },
  {
    "objectID": "wk4P3.html#functional-form",
    "href": "wk4P3.html#functional-form",
    "title": "Ordinal response regression models",
    "section": "Functional form",
    "text": "Functional form\n\n(p170)\nThe model is the log-odds of cumulative probabilities, called cumulative logits\n\n\\text{logit}(P(Y \\le j)) = \\text{log}\\frac{P(Y \\le j)}{1-P(Y \\le j)} In particular, the proportional odds model is assumes the probabilities changes linear as the explanatory variable chagne and the also the slope of this relationship is the same regardless of the category j. (see page 170)\n\\text{logit}(P(Y \\le j)) = \\beta_{j0} + \\beta_{1}x_1 .. + \\beta_{p}x_p - Notice the subscript j exists only for the constant term. This is the simplified version of the nominal response model.\n\nThe model assumes that the effects of the explanatory variables are the same regardless of which cumulative probabilities are used to from the log odds\nFor a fixed j, increasing x_j by c units changes every log-odds by c\\beta_r when holding other explanatory variables constant."
  },
  {
    "objectID": "wk4P3.html#probability",
    "href": "wk4P3.html#probability",
    "title": "Ordinal response regression models",
    "section": "Probability",
    "text": "Probability\n\\pi_1 = P(Y \\le 1) - P(Y \\le 0) = \\frac{\\text{exp}(\\beta_{j0} + \\beta_{1}x_1 .. + \\beta_{p}x_p)}{1+\\text{exp}(\\beta_{j0} + \\beta_{1}x_1 .. + \\beta_{p}x_p)} \\pi_J = P(Y \\le J) - P(Y \\le J-1) = \\frac{\\text{exp}(\\beta_{J-1,0} + \\beta_{1}x_1 .. + \\beta_{p}x_p)}{1+\\text{exp}(\\beta_{J-1,0} + \\beta_{1}x_1 .. + \\beta_{p}x_p)}"
  },
  {
    "objectID": "wk4P3.html#fit-the-model",
    "href": "wk4P3.html#fit-the-model",
    "title": "Ordinal response regression models",
    "section": "fit the model",
    "text": "fit the model\n\nmethod = \"logistic\" in polr() argument instruct R to use the logit transformation on the cumulative probabilities.\n\n\n\\begin{aligned}\n\\log\\left[ \\frac { P( \\operatorname{type.order}  \\leq  \\operatorname{Scab} ) }{ 1 - P( \\operatorname{type.order}  \\leq  \\operatorname{Scab} ) } \\right] &= \\alpha_{1} + \\beta_{1}(\\operatorname{class}_{\\operatorname{srw}}) + \\beta_{2}(\\operatorname{density}) + \\beta_{3}(\\operatorname{hardness}) + \\beta_{4}(\\operatorname{size}) + \\beta_{5}(\\operatorname{weight}) + \\beta_{6}(\\operatorname{moisture}) \\\\\n\\log\\left[ \\frac { P( \\operatorname{type.order}  \\leq  \\operatorname{Sprout} ) }{ 1 - P( \\operatorname{type.order}  \\leq  \\operatorname{Sprout} ) } \\right] &= \\alpha_{2} + \\beta_{1}(\\operatorname{class}_{\\operatorname{srw}}) + \\beta_{2}(\\operatorname{density}) + \\beta_{3}(\\operatorname{hardness}) + \\beta_{4}(\\operatorname{size}) + \\beta_{5}(\\operatorname{weight}) + \\beta_{6}(\\operatorname{moisture})\n\\end{aligned}\n ## evaluate\n\n\\alpha_1 = 17.57 and \\alpha_2 = 20.04 and their corresponding t-stat are 7.82 and 8.56 respectively.\n\n\n\n\nRe-fitting to get Hessian\n\n\nCall:\npolr(formula = type.order ~ class + density + hardness + size + \n    weight + moisture, data = wheat, method = \"logistic\")\n\nCoefficients:\n            Value Std. Error t value\nclasssrw  0.17370   0.391764  0.4434\ndensity  13.50534   1.713009  7.8840\nhardness  0.01039   0.005932  1.7522\nsize     -0.29253   0.413095 -0.7081\nweight    0.12721   0.029996  4.2411\nmoisture -0.03902   0.088396 -0.4414\n\nIntercepts:\n               Value   Std. Error t value\nScab|Sprout    17.5724  2.2460     7.8237\nSprout|Healthy 20.0444  2.3395     8.5677\n\nResidual Deviance: 422.4178 \nAIC: 438.4178 \n\n\n\ndensity and weight have low p-value\n\n\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: type.order\n         LR Chisq Df Pr(&gt;Chisq)    \nclass       0.197  1    0.65749    \ndensity    98.437  1  &lt; 2.2e-16 ***\nhardness    3.084  1    0.07908 .  \nsize        0.499  1    0.47982    \nweight     18.965  1  1.332e-05 ***\nmoisture    0.195  1    0.65872    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "wk4P3.html#predict-pi_j",
    "href": "wk4P3.html#predict-pi_j",
    "title": "Ordinal response regression models",
    "section": "predict \\pi_j",
    "text": "predict \\pi_j\n\n\n        Scab    Sprout   Healthy\n1 0.03661601 0.2738502 0.6895338\n2 0.03351672 0.2576769 0.7088064\n3 0.08379891 0.4362428 0.4799583\n4 0.01694278 0.1526100 0.8304472\n5 0.11408176 0.4899557 0.3959626\n6 0.02874814 0.2308637 0.7403882\n\n\n\nFor finding confidence interval see page 175"
  },
  {
    "objectID": "wk4P3.html#or",
    "href": "wk4P3.html#or",
    "title": "Ordinal response regression models",
    "section": "OR",
    "text": "OR\n\np175\nThe odds of Y \\le j vs Y &gt; j change by e^{c\\beta_1} times for a c-unit increase in x_1 while holding the other explanatory variables in the model constant.\nThe OR stays the same no matter what response category is used for j"
  }
]