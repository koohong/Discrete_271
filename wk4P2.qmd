---
title: "multinomial regression model "
format:
  html:
    toc: true
    html-math-method: katex
    css: style.css  
    theme:
      light: cosmo
      dark: [cosmo, theme-dark.scss]
execute: 
   echo: false
editor_options: 
  chunk_output_type: console
---

```{=html}
<style>
.table-hover > tbody > tr:hover { 
  background-color: #f4f442;
}
</style>
```

```{r message=FALSE, warning=FALSE, include=FALSE}
library(here)
source(here("source","get_lib.R"))
```

-   Reading
    -   Ch.3.3

# Types of Target

-   In CH1 and 2, the target was binary.
-   In CH3, target is factor which can be 
    - ordinal (i.e., ordered like Likert scale) or 
    - nominal (i.e., unordered like red and green)

# Nominal response regression models

-   target: red, blue, green (nominal)
-   recall that odd for binary response was

$$\text{Odd} = \frac{\text{probability of success}}{\text{probability of failure}}$$

-   We can define odd ratio for multinomial response in a similar way

$$\text{Odd} = \frac{\text{probability of getting red}}{\text{probability of getting blue}}$$

## Functional form 

$$\text{log}(\frac{\text{probility of getting some color}}{\text{probability of getting blue}}) = \beta_{\text{some color }0} + \beta_{\text{some color }1}x_1 .. + \beta_{\text{some color }p}x_p$$

$$\text{log}(\frac{\pi_j}{\pi_1}) = \beta_{j0} + \beta_{j1}x_1 .. + \beta_{jp}x_p$$

-   using the multinomial regression model, we can estimate $\pi_1..\pi_J$ based on sample, but finding their confidence interval is hard such that we only focus on Wald Confidence interval here.


# Wheat example

- page 154

![](image/wheat.PNG){fig-align="center" width="400"}

-   Target is nominal response: `Healthy`, `Sprout` and `Scab`

## Fit the model and evaluate


### EDA

```{r}
#| echo: true
wheat <- read.csv(here("data","Wheat.csv"), stringsAsFactors = TRUE)

hrw <- wheat %>% filter(class == "hrw")

p <- ggparcoord(data = hrw, 
                columns = c(2:6),
                groupColumn = 7,
                showPoints = TRUE, 
                title = "Parallel Coordinate Plot for the Iris Data",
                alphaLines = 0.9)

ggplotly(p)
```

### Fit the model 

```{r}
#| echo: true
#the first is the baseline
levels(wheat$type)

#library(package = nnet)
mod.fit <- multinom(formula = type ~ class + density + hardness
+ size + weight + moisture , data = wheat)
```

### Evaluation

```{r}
summary(mod.fit)
```

- The first parameter `class` has two values `hrw` and `srw`
- This model output is hard to see. 

```{r}
#| echo: true

# library(equatiomatic)
# model <- lm(formula = type ~ class + density + hardness
# + size + weight + moisture , data = wheat)
# latex_equation <- extract_eq(model)
# print(latex_equation)

```

-   The estimated coefficient for `classrw`, `-0.648` is part of the following equation

$$
\operatorname{log\frac{\hat{\pi}_{scab}}{\hat{\pi}_{Healthy}}} = 30.55 -0.65\cdot(\operatorname{class}_{\operatorname{srw}}) + \beta_{2}(\operatorname{density}) + \beta_{3}(\operatorname{hardness}) + \beta_{4}(\operatorname{size}) + \beta_{5}(\operatorname{weight}) + \beta_{6}(\operatorname{moisture}) + \hat{\epsilon}
$$ 

- test statistic is -0.978 and its corresponding p-value is 0.32 - there is not sufficient evidence that hard and soft red winter wheat have different effects on the scab or healthy status of the kernels given the other explanatory variables are in the model

- Let's print the output again. 

```{r}
#using parameter library
a <- model_parameters(mod.fit, digits = 3, p_digits = 3)

a[a$Response=="Scab",-c(8)] %>% kable("html", caption = "Scab") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"))

a[a$Response=="Sprout",-c(8)] %>% kable("html", caption = "Sprout") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"))
```

### Does anything have any predictive power?

-  So many numbers to keep track of... 

![](image/20250515_1714_Comical Cry for Help_simple_compose_01jvb71d92env8p907xwzqr048.png){fig-align="center" width="400"}



$$H_0: \beta_{21} = \beta_{31} = 0$$
$$H_0: \beta_{21} \not = 0 \text{ and/or } \beta_{31} \not= 0$$

### Anova()

Anova() test performs LR test.
  - LR Chisq is the transformed test statistic related to `class` variable,$-2\text{log}(\lambda)=0.964$
  - Because of the large p-value, there is not sufficient evidence to indicate that the class of wheat is important given that the other variables are in the model.
  
```{r}
#| echo: true
Anova(mod.fit) %>% kable("html", caption = "Anova()") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"))
```

- Separate tests for `density`, `hardness`, and `weight` in the output all indicate at least marginal evidence of importance for these explanatory variables.


```{r}
#| echo: true
pi.hat <- predict(object = mod.fit , newdata = wheat , type =
"probs")

pi.hat %>% head() %>% kable("html", caption = "pi.hat") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"))
```

- Check accuracy

```{r}
##let check the accuracy
hat_type <- predict(object = mod.fit , newdata = wheat, type = "class" )

result <- table(hat_type, wheat$type)
#accuracy
sum(diag(result))/sum(result)
```

### Changes in probability 

- With respect to changes in density when the function has only density as its parameter

```{r}
mod0 <- multinom(formula = type ~ density , data = wheat)
new_density <- as.data.frame(seq(0.7,2,by = 0.01))
colnames(new_density) <- "density"

pi_hat <- predict(object = mod0 , newdata = new_density , type =
"probs")

p <- cbind(pi_hat,new_density) %>% pivot_longer(-density) %>% 
    ggplot(aes(x=density, y = value, color = name)) + geom_point() +
    ggtitle("If your model had only density in it")

ggplotly(p)
```

## Odds ratios

- see page 160

- The log-odds are modeled directly in a multinomial regression model, odds ratios are useful for interpreting an explanatory variable’s relationship with the response

- Odds ratios for numerical explanatory variables represent the change in odds correspodning to a `c-unit` increase in a particular explanatory variable.

- The odds of category `j` response vs a category 1 response (the baseline) are $\text{exp}(\beta_{j0}+\beta_{j1}x_1 + ... + \beta_{jp}x_p)$

- The odds in `Wheat proble` are constructed as

$$\frac{P(Y=j)}{P(Y=1)}$$

where j = 2 (scab) and 3 (sprout), and 1 (healthy)

and we had 6 features (see table below)


```{r}
#| echo: true
mod.fit <- multinom(formula = type ~ class + density + hardness
+ size + weight + moisture , data = wheat)

coefficients(mod.fit) %>% kable("html", caption = "mod.fit") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"))
```

### changes in OR

- when you change the value of the feature by 1 sd

```{r}
#| echo: true
sd.wheat <- apply(X = wheat[,-c(1,7,8)], MARGIN = 2, FUN = sd)
sd.wheat

c.value <- c(1, sd.wheat)
c.value
```

```{r}
#scabl
beta.hat2 <- coefficients(mod.fit)[1,2:7]

# Odds ratios for j = 2 vs. j = 1 (scab vs. healthy) 
round(exp(c.value*beta.hat2), 2)

round(1/exp(c.value*beta.hat2), 2)
```

- `The estimated odds` of a `scab` vs. a `healthy kernel` change by `0.06` times for a `0.13` increase in the density holding the other variables constant. 

- Equivalently, we can say that `the estimated odds` of a `scab` vs. a `healthy kernel` change by `17.04` times for a `0.13` decrease in the density holding the other variables constant.

- `The estimated odds` of a `scab` vs. `healthy kernel` change by 9.90 times for a 7.92 decrease in the weight holding the other variables constant.

```{r}
#sprout
beta.hat3 <- coefficients(mod.fit)[2,2:7]

# Odds ratios for j = 3 vs. j = 1 (sprout vs. healthy)
round(exp(c.value*beta.hat3), 2)
```

- `The estimated odds` of a `sprout` vs. a` healthy kernel` change by 7.28 times for a 0.13 decrease in the density holding the other variables constant

- `The estimated odds` of a `sprout` vs. `healthy kernel` change by 1.45 times for a 7.92 decrease in the weight holding the other variables constant.

- We see that the larger the density and weight, the more likely a kernel is healthy.

## Confidence interval

- Get the Wald intervals for the odd ratios

```{r}
#| echo: true

# Wald intervals
conf.beta <- confint(object = mod.fit , level = 0.95)

conf.beta
```

# Contingency table

- page 163
- The multinomial regression model provides a convenient way to perform the LRT for independence. 

- We first perform test for independence and if we reject this $H_0$

$$H_0: \beta_{j2} = ... \beta{jI}=0$$

where $j$ is category in response and $I$ is the level of categorical feature `X`

$$H_A:\text{at least one  }\beta_{ji} \not = 0$$

- Then, the next step is to investigate the source of dependence. 

```{r}
#| echo: true
diet <- read_csv(here("data","Fiber.csv"))
dim(diet)

diet$fiber <- factor(x = diet$fiber , levels = c("none", "bran",
"gum", "both"))
diet$bloat <- factor(x = diet$bloat , levels = c("none", "low",
"medium", "high"))
diet.table <- xtabs(formula = count ~ fiber + bloat , data =
diet)

diet.table



mod.fit.nom <- multinom(formula = bloat ~ fiber , weights =
count , data = diet)
```

- The function recognizes that `fiber` is a factor with levels ordered as `none`, `bran`, `gum` and `both`

- note that se is extremely high for the cells that contains 0 count 


```{r}
summary(mod.fit.nom)
```

- Note that the residual deviance is given as `108.3915` rather than 0 even though a saturated model is being fit. (see page 165)

### Anova

- The statistics indicate that having `fiber` or not matters a lot.

- The coefficients of the models are significant. 
  - Not independent 
  
- The LRT results in −2log($\Lambda$) = 18.9 with a p-value of 0.026. Thus, there is moderate evidence of dependence between type of fiber and severity of bloating

```{r}
Anova(mod.fit.nom)
```

- See page 166 to 167 for 
  - estimating the odds ratio 
  - dealing with 0 count
  - finding the confidence interval of estimated coefficients


