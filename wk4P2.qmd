---
title: "Nominal Response"
format:
  html:
    toc: true
    html-math-method: katex
    css: style.css  
    theme:
      light: cosmo
      dark: [cosmo, theme-dark.scss]
execute: 
   echo: false
editor_options: 
  chunk_output_type: console
---

```{=html}
<style>
.table-hover > tbody > tr:hover { 
  background-color: #f4f442;
}
</style>
```

```{r message=FALSE, warning=FALSE, include=FALSE}
library(here)
source(here("source","get_lib.R"))
```

-   Reading
    -   Ch.3.3

# Types of Target

-   In CH1 and 2, the target was binary.
-   In CH3, target is factor which can be 
    - ordinal (i.e., ordered like Likert scale) or 
    - nominal (i.e., unordered like red and green)

# Nominal response regression models

-   target: red, blue, green (nominal)
-   recall that odd for binary response was

$$\text{Odd} = \frac{\text{probability of success}}{\text{probability of failure}}$$

-   We can define odd ratio for multinomial response in a similar way

$$\text{Odd} = \frac{\text{probability of getting red}}{\text{probability of getting blue}}$$

## Functional form 

$$\text{log}(\frac{\text{probility of getting some color}}{\text{probability of getting blue}}) = \beta_{\text{some color }0} + \beta_{\text{some color }1}x_1 .. + \beta_{\text{some color }p}x_p$$

$$\text{log}(\frac{\pi_j}{\pi_1}) = \beta_{j0} + \beta_{j1}x_1 .. + \beta_{jp}x_p$$

-   using the multinomial regression model, we can estimate $\pi_1..\pi_J$ based on sample, but finding their confidence interval is hard such that we only focus on Wald Confidence interval here.


# wheat example

![](image/wheat.PNG){fig-align="center" width="400"}

-   Target is nominal response: `Healthy`, `Sprout` and `Scab`

## EDA

```{r}
#| echo: true
wheat <- read.csv(here("data","Wheat.csv"), stringsAsFactors = TRUE)

hrw <- wheat %>% filter(class == "hrw")

p <- ggparcoord(data = hrw, 
                columns = c(2:6),
                groupColumn = 7,
                showPoints = TRUE, 
                title = "Parallel Coordinate Plot for the Iris Data",
                alphaLines = 0.9)

ggplotly(p)
```

## Fit the model 

```{r}
#| echo: false
#the first is the baseline
levels(wheat$type)

mod.fit <- multinom(formula = type ~ class + density + hardness
+ size + weight + moisture , data = wheat)
```

## Evaluation

```{r}
summary(mod.fit)
```

- The first parameter `class` has two values `hrw` and `srw`
- This model output is hard to see. 

```{r}


# library(equatiomatic)
# model <- lm(formula = type ~ class + density + hardness
# + size + weight + moisture , data = wheat)
# latex_equation <- extract_eq(model)
# print(latex_equation)

```

-   The estimated coefficient for `classrw`, `-0.648` is part of the following equation

$$
\operatorname{log\frac{\hat{\pi}_{scab}}{\hat{\pi}_{Healthy}}} = 30.55 -0.65\cdot(\operatorname{class}_{\operatorname{srw}}) + \beta_{2}(\operatorname{density}) + \beta_{3}(\operatorname{hardness}) + \beta_{4}(\operatorname{size}) + \beta_{5}(\operatorname{weight}) + \beta_{6}(\operatorname{moisture}) + \hat{\epsilon}
$$ 

- test statistic is -0.978 and its corresponding p-value is 0.32 - there is not sufficient evidence that hard and soft red winter wheat have different effects on the scab or healthy status of the kernels given the other explanatory variables are in the model

- Let's print the output again. 

```{r}
#using parameter library
a <- model_parameters(mod.fit, digits = 3, p_digits = 3)

a[a$Response=="Scab",-c(8)] %>% kable("html", caption = "Scab") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"))

a[a$Response=="Sprout",-c(8)] %>% kable("html", caption = "Sprout") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"))
```

### Does anything have any predictive power?

-  So many numbers to keep track of... 

![](image/20250515_1714_Comical Cry for Help_simple_compose_01jvb71d92env8p907xwzqr048.png){fig-align="center" width="400"}



$$H_0: \beta_{21} = \beta_{31} = 0$$
$$H_0: \beta_{21} \not = 0 \text{ and/or } \beta_{31} \not= 0$$

### Anova()

Anova() test performs LR test.
  - LR Chisq is the transformed test statistic related to `class` variable,$-2\text{log}(\lambda)=0.964$
  - Because of the large p-value, there is not sufficient evidence to indicate that the class of wheat is important given that the other variables are in the model.
  
```{r}
Anova(mod.fit) %>% kable("html", caption = "Anova()") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"))
```

- Separate tests for `density`, `hardness`, and `weight` in the output all indicate at least marginal evidence of importance for these explanatory variables.


```{r}
pi.hat <- predict(object = mod.fit , newdata = wheat , type =
"probs")

##let check the accuracy
hat_type <- predict(object = mod.fit , newdata = wheat, type = "class" )
result <- table(hat_type, wheat$type)
#accuracy
sum(diag(result))/sum(result)
```

```{r}
mod0 <- multinom(formula = type ~ density , data = wheat)
new_density <- as.data.frame(seq(0.7,2,by = 0.01))
colnames(new_density) <- "density"

pi_hat <- predict(object = mod0 , newdata = new_density , type =
"probs")

p <- cbind(pi_hat,new_density) %>% pivot_longer(-density) %>% 
    ggplot(aes(x=density, y = value, color = name)) + geom_point() +
    ggtitle("If your model had only density in it")

ggplotly(p)
```

## OR

- The log-odds are modeled directly in a multinomial regression model, odds ratios are useful for interpreting an explanatory variableâ€™s relationship with the response


