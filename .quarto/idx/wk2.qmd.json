{"title":"Logistic regression","markdown":{"yaml":{"title":"Logistic regression","format":{"html":{"toc":true,"html-math-method":"katex","css":"style.css","theme":{"light":"cosmo","dark":["cosmo","theme-dark.scss"]}}},"execute":{"echo":false},"editor_options":{"chunk_output_type":"console"}},"headingText":"Logistic regression","containsRefs":false,"markdown":"\n\n\n\n```{=html}\n<style>\n.table-hover > tbody > tr:hover { \n  background-color: #f4f442;\n}\n</style>\n```\n\n```{r message=FALSE, warning=FALSE, include=FALSE}\nlibrary(here)\nsource(here(\"source\",\"get_lib.R\"))\n```\n\nReading CH 2.1, 2.2.1-2.2.4 (page 61- 94)\n\n\n- In CH1, we first focused on estimating $\\pi$\n- Then, $\\pi_1$ and $\\pi_2$ and independent groups \n  - Briefly talked about `matched pair` case where $\\pi_1$ and $\\pi_2$ are dependent. \n  \n- Now we start talking about many different possible probabilities of success to estimate and perform inference upon.\n\n\n## Functional form\n\n$$\\pi_i = \\frac{\\text{exp}(\\beta_0 + \\beta_1x_{i,1}+...++ \\beta_px_{i,p})}{1+\\text{exp}(\\beta_0 + \\beta_1x_{i,1}+...++ \\beta_px_{i,p})}$$\n\n$$\\text{logit}(\\pi_i)=\\text{log}(\\frac{\\pi_i}{1-\\pi_i}) = \\beta_0 + \\beta_1x_{i,1}+...++ \\beta_px_{i,p}$$\n- Unfortunately, there are only a few simple cases where these parameter estimates have `closed-form solutions`; i.e., we cannot generally write out the parameter estimates in terms of the observed data like we could for the single probability estimate $\\pi$ in Section 1.1.2. \n\n- Instead, we use iterative numerical procedures, as described in Appendix B.3.2, to successively find estimates of the regression parameters that increase the log-likelihood function\n\n## parameter estimation in R\n\n```{r}\n#| echo: true\n#get data\nplacekick <- read.table(here(\"data\",\"Placekick.csv\"),header = T, sep = \",\")\n\nplacekick %>% head %>% kable(\"html\") %>% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n#fit the model\nmod.fit <- glm(formula = good ~ distance , family =\nbinomial(link = logit), data = placekick)\n\nlatex_equation <- extract_eq(mod.fit)\nprint(latex_equation)\n```\n\n\n\n# ODDS\n\n- see page 83, the odd of success at a particular value of `x`\n\n$$\\text{Odds}_x = \\text{exp}(\\beta_0 + \\beta_1x)$$\n\n\n# Log odds to probability\n\n-   Your aunt offers a service in which she weights coins to make them unfair.\n\n-   You give her a coin and tell her how much you want the `log-odds` to change. She returns the modified coin.\n\n-   For each of the following orders, use your function to compute the resulting probability of heads:\n\n    -   fair coin, increase log-odds by 1.\n    -   fair coin, increase log-odds by 2.\n    -   fair coin, increase log-odds by 10.\n    -   fair coin, decrease log-odds by 1.\n    -   fair coin, decrease log-odds by 2.\n    -   fair coin, decrease log-odds by 10.\n\n-   Write an R function that computes the `probability of heads`, given log-odds.\n\n```{r, error = TRUE}\nlog.odds.to.prob <- function(x){\n    p = exp(x)/(1+exp(x))\n    return(p)\n}\n\nlog.odds.to.prob(0)\n```\n\n```{r echo=FALSE}\nlog_odds <- c(10,2,1, 0, -1, -2, -10 )\n\ndata.frame(log_odds = log_odds,  probability = round(log.odds.to.prob(log_odds),3)) %>% kable(\"html\") %>% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n-   In you own words, describe how changes in log-odds translate to changes in probability\n\n```{r echo=FALSE}\nlog_odds_vector = seq(from = -10, to = 10, by = 0.25)\np = log.odds.to.prob(log_odds_vector)\nd = data.frame(log_odds_vector, p)\nggplot(d, aes(x = log_odds_vector, y = p)) +\ngeom_line() +\ngeom_vline(aes(xintercept = c(-5)), color = \"red\", linetype = \"dashed\")+\ngeom_vline(aes(xintercept = c(5)), color = \"red\", linetype = \"dashed\")+\nscale_x_continuous(breaks = seq(-10, 10, by = 1)) +\nlabs(title = \"probability versus odds\") \n\n```\n\n-   You can see in this plot, As log-odds increase, the probability of success increases relative to the probability of failure, and it approaches one. As log-odds decrease probability of success decrease and converges to zero.\n\n-   If you get log-odds values that are very very small like -10 the probability of success is almost zero, and if you get log-odds values that are very big like 10 or the probability of success is almost one.\n\n-   The relationship between log-odd and probability is not linear, but of s-curve type, and log odds ratios ranging from -5 to +5 create probabilities that range from just above 0 to very close to 1.\n\n\n# Case Study: South African Heart Disease\n\n## Introduction \n\n- Target: `Probability of getting coronary heart disease`, `chd` \n- Feature:\n  - High blood pressure, high LDL cholesterol, diabetes, smoking, secondhand smoke exposure, obesity, an unhealthy diet, and physical inactivity are among the leading risk factors for heart disease.\n\n- Data Source:\n  Source: Rousseauw, J., du Plessis, J., Benade, A., Jordaan, P., Kotze, J. and Ferreira, J. (1983). Coronary risk factor screening in three rural communities, South African   Medical Journal 64: 430â€“436.\n\n## Data Description \n\n  - sbp: systolic blood pressure\n  - tobacco: cumulative tobacco use (kg)\n  - ldl: low density lipoprotein cholesterol ('bad' cholestrol)\n  - adiposity: Body adiposity index determines body fat percentage(calculated as (HC / (HM)1.5) - 18,\n    where HC = Hip Circumference in Centimetres and HM = Height in meters)\n  - famhist: family history of heart disease\n  - typea: A personality type that could raise one's chances of developing coronary heart disease\n  - obesity: Body Mass Index (BMI) ($kg/m^2$)\n  - alcohol: current alcohol consumption\n  - age: age at onset\n  - `chd`: coronary heart disease\n\n\\newpage\n\n# Exploratory Analysis\n\nFor this case study, we focus on blood pressure, smoking, cholesterol, and age.\n\n- Load the data and answer the following questions:\n\n  - What are the number of variables and number of observations?\n  - What is the type of each variable? Do we need to change it? \n  - Are there any missing values (in each of the variables)?\n  - Are there any abnormal values in each of the variables in the raw data?\n\n```{r}\ndf <- SAheart %>%\n   dplyr::select(tobacco, ldl, sbp, age, chd, obesity) \n\nhead(df)%>%\n   kable(\"html\") %>% \n   kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n```\n\n## Univariate Analysis\n\n- The response (or dependent) variable of interest, Heart disease, is a binary variable taking the type factor.  \n\n- Use a bar chart to explore the distribution of the response variable (chd). What do you learn?\n\n```{r echo=FALSE, warning=FALSE}\n\ndf %>%\n  count(chd) %>%\n  mutate(prop = round(prop.table(n),2)) %>%\n  kable(col.names = c('Heart disease', 'N', \"Proportion\")) %>% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\ndf %>%\n  ggplot(aes(x= chd, y = ..prop.., group = 1)) + \n  geom_bar(fill = 'DarkBlue', color = 'black') +\n  geom_text(stat='count', aes(label=..count..), vjust=-1) + \n  xlab(\"Heart disease\") +\n  ylab(\"Proportion\") +\n  scale_y_continuous(label=percent,limits=c(0,1))\n\n```\n\nFor metric variables, a density plot or histogram allows us to determine the shape of the distribution and look for outliers.\n\n- Use a density plot to explore the distribution of explanatory variables. What do you discover? \n\n\n```{r echo=FALSE, fig.width=10, fig.height=11}\n\np1 <- df %>% \n  mutate(chd=factor(chd)) %>%\n  ggplot(aes(x = age)) +\n  geom_density(aes(y = ..density..,  color = chd, fill = chd), alpha = 0.2) +\n  ggtitle(\"Distribution of Subjects' Age\") + \n  theme(plot.title = element_text(lineheight=1, face=\"bold\")) +\n  xlab(\"Age\") +\n  ylab(\"Density\")\n\n\np2 <- df %>% \n  mutate(chd=factor(chd)) %>%\n  ggplot(aes(x = ldl)) +\n  geom_density(aes(y = ..density..,  color = chd, fill = chd), alpha = 0.2) +\n  ggtitle(\"Distribution of Subjects' bad cholesterol\") + \n  theme(plot.title = element_text(lineheight=1, face=\"bold\")) +\n  xlab(\"Low Density Lipoprotein Cholesterol \") +\n  ylab(\"Density\")\n\np3 <-df %>% \n  mutate(chd=factor(chd)) %>%\n  ggplot(aes(x = sbp)) +\n  geom_density(aes(y = ..density..,  color = chd, fill = chd), alpha = 0.2) +\n  ggtitle(\"Distribution of Subjects' Blood Pressure\") + \n  theme(plot.title = element_text(lineheight=1, face=\"bold\")) +\n  xlab(\"Systolic Blood Pressure\") +\n  ylab(\"Density\")\n\np4 <-df %>% \n  mutate(chd=factor(chd)) %>%\n  ggplot(aes(x = tobacco)) +\n  geom_density(aes(y = ..density..,  color = chd, fill = chd), alpha = 0.2) +\n  ggtitle(\"Distribution of Subjects' Tobacco usage\") + \n  theme(plot.title = element_text(lineheight=1, face=\"bold\")) +\n  xlab(\"Cumulative Tobacco usage\") +\n  ylab(\"Density\")\n\ngrid.arrange(p1, p2,p3,p4, nrow = 2, ncol = 2)\n\n```\n\n\n## Bivariate Analysis\n\n- Prior to moving on to the fully specified model, it is advisable to first examine the simple associations between the response and each explanatory variable.\n\n- `Box plots` are useful for exploring the association between a categorical variable and a variable measured on an interval scale.\n\n- Use a `boxplot` to examine how the explanatory variables are correlated with the response variable (chd)?\n\n  - The `coord_flip()` function is used to keep the dependent variable on the y-axis.\n\n```{r echo=FALSE, fig.height=11}\np5 <- df %>%\n  mutate(chd=factor(chd)) %>%\n  ggplot(aes(chd, age)) +\n  geom_boxplot(aes(fill = chd)) + \n  coord_flip() +\n  ggtitle(\"Subjects' Age by Heart Disease\") + \n  theme(plot.title = element_text(lineheight=1, face=\"bold\")) +\n  ylab(\"Age\") +\n  xlab(\"Heart Disease\") \n\np6 <- df %>%\n  mutate(chd=factor(chd)) %>%\n  ggplot(aes(chd, ldl)) +\n  geom_boxplot(aes(fill = chd)) + \n  coord_flip() +\n  ggtitle(\"Subjects' LDL Cholesterol by Heart Disease\") + \n  theme(plot.title = element_text(lineheight=1, face=\"bold\")) +\n  ylab(\"LDL Cholesterol\") +\n  xlab(\" Heart Disease\") \n\np7 <- df %>%\n  mutate(chd=factor(chd)) %>%\n  ggplot(aes(chd, sbp)) +\n  geom_boxplot(aes(fill = chd)) + \n  coord_flip() +\n  ggtitle(\"Subjects' Blood Pressure by Heart Disease\") + \n  theme(plot.title = element_text(lineheight=1, face=\"bold\")) +\n  ylab(\"Systolic Blood Pressure\") +\n  xlab(\" Heart Disease\") \n\np8 <- df %>%\n  mutate(chd=factor(chd)) %>%\n  ggplot(aes(chd, tobacco)) +\n  geom_boxplot(aes(fill = chd)) + \n  coord_flip() +\n  ggtitle(\" Tobacco Usage by Heart Disease\") + \n  theme(plot.title = element_text(lineheight=1, face=\"bold\")) +\n  ylab(\"Tobacco Usage \") +\n  xlab(\" Heart Disease\") \n\np5/p6/p7/p8\n```\n\n\n\\newpage\n\n- Use the convenient summary_factorlist() function from the finalfit package to tabulate data. \n\n```{r}\ndependent <- \"chd\"\nexplanatory <- c(\"ldl\",\"sbp\",\"tobacco\",\"age\")\n\ndf %>% \n  mutate(chd=as.factor(chd)) %>%\n  summary_factorlist(dependent, explanatory, add_dependent_label = TRUE, p = TRUE) %>%\n  kable(\"html\") %>% \n   kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n\n- According to the plots and the tables, What variable is most important for explaining heart disease? How is that variable correlated with heart disease?\n\n\n\\newpage\n\n# Model Development\n\n## Linear probability model\n\n- Is the linear probability model an appropriate choice to study the relationship between heart disease and risk factors?\n\n- Estimate the following linear probability model and interpret the model results.\n\n$$ chd = \\beta_0 + \\beta_1 ldl + \\beta_2 sbp + + \\beta_3 tobacco + \\beta_4 age + u$$\n```{r error = TRUE}\n#mod.linear <-  # uncomment and replace with your code\nmod.linear <- lm(chd ~ ldl + sbp + tobacco + age, data = df)\n\nsummary(mod.linear)\n```\n\n- What are the advantages and disadvantages of the linear probability model?\n\n```{r}\nfitted_values <- data.frame(mod.linear$fitted.values)\n \nfitted_values %>%\n  ggplot(aes(x= 1:length(mod.linear.fitted.values), y = mod.linear.fitted.values)) +\n  geom_line() +\n  geom_hline(aes(yintercept = 0), color = \"red\") +\n  geom_hline(aes(yintercept = 1), color = \"red\")\n```\n\n\n\\newpage\n\n##  Generalized linear model\n\n- Estimate the following logistic regression model and interpret the model results.\n\n$$ logit(\\pi_i) =\\beta_0 + \\beta_1 ldl + \\beta_2 sbp + + \\beta_3 tobacco + \\beta_4 age +  u$$\n\n```{r ,error = TRUE}\n#mod.logit.h0 <-   # uncomment and replace with your code\nmod.logit.h0 <- glm(chd ~ ldl + sbp + tobacco + age, family = binomial(link = logit), data = df)\n\nsummary(mod.logit.h0)\n```\n\n###  Interpretation of model results\n\n- Do the `raw` coefficient estimates `directionally make sense`?\n\n```{r}\n# Replace with your code\nsummary(mod.logit.h0)\n```\n\n- > **Again, all of the explanatory variables except blood pressure are statistically significant and positively correlated with the probability of heart disease, same as the linear probability model.**\n\n\n- Recall that (page 83)\n\n$$\n\\text{OR} = \\frac{\\text{Odds}_{x_k+c}}{\\text{Odds}_{x_k}}=exp(c \\beta_k)\n$$\n- The odd of a success change by exp(c$\\beta_k$) `times` for every c-unit increase in x  \n\n- Compute and interpret the estimated odds ratio for a 10-unit increase in each explanatory variable.\n\n```{r}\n#| echo: true\n# Replace with your code\nround(exp(10*coef(mod.logit.h0)),2)\n```\n\n\n> **The estimated odds of success or having a heart disease change by 6.37 times for every 10-unit increase in LDL or 'bad' cholesterol.** \n    \n> **Interestingly, the odds of having a heart disease is almost 1 for every 10-unit increase in blood pressure, which means an increase in blood pressure doesn't change the odds of having heart disease, and it's consistent with its insignificant coefficient.**\n\n\n# Statistical Inference\n\n\n## Hypothesis Test\n\n- Using the likelihood ratio test (LRT) for hypothesis testing is a common practice in a logistic regression model. \n\n- Use LRT to test whether ($obesity$) is associated with heart disease.\n\n  - $H_0: \\beta_{obesity} = 0$\n\n  - $H_a: \\beta_{obesity} \\ne 0$\n\nUse both *Anova()* or *anova()* functions.\n\n```{r}\n#| echo: true\n#| \n#mod.logit.ha <- # uncomment and replace with your code\nmod.logit.ha <- glm(chd ~ ldl + sbp +tobacco + age + obesity, family = binomial(link = logit), data = df)\n\n#anova()\nanova(mod.logit.h0, mod.logit.ha, test = \"Chisq\")\n\n#Anova()\nAnova(mod.logit.ha, test = \"LR\")\n```\n\n## Deviance \n\n- `deviance` refers to the amount that a particular model deviates from another model as measured by $-2\\text{log}(\\Lambda)$. \n\n- What are the null deviance and residual deviance in the model summary?\n  - For `null` and `residual deviance`, the alternative model we use is the `saturated model`, which has a different coefficent for each data point, leading to perfect prediction, a likelihood of one, and a log likelihood of zero.\n\n- The `null deviance` measures the performance of the worst model using only `an intercept`, providing a benchmark.\n\n$$\n\\text{Null Deviance}= -2 \\text{log}(L(\\hat{\\beta_0}|y_1,..., y_n))\n$$\n\n- The `residual deviance` is the deviance of our fitted model. \n  - It is always greater than zero unless it is the saturated model / explains the data perfectly.\n\n$$\n\\text{Residual Deviance}= -2 \\text{log}(L(\\hat{\\beta}|y_1,..., y_n))\n$$\n\n- Therefore, how much better (smaller) our residual deviance is compared to the null deviance and how close it is to zero is a measure of model fit.\n\n- Sometimes people will compute an $R^2$ for logistic regression using $1-\\frac{\\text{Residual Deviance}}{\\text{Null Deviance}}$ since it is bounded between 0 (residual deviance = null deviance) and 1 (residual deviance = saturated model = 0).\n\n- Note that we can compute deviance of two separate models by substracting the null model residual deviance and the alternative model residual deviance from separate logistic regression fits. (Why is this?)\n\n- Using deviance, test whether ($obesity$) is associated with heart disease.\n\n  - $H_0: \\beta_{obesity} = 0$\n\n  - $H_a: \\beta_{obesity} \\ne 0$\n\n```{r message = FALSE ,error = TRUE}\n#degree_freedom <- # uncomment and replace with your code\ndegree_freedom <- mod.logit.h0$df.residual - mod.logit.ha$df.residual\n\n#test_stat <- # uncomment and replace with your code\ntest_stat <- mod.logit.h0$deviance - mod.logit.ha$deviance\n\n#pvalue <- # uncomment and replace with your code\npvalue <- 1-pchisq(test_stat, df = degree_freedom)\n```\n\n\n> **We get a p-value of 0.29, the same as what we got from both anova() and Anova() functions, and again we fail to reject the null hypothesis that obesity is not correlated with heart disease given this data set.** \n\n\n\n##  Confidence Interval\n\n### Confidence Interval for odds ratio\n\n**Wald Confidence:**\n\n$$\nc*\\hat{\\beta_k} \\pm c*Z_{1-\\alpha/2} \\sqrt{\\widehat{Var}(\\hat{\\beta}_k)}\n$$\n\n$$\nexp \\left(c*\\hat{\\beta_k} \\pm c*Z_{1-\\alpha/2} \\sqrt{\\widehat{Var}(\\hat{\\beta}_k)} \\right)\n$$\n\n- Calculate Wald CI for odds ratio of 10-unit increase in LDL cholesterol  based on the above formula:\n\n```{r}\n# Replace with your code\nvcov(mod.logit.h0)\n\nround(exp(10*mod.logit.h0$coefficients[2] +10*qnorm(p=c(0.025, 0.975))*\n            sqrt(vcov(mod.logit.h0)[2,2])),2)\n```\n\n\n> **With 95% confidence, the odds of having a  heart disease change between 2.20 to 18.4 times for every 10-unit increase in LDL or 'bad' cholesterol.**\n\n\n- What is the main concern with Wald CI?\n\n\n> **Wald confidence interval has a true confidence level close to the 95% only when we have large samples. When the sample size is not large, profile LR confidence intervals generally perform better.**\n\n\n- Now calculate the *profile likelihood ratio (LR)* confidence interval using the confint function.\n\n```{r}\n#| echo: true\n# Replace with your code\nbeta_ci <- confint(mod.logit.h0)\n\nodds_ci <- exp(10*beta_ci)\n\nround(cbind(odds_ci ),2)\n```\n\n\n> **Since we have a large sample, 462 observations, the profile likelihood ratio (LR) confidence interval is pretty close to the Wald CI.**\n\n### Confidence Interval for the Probability of Success\n\n- Recall that the estimated probability of success is\n$$\n\\hat{\\pi} = \\frac{exp \\left( \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\dots + \\hat{\\beta}_K x_k \\right)}{1+exp \\left(\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\dots + \\hat{\\beta}_K x_k \\right)}\n$$\n\nWhile backing out the estimated probability of success is straightforward, obtaining its confidence interval is not, as it involves many parameters.\n\n**Wald Confidence Interval**\n\n$$\n\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\dots + \\hat{\\beta}_K x_K \\pm Z_{1-\\alpha/2} \\sqrt{\\widehat{Var}(\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\dots + \\hat{\\beta}_K x_K)} \n$$\n\nwhere \n\n$$\n\\widehat{Var}(\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\dots + \\hat{\\beta}_K x_K) = \\sum_{i=0}^K x_i^2 \\widehat{Var}(\\hat{\\beta_i}) + 2 \\sum_{i=0}^{K-1} \\sum_{j=i+1}^{K} x_i x_j \\widehat{Cov}(\\hat{\\beta}_i,\\hat{\\beta}_j)\n$$\n\nSo, the Wald Interval for $\\pi$ is\n\n$$\n\\frac{exp \\left( \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\dots + \\hat{\\beta}_K x_k \\pm \\sqrt{\\sum_{i=0}^K x_i^2 \\widehat{Var}(\\hat{\\beta_i}) + 2 \\sum_{i=0}^{K-1} \\sum_{j=i+1}^{K} x_i x_j \\widehat{Cov}(\\hat{\\beta}_i,\\hat{\\beta}_j)}  \\right)}{1+exp \\left(\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\dots + \\hat{\\beta}_K x_k \\right)  \\pm \\sqrt{\\sum_{i=0}^K x_i^2 \\widehat{Var}(\\hat{\\beta_i}) + 2 \\sum_{i=0}^{K-1} \\sum_{j=i+1}^{K} x_i x_j \\widehat{Cov}(\\hat{\\beta}_i,\\hat{\\beta}_j)}}\n$$\n\n- For an average value of all explanatory variables, compute the Confidence Interval for the Probability of Success given the formula above\n\n```{r message = FALSE ,error = TRUE}\nalpha = 0.5\n\npredict.data <- data.frame(ldl = mean(df$ldl),\n                           sbp = mean(df$sbp),\n                           tobacco = mean(df$tobacco),\n                           age = mean(df$age))\n# Obtain the linear predictor\nlinear.pred = predict(object = mod.logit.h0, newdata = predict.data, type = \"link\", se = TRUE)\n\n# Then, compute pi.hat\npi.hat = exp(linear.pred$fit)/(1+exp(linear.pred$fit))\n\n# Compute Wald Confidence Interval (in 2 steps)\n# Step 1\nCI.lin.pred = linear.pred$fit + qnorm(p = c(alpha/2, 1-alpha/2))*linear.pred$se.fit\n#CI.lin.pred\n\n# Step 2\nCI.pi = exp(CI.lin.pred)/(1+exp(CI.lin.pred))\n#CI.pi\n\n# Store all the components in a data frame\n#str(predict.data)\nround(data.frame(pi.hat, lower=CI.pi[1], upper=CI.pi[2]),4)\n```\n\n\n### Final Visualization\n\n- Using both the linear probability and logistic regression models, plot the estimated probability of heart disease for different values of cholesterol, holding other variables constant at their average level.\n\n- Discuss which one can better explain this relationship.\n\n```{r message = FALSE ,error = TRUE}\ncoef <- mod.logit.h0$coefficients\n\n# Effect of income on LDL for a person's average age, sbp, and tobacco usage\n\nxx = c(1, mean(df$ldl), mean(df$sbp), mean(df$tobacco), mean(df$age))\n\nz = coef[1]*xx[1]+ coef[3]*xx[3] + coef[4]*xx[4] + coef[5]*xx[5]\n\nx <- df$ldl\n\n# Reproduce the graph overlaying the same result from the linear model as a comparison\ncurve(expr = exp(z + coef[2]*x)/(1+exp(z + coef[2]*x)), \n    xlim = c(min(df$ldl), max(df$ldl)), \n    ylim = c(0,2),\n    col = \"blue\", main = expression(pi == frac(e^{z + coef[inc]*ldl}, 1+e^{z+coef[inc]*ldl})),\n      xlab =  expression(cholesterol), ylab = expression(pi))\n\n# par(new=TRUE)\n\n\nlm.coef <- mod.linear$coefficients\nlm.z <- lm.coef[1]*xx[1] + lm.coef[3]*xx[3] + lm.coef[4]*xx[4] + lm.coef[5]*xx[5] \nlines(df$ldl, lm.z + lm.coef[2]*x,col=\"green\")\n\n```\n\n\n\\newpage\n\n### Final Report\n\n- Display both estimated linear and logistic models in a regression table. Is there any significant difference between their results?\n\n```{r message = FALSE ,error = TRUE}\n# uncomment and run the code\n\nstargazer(mod.linear, mod.logit.h0, type = \"text\", omit.stat = \"f\",\n                   star.cutoffs = c(0.05, 0.01, 0.001), title = \"Table 1: The estimated relationship between heart disease and  risk factors\")\n\n\n```\n\n> **In both models, all the coefficients except blood pressure are statistically significant and positively associated with the probability of having heart disease. Also, LDL is the most correlated variable with the probability of heart disease in both models.**    \n\n\n# Terms\n\n```{r include=FALSE}\n# library(here)\n# library(readxl)\ndf <- read_excel(here(\"def.xlsx\"), sheet = \"wk1\")\n```\n\n\n```{r echo=FALSE}\n#| label: tbl-penguins-top10\n#| tbl-cap: First 10 Penguins\n\n#replacing NA with white space\ndf[is.na(df)] <- \"\" \n\ndf %>% kable(\"html\") %>% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n","srcMarkdownNoYaml":"\n\n\n\n```{=html}\n<style>\n.table-hover > tbody > tr:hover { \n  background-color: #f4f442;\n}\n</style>\n```\n\n```{r message=FALSE, warning=FALSE, include=FALSE}\nlibrary(here)\nsource(here(\"source\",\"get_lib.R\"))\n```\n\nReading CH 2.1, 2.2.1-2.2.4 (page 61- 94)\n\n# Logistic regression \n\n- In CH1, we first focused on estimating $\\pi$\n- Then, $\\pi_1$ and $\\pi_2$ and independent groups \n  - Briefly talked about `matched pair` case where $\\pi_1$ and $\\pi_2$ are dependent. \n  \n- Now we start talking about many different possible probabilities of success to estimate and perform inference upon.\n\n\n## Functional form\n\n$$\\pi_i = \\frac{\\text{exp}(\\beta_0 + \\beta_1x_{i,1}+...++ \\beta_px_{i,p})}{1+\\text{exp}(\\beta_0 + \\beta_1x_{i,1}+...++ \\beta_px_{i,p})}$$\n\n$$\\text{logit}(\\pi_i)=\\text{log}(\\frac{\\pi_i}{1-\\pi_i}) = \\beta_0 + \\beta_1x_{i,1}+...++ \\beta_px_{i,p}$$\n- Unfortunately, there are only a few simple cases where these parameter estimates have `closed-form solutions`; i.e., we cannot generally write out the parameter estimates in terms of the observed data like we could for the single probability estimate $\\pi$ in Section 1.1.2. \n\n- Instead, we use iterative numerical procedures, as described in Appendix B.3.2, to successively find estimates of the regression parameters that increase the log-likelihood function\n\n## parameter estimation in R\n\n```{r}\n#| echo: true\n#get data\nplacekick <- read.table(here(\"data\",\"Placekick.csv\"),header = T, sep = \",\")\n\nplacekick %>% head %>% kable(\"html\") %>% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n#fit the model\nmod.fit <- glm(formula = good ~ distance , family =\nbinomial(link = logit), data = placekick)\n\nlatex_equation <- extract_eq(mod.fit)\nprint(latex_equation)\n```\n\n\n\n# ODDS\n\n- see page 83, the odd of success at a particular value of `x`\n\n$$\\text{Odds}_x = \\text{exp}(\\beta_0 + \\beta_1x)$$\n\n\n# Log odds to probability\n\n-   Your aunt offers a service in which she weights coins to make them unfair.\n\n-   You give her a coin and tell her how much you want the `log-odds` to change. She returns the modified coin.\n\n-   For each of the following orders, use your function to compute the resulting probability of heads:\n\n    -   fair coin, increase log-odds by 1.\n    -   fair coin, increase log-odds by 2.\n    -   fair coin, increase log-odds by 10.\n    -   fair coin, decrease log-odds by 1.\n    -   fair coin, decrease log-odds by 2.\n    -   fair coin, decrease log-odds by 10.\n\n-   Write an R function that computes the `probability of heads`, given log-odds.\n\n```{r, error = TRUE}\nlog.odds.to.prob <- function(x){\n    p = exp(x)/(1+exp(x))\n    return(p)\n}\n\nlog.odds.to.prob(0)\n```\n\n```{r echo=FALSE}\nlog_odds <- c(10,2,1, 0, -1, -2, -10 )\n\ndata.frame(log_odds = log_odds,  probability = round(log.odds.to.prob(log_odds),3)) %>% kable(\"html\") %>% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n-   In you own words, describe how changes in log-odds translate to changes in probability\n\n```{r echo=FALSE}\nlog_odds_vector = seq(from = -10, to = 10, by = 0.25)\np = log.odds.to.prob(log_odds_vector)\nd = data.frame(log_odds_vector, p)\nggplot(d, aes(x = log_odds_vector, y = p)) +\ngeom_line() +\ngeom_vline(aes(xintercept = c(-5)), color = \"red\", linetype = \"dashed\")+\ngeom_vline(aes(xintercept = c(5)), color = \"red\", linetype = \"dashed\")+\nscale_x_continuous(breaks = seq(-10, 10, by = 1)) +\nlabs(title = \"probability versus odds\") \n\n```\n\n-   You can see in this plot, As log-odds increase, the probability of success increases relative to the probability of failure, and it approaches one. As log-odds decrease probability of success decrease and converges to zero.\n\n-   If you get log-odds values that are very very small like -10 the probability of success is almost zero, and if you get log-odds values that are very big like 10 or the probability of success is almost one.\n\n-   The relationship between log-odd and probability is not linear, but of s-curve type, and log odds ratios ranging from -5 to +5 create probabilities that range from just above 0 to very close to 1.\n\n\n# Case Study: South African Heart Disease\n\n## Introduction \n\n- Target: `Probability of getting coronary heart disease`, `chd` \n- Feature:\n  - High blood pressure, high LDL cholesterol, diabetes, smoking, secondhand smoke exposure, obesity, an unhealthy diet, and physical inactivity are among the leading risk factors for heart disease.\n\n- Data Source:\n  Source: Rousseauw, J., du Plessis, J., Benade, A., Jordaan, P., Kotze, J. and Ferreira, J. (1983). Coronary risk factor screening in three rural communities, South African   Medical Journal 64: 430â€“436.\n\n## Data Description \n\n  - sbp: systolic blood pressure\n  - tobacco: cumulative tobacco use (kg)\n  - ldl: low density lipoprotein cholesterol ('bad' cholestrol)\n  - adiposity: Body adiposity index determines body fat percentage(calculated as (HC / (HM)1.5) - 18,\n    where HC = Hip Circumference in Centimetres and HM = Height in meters)\n  - famhist: family history of heart disease\n  - typea: A personality type that could raise one's chances of developing coronary heart disease\n  - obesity: Body Mass Index (BMI) ($kg/m^2$)\n  - alcohol: current alcohol consumption\n  - age: age at onset\n  - `chd`: coronary heart disease\n\n\\newpage\n\n# Exploratory Analysis\n\nFor this case study, we focus on blood pressure, smoking, cholesterol, and age.\n\n- Load the data and answer the following questions:\n\n  - What are the number of variables and number of observations?\n  - What is the type of each variable? Do we need to change it? \n  - Are there any missing values (in each of the variables)?\n  - Are there any abnormal values in each of the variables in the raw data?\n\n```{r}\ndf <- SAheart %>%\n   dplyr::select(tobacco, ldl, sbp, age, chd, obesity) \n\nhead(df)%>%\n   kable(\"html\") %>% \n   kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n```\n\n## Univariate Analysis\n\n- The response (or dependent) variable of interest, Heart disease, is a binary variable taking the type factor.  \n\n- Use a bar chart to explore the distribution of the response variable (chd). What do you learn?\n\n```{r echo=FALSE, warning=FALSE}\n\ndf %>%\n  count(chd) %>%\n  mutate(prop = round(prop.table(n),2)) %>%\n  kable(col.names = c('Heart disease', 'N', \"Proportion\")) %>% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\ndf %>%\n  ggplot(aes(x= chd, y = ..prop.., group = 1)) + \n  geom_bar(fill = 'DarkBlue', color = 'black') +\n  geom_text(stat='count', aes(label=..count..), vjust=-1) + \n  xlab(\"Heart disease\") +\n  ylab(\"Proportion\") +\n  scale_y_continuous(label=percent,limits=c(0,1))\n\n```\n\nFor metric variables, a density plot or histogram allows us to determine the shape of the distribution and look for outliers.\n\n- Use a density plot to explore the distribution of explanatory variables. What do you discover? \n\n\n```{r echo=FALSE, fig.width=10, fig.height=11}\n\np1 <- df %>% \n  mutate(chd=factor(chd)) %>%\n  ggplot(aes(x = age)) +\n  geom_density(aes(y = ..density..,  color = chd, fill = chd), alpha = 0.2) +\n  ggtitle(\"Distribution of Subjects' Age\") + \n  theme(plot.title = element_text(lineheight=1, face=\"bold\")) +\n  xlab(\"Age\") +\n  ylab(\"Density\")\n\n\np2 <- df %>% \n  mutate(chd=factor(chd)) %>%\n  ggplot(aes(x = ldl)) +\n  geom_density(aes(y = ..density..,  color = chd, fill = chd), alpha = 0.2) +\n  ggtitle(\"Distribution of Subjects' bad cholesterol\") + \n  theme(plot.title = element_text(lineheight=1, face=\"bold\")) +\n  xlab(\"Low Density Lipoprotein Cholesterol \") +\n  ylab(\"Density\")\n\np3 <-df %>% \n  mutate(chd=factor(chd)) %>%\n  ggplot(aes(x = sbp)) +\n  geom_density(aes(y = ..density..,  color = chd, fill = chd), alpha = 0.2) +\n  ggtitle(\"Distribution of Subjects' Blood Pressure\") + \n  theme(plot.title = element_text(lineheight=1, face=\"bold\")) +\n  xlab(\"Systolic Blood Pressure\") +\n  ylab(\"Density\")\n\np4 <-df %>% \n  mutate(chd=factor(chd)) %>%\n  ggplot(aes(x = tobacco)) +\n  geom_density(aes(y = ..density..,  color = chd, fill = chd), alpha = 0.2) +\n  ggtitle(\"Distribution of Subjects' Tobacco usage\") + \n  theme(plot.title = element_text(lineheight=1, face=\"bold\")) +\n  xlab(\"Cumulative Tobacco usage\") +\n  ylab(\"Density\")\n\ngrid.arrange(p1, p2,p3,p4, nrow = 2, ncol = 2)\n\n```\n\n\n## Bivariate Analysis\n\n- Prior to moving on to the fully specified model, it is advisable to first examine the simple associations between the response and each explanatory variable.\n\n- `Box plots` are useful for exploring the association between a categorical variable and a variable measured on an interval scale.\n\n- Use a `boxplot` to examine how the explanatory variables are correlated with the response variable (chd)?\n\n  - The `coord_flip()` function is used to keep the dependent variable on the y-axis.\n\n```{r echo=FALSE, fig.height=11}\np5 <- df %>%\n  mutate(chd=factor(chd)) %>%\n  ggplot(aes(chd, age)) +\n  geom_boxplot(aes(fill = chd)) + \n  coord_flip() +\n  ggtitle(\"Subjects' Age by Heart Disease\") + \n  theme(plot.title = element_text(lineheight=1, face=\"bold\")) +\n  ylab(\"Age\") +\n  xlab(\"Heart Disease\") \n\np6 <- df %>%\n  mutate(chd=factor(chd)) %>%\n  ggplot(aes(chd, ldl)) +\n  geom_boxplot(aes(fill = chd)) + \n  coord_flip() +\n  ggtitle(\"Subjects' LDL Cholesterol by Heart Disease\") + \n  theme(plot.title = element_text(lineheight=1, face=\"bold\")) +\n  ylab(\"LDL Cholesterol\") +\n  xlab(\" Heart Disease\") \n\np7 <- df %>%\n  mutate(chd=factor(chd)) %>%\n  ggplot(aes(chd, sbp)) +\n  geom_boxplot(aes(fill = chd)) + \n  coord_flip() +\n  ggtitle(\"Subjects' Blood Pressure by Heart Disease\") + \n  theme(plot.title = element_text(lineheight=1, face=\"bold\")) +\n  ylab(\"Systolic Blood Pressure\") +\n  xlab(\" Heart Disease\") \n\np8 <- df %>%\n  mutate(chd=factor(chd)) %>%\n  ggplot(aes(chd, tobacco)) +\n  geom_boxplot(aes(fill = chd)) + \n  coord_flip() +\n  ggtitle(\" Tobacco Usage by Heart Disease\") + \n  theme(plot.title = element_text(lineheight=1, face=\"bold\")) +\n  ylab(\"Tobacco Usage \") +\n  xlab(\" Heart Disease\") \n\np5/p6/p7/p8\n```\n\n\n\\newpage\n\n- Use the convenient summary_factorlist() function from the finalfit package to tabulate data. \n\n```{r}\ndependent <- \"chd\"\nexplanatory <- c(\"ldl\",\"sbp\",\"tobacco\",\"age\")\n\ndf %>% \n  mutate(chd=as.factor(chd)) %>%\n  summary_factorlist(dependent, explanatory, add_dependent_label = TRUE, p = TRUE) %>%\n  kable(\"html\") %>% \n   kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n\n- According to the plots and the tables, What variable is most important for explaining heart disease? How is that variable correlated with heart disease?\n\n\n\\newpage\n\n# Model Development\n\n## Linear probability model\n\n- Is the linear probability model an appropriate choice to study the relationship between heart disease and risk factors?\n\n- Estimate the following linear probability model and interpret the model results.\n\n$$ chd = \\beta_0 + \\beta_1 ldl + \\beta_2 sbp + + \\beta_3 tobacco + \\beta_4 age + u$$\n```{r error = TRUE}\n#mod.linear <-  # uncomment and replace with your code\nmod.linear <- lm(chd ~ ldl + sbp + tobacco + age, data = df)\n\nsummary(mod.linear)\n```\n\n- What are the advantages and disadvantages of the linear probability model?\n\n```{r}\nfitted_values <- data.frame(mod.linear$fitted.values)\n \nfitted_values %>%\n  ggplot(aes(x= 1:length(mod.linear.fitted.values), y = mod.linear.fitted.values)) +\n  geom_line() +\n  geom_hline(aes(yintercept = 0), color = \"red\") +\n  geom_hline(aes(yintercept = 1), color = \"red\")\n```\n\n\n\\newpage\n\n##  Generalized linear model\n\n- Estimate the following logistic regression model and interpret the model results.\n\n$$ logit(\\pi_i) =\\beta_0 + \\beta_1 ldl + \\beta_2 sbp + + \\beta_3 tobacco + \\beta_4 age +  u$$\n\n```{r ,error = TRUE}\n#mod.logit.h0 <-   # uncomment and replace with your code\nmod.logit.h0 <- glm(chd ~ ldl + sbp + tobacco + age, family = binomial(link = logit), data = df)\n\nsummary(mod.logit.h0)\n```\n\n###  Interpretation of model results\n\n- Do the `raw` coefficient estimates `directionally make sense`?\n\n```{r}\n# Replace with your code\nsummary(mod.logit.h0)\n```\n\n- > **Again, all of the explanatory variables except blood pressure are statistically significant and positively correlated with the probability of heart disease, same as the linear probability model.**\n\n\n- Recall that (page 83)\n\n$$\n\\text{OR} = \\frac{\\text{Odds}_{x_k+c}}{\\text{Odds}_{x_k}}=exp(c \\beta_k)\n$$\n- The odd of a success change by exp(c$\\beta_k$) `times` for every c-unit increase in x  \n\n- Compute and interpret the estimated odds ratio for a 10-unit increase in each explanatory variable.\n\n```{r}\n#| echo: true\n# Replace with your code\nround(exp(10*coef(mod.logit.h0)),2)\n```\n\n\n> **The estimated odds of success or having a heart disease change by 6.37 times for every 10-unit increase in LDL or 'bad' cholesterol.** \n    \n> **Interestingly, the odds of having a heart disease is almost 1 for every 10-unit increase in blood pressure, which means an increase in blood pressure doesn't change the odds of having heart disease, and it's consistent with its insignificant coefficient.**\n\n\n# Statistical Inference\n\n\n## Hypothesis Test\n\n- Using the likelihood ratio test (LRT) for hypothesis testing is a common practice in a logistic regression model. \n\n- Use LRT to test whether ($obesity$) is associated with heart disease.\n\n  - $H_0: \\beta_{obesity} = 0$\n\n  - $H_a: \\beta_{obesity} \\ne 0$\n\nUse both *Anova()* or *anova()* functions.\n\n```{r}\n#| echo: true\n#| \n#mod.logit.ha <- # uncomment and replace with your code\nmod.logit.ha <- glm(chd ~ ldl + sbp +tobacco + age + obesity, family = binomial(link = logit), data = df)\n\n#anova()\nanova(mod.logit.h0, mod.logit.ha, test = \"Chisq\")\n\n#Anova()\nAnova(mod.logit.ha, test = \"LR\")\n```\n\n## Deviance \n\n- `deviance` refers to the amount that a particular model deviates from another model as measured by $-2\\text{log}(\\Lambda)$. \n\n- What are the null deviance and residual deviance in the model summary?\n  - For `null` and `residual deviance`, the alternative model we use is the `saturated model`, which has a different coefficent for each data point, leading to perfect prediction, a likelihood of one, and a log likelihood of zero.\n\n- The `null deviance` measures the performance of the worst model using only `an intercept`, providing a benchmark.\n\n$$\n\\text{Null Deviance}= -2 \\text{log}(L(\\hat{\\beta_0}|y_1,..., y_n))\n$$\n\n- The `residual deviance` is the deviance of our fitted model. \n  - It is always greater than zero unless it is the saturated model / explains the data perfectly.\n\n$$\n\\text{Residual Deviance}= -2 \\text{log}(L(\\hat{\\beta}|y_1,..., y_n))\n$$\n\n- Therefore, how much better (smaller) our residual deviance is compared to the null deviance and how close it is to zero is a measure of model fit.\n\n- Sometimes people will compute an $R^2$ for logistic regression using $1-\\frac{\\text{Residual Deviance}}{\\text{Null Deviance}}$ since it is bounded between 0 (residual deviance = null deviance) and 1 (residual deviance = saturated model = 0).\n\n- Note that we can compute deviance of two separate models by substracting the null model residual deviance and the alternative model residual deviance from separate logistic regression fits. (Why is this?)\n\n- Using deviance, test whether ($obesity$) is associated with heart disease.\n\n  - $H_0: \\beta_{obesity} = 0$\n\n  - $H_a: \\beta_{obesity} \\ne 0$\n\n```{r message = FALSE ,error = TRUE}\n#degree_freedom <- # uncomment and replace with your code\ndegree_freedom <- mod.logit.h0$df.residual - mod.logit.ha$df.residual\n\n#test_stat <- # uncomment and replace with your code\ntest_stat <- mod.logit.h0$deviance - mod.logit.ha$deviance\n\n#pvalue <- # uncomment and replace with your code\npvalue <- 1-pchisq(test_stat, df = degree_freedom)\n```\n\n\n> **We get a p-value of 0.29, the same as what we got from both anova() and Anova() functions, and again we fail to reject the null hypothesis that obesity is not correlated with heart disease given this data set.** \n\n\n\n##  Confidence Interval\n\n### Confidence Interval for odds ratio\n\n**Wald Confidence:**\n\n$$\nc*\\hat{\\beta_k} \\pm c*Z_{1-\\alpha/2} \\sqrt{\\widehat{Var}(\\hat{\\beta}_k)}\n$$\n\n$$\nexp \\left(c*\\hat{\\beta_k} \\pm c*Z_{1-\\alpha/2} \\sqrt{\\widehat{Var}(\\hat{\\beta}_k)} \\right)\n$$\n\n- Calculate Wald CI for odds ratio of 10-unit increase in LDL cholesterol  based on the above formula:\n\n```{r}\n# Replace with your code\nvcov(mod.logit.h0)\n\nround(exp(10*mod.logit.h0$coefficients[2] +10*qnorm(p=c(0.025, 0.975))*\n            sqrt(vcov(mod.logit.h0)[2,2])),2)\n```\n\n\n> **With 95% confidence, the odds of having a  heart disease change between 2.20 to 18.4 times for every 10-unit increase in LDL or 'bad' cholesterol.**\n\n\n- What is the main concern with Wald CI?\n\n\n> **Wald confidence interval has a true confidence level close to the 95% only when we have large samples. When the sample size is not large, profile LR confidence intervals generally perform better.**\n\n\n- Now calculate the *profile likelihood ratio (LR)* confidence interval using the confint function.\n\n```{r}\n#| echo: true\n# Replace with your code\nbeta_ci <- confint(mod.logit.h0)\n\nodds_ci <- exp(10*beta_ci)\n\nround(cbind(odds_ci ),2)\n```\n\n\n> **Since we have a large sample, 462 observations, the profile likelihood ratio (LR) confidence interval is pretty close to the Wald CI.**\n\n### Confidence Interval for the Probability of Success\n\n- Recall that the estimated probability of success is\n$$\n\\hat{\\pi} = \\frac{exp \\left( \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\dots + \\hat{\\beta}_K x_k \\right)}{1+exp \\left(\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\dots + \\hat{\\beta}_K x_k \\right)}\n$$\n\nWhile backing out the estimated probability of success is straightforward, obtaining its confidence interval is not, as it involves many parameters.\n\n**Wald Confidence Interval**\n\n$$\n\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\dots + \\hat{\\beta}_K x_K \\pm Z_{1-\\alpha/2} \\sqrt{\\widehat{Var}(\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\dots + \\hat{\\beta}_K x_K)} \n$$\n\nwhere \n\n$$\n\\widehat{Var}(\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\dots + \\hat{\\beta}_K x_K) = \\sum_{i=0}^K x_i^2 \\widehat{Var}(\\hat{\\beta_i}) + 2 \\sum_{i=0}^{K-1} \\sum_{j=i+1}^{K} x_i x_j \\widehat{Cov}(\\hat{\\beta}_i,\\hat{\\beta}_j)\n$$\n\nSo, the Wald Interval for $\\pi$ is\n\n$$\n\\frac{exp \\left( \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\dots + \\hat{\\beta}_K x_k \\pm \\sqrt{\\sum_{i=0}^K x_i^2 \\widehat{Var}(\\hat{\\beta_i}) + 2 \\sum_{i=0}^{K-1} \\sum_{j=i+1}^{K} x_i x_j \\widehat{Cov}(\\hat{\\beta}_i,\\hat{\\beta}_j)}  \\right)}{1+exp \\left(\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\dots + \\hat{\\beta}_K x_k \\right)  \\pm \\sqrt{\\sum_{i=0}^K x_i^2 \\widehat{Var}(\\hat{\\beta_i}) + 2 \\sum_{i=0}^{K-1} \\sum_{j=i+1}^{K} x_i x_j \\widehat{Cov}(\\hat{\\beta}_i,\\hat{\\beta}_j)}}\n$$\n\n- For an average value of all explanatory variables, compute the Confidence Interval for the Probability of Success given the formula above\n\n```{r message = FALSE ,error = TRUE}\nalpha = 0.5\n\npredict.data <- data.frame(ldl = mean(df$ldl),\n                           sbp = mean(df$sbp),\n                           tobacco = mean(df$tobacco),\n                           age = mean(df$age))\n# Obtain the linear predictor\nlinear.pred = predict(object = mod.logit.h0, newdata = predict.data, type = \"link\", se = TRUE)\n\n# Then, compute pi.hat\npi.hat = exp(linear.pred$fit)/(1+exp(linear.pred$fit))\n\n# Compute Wald Confidence Interval (in 2 steps)\n# Step 1\nCI.lin.pred = linear.pred$fit + qnorm(p = c(alpha/2, 1-alpha/2))*linear.pred$se.fit\n#CI.lin.pred\n\n# Step 2\nCI.pi = exp(CI.lin.pred)/(1+exp(CI.lin.pred))\n#CI.pi\n\n# Store all the components in a data frame\n#str(predict.data)\nround(data.frame(pi.hat, lower=CI.pi[1], upper=CI.pi[2]),4)\n```\n\n\n### Final Visualization\n\n- Using both the linear probability and logistic regression models, plot the estimated probability of heart disease for different values of cholesterol, holding other variables constant at their average level.\n\n- Discuss which one can better explain this relationship.\n\n```{r message = FALSE ,error = TRUE}\ncoef <- mod.logit.h0$coefficients\n\n# Effect of income on LDL for a person's average age, sbp, and tobacco usage\n\nxx = c(1, mean(df$ldl), mean(df$sbp), mean(df$tobacco), mean(df$age))\n\nz = coef[1]*xx[1]+ coef[3]*xx[3] + coef[4]*xx[4] + coef[5]*xx[5]\n\nx <- df$ldl\n\n# Reproduce the graph overlaying the same result from the linear model as a comparison\ncurve(expr = exp(z + coef[2]*x)/(1+exp(z + coef[2]*x)), \n    xlim = c(min(df$ldl), max(df$ldl)), \n    ylim = c(0,2),\n    col = \"blue\", main = expression(pi == frac(e^{z + coef[inc]*ldl}, 1+e^{z+coef[inc]*ldl})),\n      xlab =  expression(cholesterol), ylab = expression(pi))\n\n# par(new=TRUE)\n\n\nlm.coef <- mod.linear$coefficients\nlm.z <- lm.coef[1]*xx[1] + lm.coef[3]*xx[3] + lm.coef[4]*xx[4] + lm.coef[5]*xx[5] \nlines(df$ldl, lm.z + lm.coef[2]*x,col=\"green\")\n\n```\n\n\n\\newpage\n\n### Final Report\n\n- Display both estimated linear and logistic models in a regression table. Is there any significant difference between their results?\n\n```{r message = FALSE ,error = TRUE}\n# uncomment and run the code\n\nstargazer(mod.linear, mod.logit.h0, type = \"text\", omit.stat = \"f\",\n                   star.cutoffs = c(0.05, 0.01, 0.001), title = \"Table 1: The estimated relationship between heart disease and  risk factors\")\n\n\n```\n\n> **In both models, all the coefficients except blood pressure are statistically significant and positively associated with the probability of having heart disease. Also, LDL is the most correlated variable with the probability of heart disease in both models.**    \n\n\n# Terms\n\n```{r include=FALSE}\n# library(here)\n# library(readxl)\ndf <- read_excel(here(\"def.xlsx\"), sheet = \"wk1\")\n```\n\n\n```{r echo=FALSE}\n#| label: tbl-penguins-top10\n#| tbl-cap: First 10 Penguins\n\n#replacing NA with white space\ndf[is.na(df)] <- \"\" \n\ndf %>% kable(\"html\") %>% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"html-math-method":"katex","css":["style.css"],"output-file":"wk2.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.42","title":"Logistic regression","editor_options":{"chunk_output_type":"console"},"theme":{"light":"cosmo","dark":["cosmo","theme-dark.scss"]}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}