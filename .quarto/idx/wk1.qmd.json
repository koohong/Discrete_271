{"title":"Binary groups","markdown":{"yaml":{"title":"Binary groups ","format":{"html":{"toc":true,"html-math-method":"katex","css":"style.css","theme":{"light":"cosmo","dark":["cosmo","theme-dark.scss"]}}},"execute":{"echo":false},"editor_options":{"chunk_output_type":"console"}},"headingText":"Brief review on terms","containsRefs":false,"markdown":"\n\n[Source](https://quarto.org/docs/get-started/hello/rstudio.html)\n\n```{=html}\n<style>\n.table-hover > tbody > tr:hover { \n  background-color: #f4f442;\n}\n</style>\n```\n\n```{r}\n#| message: false\n#| warning: false\n#| include: false\nlibrary(here)\nsource(here(\"source\",\"get_lib.R\"))\n```\n\n\n- `Distribution` is a `function` and has `parameters`.\n  - We can estimate parameters of the distribution based on `sample`\n  \n- In `MLR` (multiple linear regression), we estimate the parameter of the function using `OLS` and predict `target`.\n  - The target of model whose parameters were estimated using OLS follows `normal distribution`\n  - Statistical significance of the parameters were evaluated using `t-statistic` and `p-value` associated with it.\n  - We also talked about the physical meaning of the estimated coefficient of the parameters.\n  - In OLS, we had several assumptions.\n  \n- `Contingency table`, `joint, marginal`, `conditional probabilities` and talked about independent events.\n  \n# Overview\n\n- CH1 first talks about `Bernoulli` and `Binomial distribution` and how we can estimate thir `parameters` using `MLE`.\n- Likelihood Function (aka likelihood), likelihood ratio test (LRT), and transformed LRT are statistics that we will be using a lot\n  - This statistic follows $\\chi^2$ distribution\n- You may also want to remember another statistic called `score test` (see page 17), `odd` and `odd ratio`\n  - Will be using `odd` to develop model \n  - and `odd ratio` to evaluate the meaning of the parameters in the model\n\n# Distributions\n\n- Reading `CB` CH1 (skip 1.2.6 and 1.2.7)\n\n## Bernoulli distribution\n\n### parameters\n\n-   Y is 0 or 1. Has one parameter $\\pi$\n-   $E[Y] = \\pi$, $\\text{V}[Y]=\\pi(1-\\pi)$\n\n### PMF\n\n$$P(Y=y) = \\pi^y(1-\\pi)^{1-y}$$ \\\n\n## Binomial distribution\n\n### parameters\n\n-   Multiple bernoulli trials. Suppose we have `n` trials\n-   $E[Y] = n\\pi$, $\\text{V}[Y]= n\\pi(1-\\pi)$\n\n### PMF\n\n$$P(W=w) = {n \\choose w} \\pi^w(1-\\pi)^{1-w}$$\n\n### Likelihood Function\n\n- What is Likelihood function?\n\n$$\\begin{align}\nL(\\pi|y_1,y_2,....y_n) &= P(Y_1 = y_1)\\cdot ... P(Y_n=y_n)\\\\\n&=\\pi^w(1-\\pi)^{n-w}\n\\end{align}$$\n\n### MLE\n\n-   Maximum Likelihood Estimation\n\n-   Suppose your aunt sends you an `unfair coin`, but you forgot what your order was.\n\n    -   To figure out the probability of success, you flip the coin three times and collect the following data (we are defining heads as success here):\n\n> HTH\n\n-   For a hypothesized Bernoulli parameter $\\pi$, what is the likelihood of the data? Your answer should be a function of $\\pi$.\n\n-   likelihood function is:\n\n$$\n\\begin{aligned}\n  L(\\pi|x_1,x_2,x_3) &= P(X_1 = x_1,X_1 = x_2, X_3 =  x_{3}) \\\\\n       &= \\prod_{i=1}^{3} P({X=x_i}) \\\\\n       &= \\prod_{i=1}^{3} \\pi^{x_i}(1-\\pi)^{1-x_i} \\\\\n       &= \\pi^{\\sum_{i=1}^{3} x_i} (1-\\pi)^{\\sum_{i=1}^{3}(1- x_i)} \\\\\n\\end{aligned}\n$$\n\n-   log of the likelihood function\n\n$$\n\\begin{aligned}\n  \\text{Log}[L(\\pi|x_1,x_2,x_3)] &= \\left( {\\sum_{i=1}^{3} x_i} \\right)\\text{log}(\\pi) + \\left({\\sum_{i=1}^{3}(1- x_i)} \\right) \\text{log}(1-\\pi)\\\\\n\\end{aligned}\n$$\n\n-   What is the natural log of the likelihood of the data? Write an R function that computes the log likelihood.\n\n```{r}\nloglikelihood <- function(pi) {\n    data <- c(1, 0, 1)\n    return(sum(data==1)*log(pi) + (sum(data==0)*log(1-pi)))}\n```\n\n-   Graph your function and visually estimate what the maximum likelihood estimate for $\\pi$ is.\n\n```{r}\nprob = seq(0, 1, by=.001)\nd1 <- data.frame(probability = prob, log_likelihood = loglikelihood(prob))\nggplot(d1, aes(x = probability, y = log_likelihood)) +\ngeom_line() +\ngeom_vline(aes(xintercept = c(2/3)), color = \"red\", linetype = \"dashed\")+\nscale_x_continuous(breaks = seq(0, 1, by = 0.1)) +\nlabs(title = \"Computed Log-Likelihood for Bernoulli Parameter\",\n     x = quote(pi), \n     y = 'log likelihood'\n     ) \n\n```\n\n-   We know that MLE of $\\pi$ is: $$\\hat{\\pi} =\\frac{\\sum x_i}{N} = \\frac{2}{3} $$\n\n-   and in this question, it's:\n\n$$\\hat{\\pi} = \\frac{2}{3} $$\n\n-   In the plot, we can see that log-likelihood has a single peak at 2/3.\n\n# One binary group\n\n## Interval \n\n-   Given observation, knowing distribution, we are estimating the parameter of the function.\n\n-   Since this is an estimator, it will change each time we collect sample.\n\n-   We talked about Wald confidence interval, similar to what we talked about in W203.\n\n    -   It rely on the underlying normal distribution approximation for the maximum likelihood estimator. (see page 11)\n\n```{r, fig.height=8,fig.width=8}\n\n# Initial settings and calculations\nalpha <- 0.05\nn <- 40\nw <- 0:n\npi.hat <- w/n\npi.tilde <- (w + qnorm(p = 1-alpha/2)^2 /2) / (n+qnorm(1-alpha/2)^2)\n\n# Wald\nvar.wald <- pi.hat*(1-pi.hat)/n\nlower.wald <- pi.hat - qnorm(p = 1-alpha/2) * sqrt(var.wald)\nupper.wald <- pi.hat + qnorm(p = 1-alpha/2) * sqrt(var.wald)\n\n# Agresti-Coull\nlower.AC <- pi.tilde - qnorm(p = 1-alpha/2) * sqrt(pi.tilde*(1-pi.tilde) / (n+qnorm(1-alpha/2)^2))\nupper.AC <- pi.tilde + qnorm(p = 1-alpha/2) * sqrt(pi.tilde*(1-pi.tilde) / (n+qnorm(1-alpha/2)^2))\n\n# Wilson\nlower.wilson <- pi.tilde - qnorm(p = 1-alpha/2) * sqrt(n) / (n+qnorm(1-alpha/2)^2) * sqrt(pi.hat*(1-pi.hat) + qnorm(1-alpha/2)^2/(4*n))\nupper.wilson <- pi.tilde + qnorm(p = 1-alpha/2) * sqrt(n) / (n+qnorm(1-alpha/2)^2) * sqrt(pi.hat*(1-pi.hat) + qnorm(1-alpha/2)^2/(4*n))\n\n# Clopper-Pearson - This is a little more complicated due to the y = 0 and n cases\n  lower.CP <- numeric(n+1)  # This initializes a vector to save the lower bounds into\n  upper.CP <- numeric(n+1)  # This initializes a vector to save the upper bounds into\n\n  # y = 0\n  w0<-0  # Set here for emphasis\n  lower.CP[1] <- 0\n  upper.CP[1] <- qbeta(p = 1-alpha/2, shape1 = w0+1, shape2 = n-w0)\n\n  # y = n\n  wn <-n  # Set here for emphasis\n  lower.CP[n+1] <- qbeta(p = alpha/2, shape1 = wn, shape2 = n-wn+1)\n  upper.CP[n+1] <- 1\n  \n  # y = 1, ..., n-1\n  w.new <- 1:(n-1)\n  lower.CP[2:n] <- qbeta(p = alpha/2, shape1 = w.new, shape2 = n-w.new+1)\n  upper.CP[2:n] <- qbeta(p = 1-alpha/2, shape1 = w.new+1, shape2 = n-w.new)\n\n\n# All pi's\npi.seq <- seq(from = 0.001, to = 0.999, by = 0.0005)\n# pi.seq<-0.16 #Testing\n# pi.seq<-seq(from = 0.1, to = 0.9, by = 0.1) #Testing\n\n# Save true confidence levels in a matrix\nsave.true.conf <- matrix(data = NA, nrow = length(pi.seq), ncol = 5)\n\n# Create counter for the loop\ncounter <- 1\n\n# Loop over each pi that the true confidence level is calculated on\nfor(pi in pi.seq) {\n\n  pmf <- dbinom(x = w, size = n, prob = pi)\n\n  # Wald\n  save.wald <- pi>lower.wald & pi<upper.wald  # Check if pi is within interval\n  # Could use ifelse() too:\n  # save.wald <- ifelse(test = pi>lower.wald, yes = ifelse(test = pi<upper.wald, yes = 1, no = 0), no = 0)\n  wald <- sum(save.wald*pmf)\n\n  # Agresti-Coull\n  save.AC <- pi>lower.AC & pi<upper.AC\n  # ifelse(test = pi>lower.AC, yes = ifelse(test = pi<upper.AC, yes = 1, no = 0), no = 0)\n  AC <- sum(save.AC*pmf)\n\n  # Wilson\n  save.wilson <- pi>lower.wilson & pi<upper.wilson\n  # save.wilson <- ifelse(test = pi>lower.wilson, yes = ifelse(test = pi<upper.wilson, yes = 1, no = 0), no = 0)\n  wilson <- sum(save.wilson*pmf)\n\n  # Clopper-Pearson\n  save.CP <- pi>lower.CP & pi<upper.CP\n  # save.CP <- ifelse(test = pi>lower.CP, yes = ifelse(test = pi<upper.CP, yes = 1, no = 0), no = 0)\n  CP <- sum(save.CP*pmf)\n\n  save.true.conf[counter,] <- c(pi, wald, AC, wilson, CP)\n  counter <- counter+1\n  \n}\n  \n\n# Plots\n# dev.new(width = 7, height = 6, pointsize = 12)\n# pdf(file = \"c:\\\\figures\\\\Figure1.3.pdf\", width = 7, height = 6, colormodel = \"cmyk\")   # Create plot for book\npar(mfrow = c(2,2))  # 2x2 plotting grid\nplot(x = save.true.conf[,1], y = save.true.conf[,2], main = \"Wald\", xlab = expression(pi),\n  ylab = \"True confidence level\", type = \"l\", ylim = c(0.85,1))\nabline(h = 1-alpha, lty = \"dotted\")\nsegments(x0 = 0.157, y0 = 0, x1 = 0.157,\n  y1 = save.true.conf[save.true.conf[,1]==0.157,2], lty = \"dotdash\")\nsegments(x0 = -1, y0 = save.true.conf[save.true.conf[,1]==0.157,2], x1 = 0.157,\n  y1 = save.true.conf[save.true.conf[,1]==0.157,2], lty = \"dotdash\")\n\nplot(x = save.true.conf[,1], y = save.true.conf[,3], main = \"Agresti-Coull\", xlab = expression(pi),\n  ylab = \"True confidence level\", type = \"l\", ylim = c(0.85,1))\nabline(h = 1-alpha, lty = \"dotted\")\nplot(x = save.true.conf[,1], y = save.true.conf[,4], main = \"Wilson\", xlab = expression(pi),\n  ylab = \"True confidence level\", type = \"l\", ylim = c(0.85,1))\nabline(h = 1-alpha, lty = \"dotted\")\nplot(x = save.true.conf[,1], y = save.true.conf[,5], main = \"Clopper-Pearson\", xlab = expression(pi),\n  ylab = \"True confidence level\", type = \"l\", ylim = c(0.85,1))\nabline(h = 1-alpha, lty = \"dotted\")\n# dev.off()  # Create plot for book\n\n# Pi = 0.157\n# save.true.conf[save.true.conf[,1]==0.157, ]\n# While AC and Wilson have same true confidence levels at pi=0.157, this will not always be the case\n# sum(save.true.conf[,3] != save.true.conf[,4])  # Number of differences\n# length(pi.seq)  # Number of true confidence levels\n\n```\n\n\n## Hypothesis test\n\n-   When only one simple parameter is of interest, such as $\\pi$ here, we generally prefer confidence intervals over hypothesis tests, because the interval gives a range of possible parameter values.\n\n-   We can typically infer that a hypothesized value for a parameter can be rejected if it does not lie within the confidence interval for the parameter.\n\n-   However, there are situations where a `fixed known value` of $\\pi$, say $\\pi_0$ , is of special interest, leading to a formal hypothesis test of\n\n$$H_0 : \\pi = \\pi_0$$ $$H_a : \\pi  \\not= \\pi_0$$\n\n-   Situations where a `fixed known value` of $\\pi$, say $\\pi_0$ , is of special interest, leading to a formal hypothesis test of\n\n$$H_0 : \\pi = \\pi_0$$ $$H_a : \\pi  \\not= \\pi_0$$\n\n- See page 11 and 12\n\n\n### Hypothesis testing using Wilson interval, score test statistic\n\n![](image/wilson.PNG){fig-align=\"center\" width=\"400\"}\n\nThis statistic is called the `score test statistic`\n\n$$Z_0 = \\frac{\\hat{\\pi}-\\pi_0}{\\sqrt{\\pi_0(1-\\pi_0)/n}}$$ \n\n- When $H_0$ is true, $Z_0$ should have approximately standard normal distribution.\n\n-   The book recommend using the `score test` when performing a test for $\\pi$ (see page 17)\n\n\n\n### Hypothesis testing using likelihood ratio test (LRT)\n\n![](image/LRT.png){fig-align=\"center\" width=\"400\"}\n\n-   LRT statistics look like this.\n\n$$\\Lambda = \\frac{\\text{Maximum of likelihood function udner } H_0}{\\text{Maximum of likelihood function udner } H_A}$$\n\n- LRT is used to calculate confidence intervals in some more complicated contexts where better intervals are not available. \n\n- This interval is better than the wald interval in most problems. (see page 10)\n\n### Hypothesis testing using Transformed LRT\n\n![](image/transformed LRT.webp){fig-align=\"center\" width=\"400\"}\n\n$$-2\\text{log}(\\Lambda)$$\n\n- Above statistic is called `transformed LRT` statistisc and have an approximate $\\chi^2$ distribution.\n\n# Two binary groups\n\n- Consider Bernoulli trial is measured on units that can be classified into groups.\n  - Female and male\n  - Fresh and Salt-Water fish\n  - Larry birds free throw [LINK](https://docs.google.com/spreadsheets/d/1YyO_sX1hIn7siATwF7L-mE8OfHktGX2w/edit?gid=50379254#gid=50379254)\n    - Basketball fans and commentators often speculate that the result of a second free throw might depend on the result of the first.\n  - Salk vaccine clinical trial. [LINK](https://docs.google.com/spreadsheets/d/1YyO_sX1hIn7siATwF7L-mE8OfHktGX2w/edit?gid=50379254#gid=50379254)\n    - Randomized experiment. \n    - 57 out of 200,745 developed polio during the study period\n    - 142 out of 201,220 developed polio during the placebo group.\n    - `Does the vaccine help to prevent polio?`\n\n- We have two random variables $Y_1$ and $Y_2$ and their outcome is independent of each other.\n\n- We have `TWO BINARY VARIABLES`\n\n## Interval test\n\n- Wald Confidence interval using normal approximation.\n- Book recommend using the Agresti-Caffo method\n\n### Larry Bird’s free throw shooting\n\n```{r}\n#| echo: true\nc.table <- array(data = c(251, 48, 34, 5), dim = c(2,2),\ndimnames = list(First = c(\"made\", \"missed\"), Second = c(\"made\", \"missed\")))\nc.table\n\n#conditional probabilities\npi.hat.table <- c.table/rowSums(c.table)\n\n#get the pi estimates\npi.hat1 <- pi.hat.table[1,1] \npi.hat2 <- pi.hat.table[2,1]\n\n#set type I error\nalpha <- 0.05\n```\n\n- `Wald Confidence interval`\n\n```{r}\n#| echo: true\n#########################\n#wald CI\n#########################\nvar.wald <- pi.hat1*(1-pi.hat1) / sum(c.table[1,]) +\npi.hat2*(1-pi.hat2) / sum(c.table[2,])\n\npi.hat1 - pi.hat2 + qnorm(p = c(alpha/2, 1-alpha /2)) *\nsqrt(var.wald)\n\n```\n\n- `Agresti-Caffo confidence interval`\n\n```{r}\n#| echo: true\n#########################\n# Agresti-Caffo\n#########################\npi.tilde1 <- (c.table[1,1] + 1) / (sum(c.table[1,]) + 2) \npi.tilde2 <- (c.table[2,1] + 1) / (sum(c.table[2,]) + 2) \nvar.AC <- pi.tilde1*(1-pi.tilde1) / (sum(c.table[1,]) + 2) +\n\npi.tilde2*(1-pi.tilde2) / (sum(c.table[2,]) + 2)\npi.tilde1 - pi.tilde2 + qnorm(p = c(alpha/2, 1-alpha /2)) *sqrt(var.AC)\n```\n\n- Because these interval contains 0, we cannot reject $H_0$\n\n\n## Hypothesis test\n\n$$H_0: \\pi_1 - \\pi_2 = 0$$\n$$H_0: \\pi_1 - \\pi_2 \\not= 0$$\n\n\n### Person chi-square test \n\n- So, this is the test for TWO BINARY VARIABLES.\n\n- Create a statistic comparing the difference between what was observed and what was predicted under $H_0$ that there is no difference.\n\n- This statistic following $\\Chi^2$ with $n_1$ and $n_2$ degress of freedom.\n\n$$\\chi^2 = \\sum_{j=1}^2\\frac{(w_j-n_j\\bar{\\pi})^2}{n_j\\bar{\\pi}(1-\\bar{\\pi})}$$\n\n### LRT test\n\n$$-2\\text{log}({\\Lambda})= ... $$\n- if $-2\\text{log}({\\Lambda}) > \\chi^2$ with some degrees of freedom, Reject $H_0$\n\n### Score test\n\n- Use score statistic to perform test.  Score test performs the best when the same size is small.\n\n```{r}\n#| echo: true\nprop.test(x = c.table , conf.level = 0.95, correct = FALSE)\n```\n\n- `correct = FALSE` argument value guarantees that the test statistic is calculated as shown $Z_0$.\n\n-   The `prevalence of a disease` is the proportion of a population that is afflicted with that disease\n\n\n## Relative Risk\n\n- The problem with basing inferences on $\\pi_1 - \\pi_2$ is that it measures a quantity whose meaning changes depending on the value of $\\pi_1,\\pi_2$\n\n1. $\\pi_1 = 0.51 \\text{ and }  \\pi_2 = 0.5$\n2. $\\pi_1 = 0.011 \\text{ and }  \\pi_2 = 0.001$\n\n- In both cases, 1. $\\pi_1 - \\pi_2 = 0.01$. \n- In case (1), this change is small compare to 1. $\\pi_1\\text{ and } \\pi_2$\n- In case (2), this change is 11 times the chance of $\\pi_2$.  Suppose $\\pi_2$ is the chance of nonsmoking population getting a disease, then, the chance is 11 times the chance of nonsmoking population getting a disease.\n\n- To capture this information, we use another statistic called `relative risk`\n\n$$\\text{RR} = \\frac{\\pi_1}{\\pi_2}=\\frac{0.011}{0.001}$$\n\n- Smokers are 11 times `as likely` to have the disease than nonsmokers\n\n- Smokers are 10 times `more likely` to have that disease than nonsmokers. (see page 38)\n\n- $\\hat{RR}$ is MLE and using normal approximation is rather poor for MLE and not recommended.  \n  - Use normal approximation on $\\text{log}(\\hat{RR})$\n\n\n### RR of Salk vaccine clinical trial\n\n```{r}\n#| echo: true\nc.table <- array(data = c(57, 142, 200688, 201087), dim =\nc(2,2), dimnames = list(Treatment = c(\"vaccine\", \"placebo\"), Result = c(\"polio\", \"polio free\")))\n\nc.table\n\n#calculate conditional probability\npi.hat.table <- c.table/rowSums(c.table) \npi.hat.table\n\n#estimated parameters to be compared\npi.hat1 <- pi.hat.table[1,1] \npi.hat2 <- pi.hat.table[2,1]\n\nRR <- pi.hat1/pi.hat2\nRR\n```\n\n- The `estimated` probability of contracting polio is 0.4 times `as likely` for the vaccine group that for thh placebo group.\n\n```{r}\n#| echo: true\n#set type 1 error\nalpha <- 0.05\nn1 <- sum(c.table[1,]) \nn2 <- sum(c.table[2,])\n\nvar.log.rr <- (1-pi.hat1)/(n1*pi.hat1) + (1-pi.hat2)/(n2*pi.hat2)\n\nci <- exp(log(pi.hat1/pi.hat2) + qnorm(p = c(alpha/2,\n1-alpha/2)) * sqrt(var.log.rr))\n\n\nci\n```\n\n# Odds \n\n## Odds\n\n- Odds can also be used as a similar measure as relative risk.\n- Odds are defined as the probability of a success divided by the probability of a failure.\n\n$$\\text{Odds} = \\frac{\\pi}{1-\\pi} = \\frac{0.1}{1-0.1}= \\frac{0.1}{0.9}$$\n- This will be referred to as `9-to-1` odds against.\n\n- Odds have no upper limit unlike probabilities.\n- Like RR, odds are estimated with MLE.\n\n\n## Odds ratios\n\n- Determining whether or not an `odds ratio` is equal to 1, greater than 1 or less than 1 is often of interest. \n\n$$\\text{OR} = \\frac{\\text{Odds}_1}{\\text{Odds}_2}$$\n\n- The estimated odds of a success is $\\hat{\\text{OR}}$ times as larag as `in group 1` than in `group 2`\n\n- Since OR is a statistic, each time you get a sample and estimate, you will get difference value. You can calculate CI of this estimate.\n\n\n```{r include=FALSE}\n# library(here)\n# library(readxl)\ndf <- read_excel(here(\"def.xlsx\"), sheet = \"or\")\n```\n\n```{r echo=FALSE}\n#| label: tbl-penguins-top10\n#| tbl-cap: First 10 Penguins\n\n#replacing NA with white space\ndf[is.na(df)] <- \"\" \n\ndf %>% kable(\"html\") %>% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n### OR of Salk vaccine clinical trial\n\n```{r}\n#| echo: true\nOR.hat <- c.table[1,1] * c.table[2,2] / (c.table[2,1] *\nc.table[1,2])\nround(OR.hat , 4)\n\n#get confidence interval\nalpha <- 0.05 \n\nvar.log.or <- 1/c.table[1,1] + 1/c.table[1,2] + 1/c.table[2,1] + 1/c.table[2,2]\n\nOR.CI <- exp(log(OR.hat) + qnorm(p = c(alpha/2, 1-alpha /2))*sqrt(var.log.or))\n\nround(OR.CI , 2)\n\n```\n\n\n# Matched pair data\n\n- $w_1$ and $w_2$ were independent random variables in the examples shown before. [LINK](https://docs.google.com/spreadsheets/d/1YyO_sX1hIn7siATwF7L-mE8OfHktGX2w/edit?gid=50379254#gid=50379254)\n\n- There are other situations where the two probabilities being compared are `dependent` random variables.\n\n- This occurs with `matched pair` data, where two binary response observations, $X$ and $Y$ are made on each `sample unit`\n\n- The desired comparision between success probabilities for X and Y involves two correlated statistics.\n\n## Prostate cancer diagnosis procedures\n\n- (P44) Zhou and Qin (2005) discuss a study that was used to compare the diagnostic accuracy of magnetic resonance imaging (`MRI`) and `ultrasound` in patients who had been established as having `localized prostate cancer` by a gold standard test.\n\n### Hypothesis testing \n\n- This is comparing marginal probabilities\n\n$$H_0: \\pi_{+1}=\\pi_{1+} $$\n$$H_a: \\pi_{+1}\\not= \\pi_{1+} $$\n\n- We can use `wald test statistic` (the book uses $Z_0$ notation which was used for score statistic) or `McNemar's test statistic`, `M` which has approximately $\\chi^2$ distribution for large samples. \n  - Reject $H_0$ when `M` > $\\chi^2_{..}$\n\n\n```{r}\n#| echo: true\n#get marginal distribution \nn <- sum(c.table) \npi.hat.plus1 <- sum(c.table[,1])/n \npi.hat.1plus <- sum(c.table[1,])/n\n\n#evaluate the difference by \n#subtracting the sample statistics\ndata.frame(pi.hat.plus1 , pi.hat.1plus , \n           diff = pi.hat.plus1 - pi.hat.1plus)\n```\n\n```{r}\n#| echo: true\nc.table <- array(data = c(4, 3, 6, 3), dim = c(2,2), dimnames =\nlist(MRI = c(\"Localized\", \"Advanced\"), Ultrasound =\nc(\"Localized\", \"Advanced\"))) \n\nc.table\n\nmcnemar.test(x = c.table , correct = FALSE)\n\n```\n\n\n### Interval \n\n- `Wald confidence interval`\n\n```{r}\n##library(PropCIs)\n## wald confidence interval\ndiffpropci.Wald.mp(b = c.table[1,2], c = c.table[2,1], n =\nsum(c.table), conf.level = 0.95)\n```\n\n- `Agresti-Min confidence interval`\n  - You also get difference sample esitmate.\n\n```{r}\ndiffpropci.mp(b = c.table[1,2], c = c.table[2,1], n =\nsum(c.table), conf.level = 0.95)\n```\n\n\n# Larger contingency tables\n\n- There are many instances where more than two groups exist (i.e., there are more than two rows within a contingency table)\n\n- Once more rows are added to a contingency table, it is often easier to perform the analysis with a binary regression models as covered in Chapter 2. \n\n","srcMarkdownNoYaml":"\n\n[Source](https://quarto.org/docs/get-started/hello/rstudio.html)\n\n```{=html}\n<style>\n.table-hover > tbody > tr:hover { \n  background-color: #f4f442;\n}\n</style>\n```\n\n```{r}\n#| message: false\n#| warning: false\n#| include: false\nlibrary(here)\nsource(here(\"source\",\"get_lib.R\"))\n```\n\n# Brief review on terms\n\n- `Distribution` is a `function` and has `parameters`.\n  - We can estimate parameters of the distribution based on `sample`\n  \n- In `MLR` (multiple linear regression), we estimate the parameter of the function using `OLS` and predict `target`.\n  - The target of model whose parameters were estimated using OLS follows `normal distribution`\n  - Statistical significance of the parameters were evaluated using `t-statistic` and `p-value` associated with it.\n  - We also talked about the physical meaning of the estimated coefficient of the parameters.\n  - In OLS, we had several assumptions.\n  \n- `Contingency table`, `joint, marginal`, `conditional probabilities` and talked about independent events.\n  \n# Overview\n\n- CH1 first talks about `Bernoulli` and `Binomial distribution` and how we can estimate thir `parameters` using `MLE`.\n- Likelihood Function (aka likelihood), likelihood ratio test (LRT), and transformed LRT are statistics that we will be using a lot\n  - This statistic follows $\\chi^2$ distribution\n- You may also want to remember another statistic called `score test` (see page 17), `odd` and `odd ratio`\n  - Will be using `odd` to develop model \n  - and `odd ratio` to evaluate the meaning of the parameters in the model\n\n# Distributions\n\n- Reading `CB` CH1 (skip 1.2.6 and 1.2.7)\n\n## Bernoulli distribution\n\n### parameters\n\n-   Y is 0 or 1. Has one parameter $\\pi$\n-   $E[Y] = \\pi$, $\\text{V}[Y]=\\pi(1-\\pi)$\n\n### PMF\n\n$$P(Y=y) = \\pi^y(1-\\pi)^{1-y}$$ \\\n\n## Binomial distribution\n\n### parameters\n\n-   Multiple bernoulli trials. Suppose we have `n` trials\n-   $E[Y] = n\\pi$, $\\text{V}[Y]= n\\pi(1-\\pi)$\n\n### PMF\n\n$$P(W=w) = {n \\choose w} \\pi^w(1-\\pi)^{1-w}$$\n\n### Likelihood Function\n\n- What is Likelihood function?\n\n$$\\begin{align}\nL(\\pi|y_1,y_2,....y_n) &= P(Y_1 = y_1)\\cdot ... P(Y_n=y_n)\\\\\n&=\\pi^w(1-\\pi)^{n-w}\n\\end{align}$$\n\n### MLE\n\n-   Maximum Likelihood Estimation\n\n-   Suppose your aunt sends you an `unfair coin`, but you forgot what your order was.\n\n    -   To figure out the probability of success, you flip the coin three times and collect the following data (we are defining heads as success here):\n\n> HTH\n\n-   For a hypothesized Bernoulli parameter $\\pi$, what is the likelihood of the data? Your answer should be a function of $\\pi$.\n\n-   likelihood function is:\n\n$$\n\\begin{aligned}\n  L(\\pi|x_1,x_2,x_3) &= P(X_1 = x_1,X_1 = x_2, X_3 =  x_{3}) \\\\\n       &= \\prod_{i=1}^{3} P({X=x_i}) \\\\\n       &= \\prod_{i=1}^{3} \\pi^{x_i}(1-\\pi)^{1-x_i} \\\\\n       &= \\pi^{\\sum_{i=1}^{3} x_i} (1-\\pi)^{\\sum_{i=1}^{3}(1- x_i)} \\\\\n\\end{aligned}\n$$\n\n-   log of the likelihood function\n\n$$\n\\begin{aligned}\n  \\text{Log}[L(\\pi|x_1,x_2,x_3)] &= \\left( {\\sum_{i=1}^{3} x_i} \\right)\\text{log}(\\pi) + \\left({\\sum_{i=1}^{3}(1- x_i)} \\right) \\text{log}(1-\\pi)\\\\\n\\end{aligned}\n$$\n\n-   What is the natural log of the likelihood of the data? Write an R function that computes the log likelihood.\n\n```{r}\nloglikelihood <- function(pi) {\n    data <- c(1, 0, 1)\n    return(sum(data==1)*log(pi) + (sum(data==0)*log(1-pi)))}\n```\n\n-   Graph your function and visually estimate what the maximum likelihood estimate for $\\pi$ is.\n\n```{r}\nprob = seq(0, 1, by=.001)\nd1 <- data.frame(probability = prob, log_likelihood = loglikelihood(prob))\nggplot(d1, aes(x = probability, y = log_likelihood)) +\ngeom_line() +\ngeom_vline(aes(xintercept = c(2/3)), color = \"red\", linetype = \"dashed\")+\nscale_x_continuous(breaks = seq(0, 1, by = 0.1)) +\nlabs(title = \"Computed Log-Likelihood for Bernoulli Parameter\",\n     x = quote(pi), \n     y = 'log likelihood'\n     ) \n\n```\n\n-   We know that MLE of $\\pi$ is: $$\\hat{\\pi} =\\frac{\\sum x_i}{N} = \\frac{2}{3} $$\n\n-   and in this question, it's:\n\n$$\\hat{\\pi} = \\frac{2}{3} $$\n\n-   In the plot, we can see that log-likelihood has a single peak at 2/3.\n\n# One binary group\n\n## Interval \n\n-   Given observation, knowing distribution, we are estimating the parameter of the function.\n\n-   Since this is an estimator, it will change each time we collect sample.\n\n-   We talked about Wald confidence interval, similar to what we talked about in W203.\n\n    -   It rely on the underlying normal distribution approximation for the maximum likelihood estimator. (see page 11)\n\n```{r, fig.height=8,fig.width=8}\n\n# Initial settings and calculations\nalpha <- 0.05\nn <- 40\nw <- 0:n\npi.hat <- w/n\npi.tilde <- (w + qnorm(p = 1-alpha/2)^2 /2) / (n+qnorm(1-alpha/2)^2)\n\n# Wald\nvar.wald <- pi.hat*(1-pi.hat)/n\nlower.wald <- pi.hat - qnorm(p = 1-alpha/2) * sqrt(var.wald)\nupper.wald <- pi.hat + qnorm(p = 1-alpha/2) * sqrt(var.wald)\n\n# Agresti-Coull\nlower.AC <- pi.tilde - qnorm(p = 1-alpha/2) * sqrt(pi.tilde*(1-pi.tilde) / (n+qnorm(1-alpha/2)^2))\nupper.AC <- pi.tilde + qnorm(p = 1-alpha/2) * sqrt(pi.tilde*(1-pi.tilde) / (n+qnorm(1-alpha/2)^2))\n\n# Wilson\nlower.wilson <- pi.tilde - qnorm(p = 1-alpha/2) * sqrt(n) / (n+qnorm(1-alpha/2)^2) * sqrt(pi.hat*(1-pi.hat) + qnorm(1-alpha/2)^2/(4*n))\nupper.wilson <- pi.tilde + qnorm(p = 1-alpha/2) * sqrt(n) / (n+qnorm(1-alpha/2)^2) * sqrt(pi.hat*(1-pi.hat) + qnorm(1-alpha/2)^2/(4*n))\n\n# Clopper-Pearson - This is a little more complicated due to the y = 0 and n cases\n  lower.CP <- numeric(n+1)  # This initializes a vector to save the lower bounds into\n  upper.CP <- numeric(n+1)  # This initializes a vector to save the upper bounds into\n\n  # y = 0\n  w0<-0  # Set here for emphasis\n  lower.CP[1] <- 0\n  upper.CP[1] <- qbeta(p = 1-alpha/2, shape1 = w0+1, shape2 = n-w0)\n\n  # y = n\n  wn <-n  # Set here for emphasis\n  lower.CP[n+1] <- qbeta(p = alpha/2, shape1 = wn, shape2 = n-wn+1)\n  upper.CP[n+1] <- 1\n  \n  # y = 1, ..., n-1\n  w.new <- 1:(n-1)\n  lower.CP[2:n] <- qbeta(p = alpha/2, shape1 = w.new, shape2 = n-w.new+1)\n  upper.CP[2:n] <- qbeta(p = 1-alpha/2, shape1 = w.new+1, shape2 = n-w.new)\n\n\n# All pi's\npi.seq <- seq(from = 0.001, to = 0.999, by = 0.0005)\n# pi.seq<-0.16 #Testing\n# pi.seq<-seq(from = 0.1, to = 0.9, by = 0.1) #Testing\n\n# Save true confidence levels in a matrix\nsave.true.conf <- matrix(data = NA, nrow = length(pi.seq), ncol = 5)\n\n# Create counter for the loop\ncounter <- 1\n\n# Loop over each pi that the true confidence level is calculated on\nfor(pi in pi.seq) {\n\n  pmf <- dbinom(x = w, size = n, prob = pi)\n\n  # Wald\n  save.wald <- pi>lower.wald & pi<upper.wald  # Check if pi is within interval\n  # Could use ifelse() too:\n  # save.wald <- ifelse(test = pi>lower.wald, yes = ifelse(test = pi<upper.wald, yes = 1, no = 0), no = 0)\n  wald <- sum(save.wald*pmf)\n\n  # Agresti-Coull\n  save.AC <- pi>lower.AC & pi<upper.AC\n  # ifelse(test = pi>lower.AC, yes = ifelse(test = pi<upper.AC, yes = 1, no = 0), no = 0)\n  AC <- sum(save.AC*pmf)\n\n  # Wilson\n  save.wilson <- pi>lower.wilson & pi<upper.wilson\n  # save.wilson <- ifelse(test = pi>lower.wilson, yes = ifelse(test = pi<upper.wilson, yes = 1, no = 0), no = 0)\n  wilson <- sum(save.wilson*pmf)\n\n  # Clopper-Pearson\n  save.CP <- pi>lower.CP & pi<upper.CP\n  # save.CP <- ifelse(test = pi>lower.CP, yes = ifelse(test = pi<upper.CP, yes = 1, no = 0), no = 0)\n  CP <- sum(save.CP*pmf)\n\n  save.true.conf[counter,] <- c(pi, wald, AC, wilson, CP)\n  counter <- counter+1\n  \n}\n  \n\n# Plots\n# dev.new(width = 7, height = 6, pointsize = 12)\n# pdf(file = \"c:\\\\figures\\\\Figure1.3.pdf\", width = 7, height = 6, colormodel = \"cmyk\")   # Create plot for book\npar(mfrow = c(2,2))  # 2x2 plotting grid\nplot(x = save.true.conf[,1], y = save.true.conf[,2], main = \"Wald\", xlab = expression(pi),\n  ylab = \"True confidence level\", type = \"l\", ylim = c(0.85,1))\nabline(h = 1-alpha, lty = \"dotted\")\nsegments(x0 = 0.157, y0 = 0, x1 = 0.157,\n  y1 = save.true.conf[save.true.conf[,1]==0.157,2], lty = \"dotdash\")\nsegments(x0 = -1, y0 = save.true.conf[save.true.conf[,1]==0.157,2], x1 = 0.157,\n  y1 = save.true.conf[save.true.conf[,1]==0.157,2], lty = \"dotdash\")\n\nplot(x = save.true.conf[,1], y = save.true.conf[,3], main = \"Agresti-Coull\", xlab = expression(pi),\n  ylab = \"True confidence level\", type = \"l\", ylim = c(0.85,1))\nabline(h = 1-alpha, lty = \"dotted\")\nplot(x = save.true.conf[,1], y = save.true.conf[,4], main = \"Wilson\", xlab = expression(pi),\n  ylab = \"True confidence level\", type = \"l\", ylim = c(0.85,1))\nabline(h = 1-alpha, lty = \"dotted\")\nplot(x = save.true.conf[,1], y = save.true.conf[,5], main = \"Clopper-Pearson\", xlab = expression(pi),\n  ylab = \"True confidence level\", type = \"l\", ylim = c(0.85,1))\nabline(h = 1-alpha, lty = \"dotted\")\n# dev.off()  # Create plot for book\n\n# Pi = 0.157\n# save.true.conf[save.true.conf[,1]==0.157, ]\n# While AC and Wilson have same true confidence levels at pi=0.157, this will not always be the case\n# sum(save.true.conf[,3] != save.true.conf[,4])  # Number of differences\n# length(pi.seq)  # Number of true confidence levels\n\n```\n\n\n## Hypothesis test\n\n-   When only one simple parameter is of interest, such as $\\pi$ here, we generally prefer confidence intervals over hypothesis tests, because the interval gives a range of possible parameter values.\n\n-   We can typically infer that a hypothesized value for a parameter can be rejected if it does not lie within the confidence interval for the parameter.\n\n-   However, there are situations where a `fixed known value` of $\\pi$, say $\\pi_0$ , is of special interest, leading to a formal hypothesis test of\n\n$$H_0 : \\pi = \\pi_0$$ $$H_a : \\pi  \\not= \\pi_0$$\n\n-   Situations where a `fixed known value` of $\\pi$, say $\\pi_0$ , is of special interest, leading to a formal hypothesis test of\n\n$$H_0 : \\pi = \\pi_0$$ $$H_a : \\pi  \\not= \\pi_0$$\n\n- See page 11 and 12\n\n\n### Hypothesis testing using Wilson interval, score test statistic\n\n![](image/wilson.PNG){fig-align=\"center\" width=\"400\"}\n\nThis statistic is called the `score test statistic`\n\n$$Z_0 = \\frac{\\hat{\\pi}-\\pi_0}{\\sqrt{\\pi_0(1-\\pi_0)/n}}$$ \n\n- When $H_0$ is true, $Z_0$ should have approximately standard normal distribution.\n\n-   The book recommend using the `score test` when performing a test for $\\pi$ (see page 17)\n\n\n\n### Hypothesis testing using likelihood ratio test (LRT)\n\n![](image/LRT.png){fig-align=\"center\" width=\"400\"}\n\n-   LRT statistics look like this.\n\n$$\\Lambda = \\frac{\\text{Maximum of likelihood function udner } H_0}{\\text{Maximum of likelihood function udner } H_A}$$\n\n- LRT is used to calculate confidence intervals in some more complicated contexts where better intervals are not available. \n\n- This interval is better than the wald interval in most problems. (see page 10)\n\n### Hypothesis testing using Transformed LRT\n\n![](image/transformed LRT.webp){fig-align=\"center\" width=\"400\"}\n\n$$-2\\text{log}(\\Lambda)$$\n\n- Above statistic is called `transformed LRT` statistisc and have an approximate $\\chi^2$ distribution.\n\n# Two binary groups\n\n- Consider Bernoulli trial is measured on units that can be classified into groups.\n  - Female and male\n  - Fresh and Salt-Water fish\n  - Larry birds free throw [LINK](https://docs.google.com/spreadsheets/d/1YyO_sX1hIn7siATwF7L-mE8OfHktGX2w/edit?gid=50379254#gid=50379254)\n    - Basketball fans and commentators often speculate that the result of a second free throw might depend on the result of the first.\n  - Salk vaccine clinical trial. [LINK](https://docs.google.com/spreadsheets/d/1YyO_sX1hIn7siATwF7L-mE8OfHktGX2w/edit?gid=50379254#gid=50379254)\n    - Randomized experiment. \n    - 57 out of 200,745 developed polio during the study period\n    - 142 out of 201,220 developed polio during the placebo group.\n    - `Does the vaccine help to prevent polio?`\n\n- We have two random variables $Y_1$ and $Y_2$ and their outcome is independent of each other.\n\n- We have `TWO BINARY VARIABLES`\n\n## Interval test\n\n- Wald Confidence interval using normal approximation.\n- Book recommend using the Agresti-Caffo method\n\n### Larry Bird’s free throw shooting\n\n```{r}\n#| echo: true\nc.table <- array(data = c(251, 48, 34, 5), dim = c(2,2),\ndimnames = list(First = c(\"made\", \"missed\"), Second = c(\"made\", \"missed\")))\nc.table\n\n#conditional probabilities\npi.hat.table <- c.table/rowSums(c.table)\n\n#get the pi estimates\npi.hat1 <- pi.hat.table[1,1] \npi.hat2 <- pi.hat.table[2,1]\n\n#set type I error\nalpha <- 0.05\n```\n\n- `Wald Confidence interval`\n\n```{r}\n#| echo: true\n#########################\n#wald CI\n#########################\nvar.wald <- pi.hat1*(1-pi.hat1) / sum(c.table[1,]) +\npi.hat2*(1-pi.hat2) / sum(c.table[2,])\n\npi.hat1 - pi.hat2 + qnorm(p = c(alpha/2, 1-alpha /2)) *\nsqrt(var.wald)\n\n```\n\n- `Agresti-Caffo confidence interval`\n\n```{r}\n#| echo: true\n#########################\n# Agresti-Caffo\n#########################\npi.tilde1 <- (c.table[1,1] + 1) / (sum(c.table[1,]) + 2) \npi.tilde2 <- (c.table[2,1] + 1) / (sum(c.table[2,]) + 2) \nvar.AC <- pi.tilde1*(1-pi.tilde1) / (sum(c.table[1,]) + 2) +\n\npi.tilde2*(1-pi.tilde2) / (sum(c.table[2,]) + 2)\npi.tilde1 - pi.tilde2 + qnorm(p = c(alpha/2, 1-alpha /2)) *sqrt(var.AC)\n```\n\n- Because these interval contains 0, we cannot reject $H_0$\n\n\n## Hypothesis test\n\n$$H_0: \\pi_1 - \\pi_2 = 0$$\n$$H_0: \\pi_1 - \\pi_2 \\not= 0$$\n\n\n### Person chi-square test \n\n- So, this is the test for TWO BINARY VARIABLES.\n\n- Create a statistic comparing the difference between what was observed and what was predicted under $H_0$ that there is no difference.\n\n- This statistic following $\\Chi^2$ with $n_1$ and $n_2$ degress of freedom.\n\n$$\\chi^2 = \\sum_{j=1}^2\\frac{(w_j-n_j\\bar{\\pi})^2}{n_j\\bar{\\pi}(1-\\bar{\\pi})}$$\n\n### LRT test\n\n$$-2\\text{log}({\\Lambda})= ... $$\n- if $-2\\text{log}({\\Lambda}) > \\chi^2$ with some degrees of freedom, Reject $H_0$\n\n### Score test\n\n- Use score statistic to perform test.  Score test performs the best when the same size is small.\n\n```{r}\n#| echo: true\nprop.test(x = c.table , conf.level = 0.95, correct = FALSE)\n```\n\n- `correct = FALSE` argument value guarantees that the test statistic is calculated as shown $Z_0$.\n\n-   The `prevalence of a disease` is the proportion of a population that is afflicted with that disease\n\n\n## Relative Risk\n\n- The problem with basing inferences on $\\pi_1 - \\pi_2$ is that it measures a quantity whose meaning changes depending on the value of $\\pi_1,\\pi_2$\n\n1. $\\pi_1 = 0.51 \\text{ and }  \\pi_2 = 0.5$\n2. $\\pi_1 = 0.011 \\text{ and }  \\pi_2 = 0.001$\n\n- In both cases, 1. $\\pi_1 - \\pi_2 = 0.01$. \n- In case (1), this change is small compare to 1. $\\pi_1\\text{ and } \\pi_2$\n- In case (2), this change is 11 times the chance of $\\pi_2$.  Suppose $\\pi_2$ is the chance of nonsmoking population getting a disease, then, the chance is 11 times the chance of nonsmoking population getting a disease.\n\n- To capture this information, we use another statistic called `relative risk`\n\n$$\\text{RR} = \\frac{\\pi_1}{\\pi_2}=\\frac{0.011}{0.001}$$\n\n- Smokers are 11 times `as likely` to have the disease than nonsmokers\n\n- Smokers are 10 times `more likely` to have that disease than nonsmokers. (see page 38)\n\n- $\\hat{RR}$ is MLE and using normal approximation is rather poor for MLE and not recommended.  \n  - Use normal approximation on $\\text{log}(\\hat{RR})$\n\n\n### RR of Salk vaccine clinical trial\n\n```{r}\n#| echo: true\nc.table <- array(data = c(57, 142, 200688, 201087), dim =\nc(2,2), dimnames = list(Treatment = c(\"vaccine\", \"placebo\"), Result = c(\"polio\", \"polio free\")))\n\nc.table\n\n#calculate conditional probability\npi.hat.table <- c.table/rowSums(c.table) \npi.hat.table\n\n#estimated parameters to be compared\npi.hat1 <- pi.hat.table[1,1] \npi.hat2 <- pi.hat.table[2,1]\n\nRR <- pi.hat1/pi.hat2\nRR\n```\n\n- The `estimated` probability of contracting polio is 0.4 times `as likely` for the vaccine group that for thh placebo group.\n\n```{r}\n#| echo: true\n#set type 1 error\nalpha <- 0.05\nn1 <- sum(c.table[1,]) \nn2 <- sum(c.table[2,])\n\nvar.log.rr <- (1-pi.hat1)/(n1*pi.hat1) + (1-pi.hat2)/(n2*pi.hat2)\n\nci <- exp(log(pi.hat1/pi.hat2) + qnorm(p = c(alpha/2,\n1-alpha/2)) * sqrt(var.log.rr))\n\n\nci\n```\n\n# Odds \n\n## Odds\n\n- Odds can also be used as a similar measure as relative risk.\n- Odds are defined as the probability of a success divided by the probability of a failure.\n\n$$\\text{Odds} = \\frac{\\pi}{1-\\pi} = \\frac{0.1}{1-0.1}= \\frac{0.1}{0.9}$$\n- This will be referred to as `9-to-1` odds against.\n\n- Odds have no upper limit unlike probabilities.\n- Like RR, odds are estimated with MLE.\n\n\n## Odds ratios\n\n- Determining whether or not an `odds ratio` is equal to 1, greater than 1 or less than 1 is often of interest. \n\n$$\\text{OR} = \\frac{\\text{Odds}_1}{\\text{Odds}_2}$$\n\n- The estimated odds of a success is $\\hat{\\text{OR}}$ times as larag as `in group 1` than in `group 2`\n\n- Since OR is a statistic, each time you get a sample and estimate, you will get difference value. You can calculate CI of this estimate.\n\n\n```{r include=FALSE}\n# library(here)\n# library(readxl)\ndf <- read_excel(here(\"def.xlsx\"), sheet = \"or\")\n```\n\n```{r echo=FALSE}\n#| label: tbl-penguins-top10\n#| tbl-cap: First 10 Penguins\n\n#replacing NA with white space\ndf[is.na(df)] <- \"\" \n\ndf %>% kable(\"html\") %>% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n### OR of Salk vaccine clinical trial\n\n```{r}\n#| echo: true\nOR.hat <- c.table[1,1] * c.table[2,2] / (c.table[2,1] *\nc.table[1,2])\nround(OR.hat , 4)\n\n#get confidence interval\nalpha <- 0.05 \n\nvar.log.or <- 1/c.table[1,1] + 1/c.table[1,2] + 1/c.table[2,1] + 1/c.table[2,2]\n\nOR.CI <- exp(log(OR.hat) + qnorm(p = c(alpha/2, 1-alpha /2))*sqrt(var.log.or))\n\nround(OR.CI , 2)\n\n```\n\n\n# Matched pair data\n\n- $w_1$ and $w_2$ were independent random variables in the examples shown before. [LINK](https://docs.google.com/spreadsheets/d/1YyO_sX1hIn7siATwF7L-mE8OfHktGX2w/edit?gid=50379254#gid=50379254)\n\n- There are other situations where the two probabilities being compared are `dependent` random variables.\n\n- This occurs with `matched pair` data, where two binary response observations, $X$ and $Y$ are made on each `sample unit`\n\n- The desired comparision between success probabilities for X and Y involves two correlated statistics.\n\n## Prostate cancer diagnosis procedures\n\n- (P44) Zhou and Qin (2005) discuss a study that was used to compare the diagnostic accuracy of magnetic resonance imaging (`MRI`) and `ultrasound` in patients who had been established as having `localized prostate cancer` by a gold standard test.\n\n### Hypothesis testing \n\n- This is comparing marginal probabilities\n\n$$H_0: \\pi_{+1}=\\pi_{1+} $$\n$$H_a: \\pi_{+1}\\not= \\pi_{1+} $$\n\n- We can use `wald test statistic` (the book uses $Z_0$ notation which was used for score statistic) or `McNemar's test statistic`, `M` which has approximately $\\chi^2$ distribution for large samples. \n  - Reject $H_0$ when `M` > $\\chi^2_{..}$\n\n\n```{r}\n#| echo: true\n#get marginal distribution \nn <- sum(c.table) \npi.hat.plus1 <- sum(c.table[,1])/n \npi.hat.1plus <- sum(c.table[1,])/n\n\n#evaluate the difference by \n#subtracting the sample statistics\ndata.frame(pi.hat.plus1 , pi.hat.1plus , \n           diff = pi.hat.plus1 - pi.hat.1plus)\n```\n\n```{r}\n#| echo: true\nc.table <- array(data = c(4, 3, 6, 3), dim = c(2,2), dimnames =\nlist(MRI = c(\"Localized\", \"Advanced\"), Ultrasound =\nc(\"Localized\", \"Advanced\"))) \n\nc.table\n\nmcnemar.test(x = c.table , correct = FALSE)\n\n```\n\n\n### Interval \n\n- `Wald confidence interval`\n\n```{r}\n##library(PropCIs)\n## wald confidence interval\ndiffpropci.Wald.mp(b = c.table[1,2], c = c.table[2,1], n =\nsum(c.table), conf.level = 0.95)\n```\n\n- `Agresti-Min confidence interval`\n  - You also get difference sample esitmate.\n\n```{r}\ndiffpropci.mp(b = c.table[1,2], c = c.table[2,1], n =\nsum(c.table), conf.level = 0.95)\n```\n\n\n# Larger contingency tables\n\n- There are many instances where more than two groups exist (i.e., there are more than two rows within a contingency table)\n\n- Once more rows are added to a contingency table, it is often easier to perform the analysis with a binary regression models as covered in Chapter 2. \n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"html-math-method":"katex","css":["style.css"],"output-file":"wk1.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.42","title":"Binary groups ","editor_options":{"chunk_output_type":"console"},"theme":{"light":"cosmo","dark":["cosmo","theme-dark.scss"]}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}