{
  "hash": "134c000f0669417e526aa6f3ae37c4f1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Multinomial Probability\"\nformat:\n  html:\n    toc: true\n    html-math-method: katex\n    css: style.css  \n    theme:\n      light: cosmo\n      dark: [cosmo, theme-dark.scss]\nexecute: \n   echo: false\neditor_options: \n  chunk_output_type: console\n---\n\n```{=html}\n<style>\n.table-hover > tbody > tr:hover { \n  background-color: #f4f442;\n}\n</style>\n```\n\n\n\n\n\n\n-   Reading\n    -   Ch.3\n        -   skip 3.4.3, 3.5 (page )\n\n\n# Multinomial probability distribution\n\n-   Suppose `Y` is categorical response random variable and we have `n` trials, $Y_1..Y_n$\n-   $N_j$ counts the number of trials responding with category `j`.\n    -   $N_j = \\sum_{i=1}^n I(Y_i=j)$\n-   parameters:\n    -   number of category, `j=1,...J`\n    -   probability associated with each category $\\pi_j = P(Y=j)$\n-   It is distribution, so\n\n$$\\sum_{j=1}^J \\pi_j = 1$$\n\n-   Then, PMF of observing a particular set of counts $n_1,...n_j$ is\n\n$$P(N_1 = n_1,....N_j = J) = \\frac{n!}{\\prod_{j=1}^{J}n_j!}\\prod_{j=1}^{J}\\pi_j^{n_j}$$\n\nThis is known as the `multinomial probability distribution`\n\n-   We use MLE to obtain estimates of $\\pi_1,...\\pi_J$ which is $\\hat{\\pi} = \\frac{n_j}{n}$, which is the observed proportion for each category.\n\n\n# One Multinomial distribution\n\n-   This is different from `multinomial distribution`.\n-   Well... here is an example [LINK](https://docs.google.com/spreadsheets/d/1YyO_sX1hIn7siATwF7L-mE8OfHktGX2w/edit?gid=139180726#gid=139180726)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npi.ij <- c(0.2, 0.3, 0.2, 0.1, 0.1, 0.1)\npi.table <- array(data = pi.ij , dim = c(2,3), dimnames = list(X\n= 1:2, Y = 1:3))\n\npi.table %>% kable(\"html\") %>% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> 1 </th>\n   <th style=\"text-align:right;\"> 2 </th>\n   <th style=\"text-align:right;\"> 3 </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 0.2 </td>\n   <td style=\"text-align:right;\"> 0.2 </td>\n   <td style=\"text-align:right;\"> 0.1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 0.3 </td>\n   <td style=\"text-align:right;\"> 0.1 </td>\n   <td style=\"text-align:right;\"> 0.1 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(9812)  \n\nsave <- rmultinom(n = 1, size = 1000, prob = pi.ij)  \n\nc.table1 <- array(data = save , dim = c(2,3), dimnames = list(X\n= 1:2, Y = 1:3))\n\nc.table1/sum(c.table1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Y\nX       1     2     3\n  1 0.191 0.206 0.094\n  2 0.311 0.095 0.103\n```\n\n\n:::\n\n```{.r .cell-code}\nrowSums(c.table1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  1   2 \n491 509 \n```\n\n\n:::\n:::\n\n\n\n\n# Product Multinomial distribution\n\n-   For the I multinomial (i.e., product multinomial) setting, we again simulate a sample for a 2 Ã— 3 contingency table (thus, I = 2).\n-   With this model, we need to draw samples of fixed size separately for each row.\n- Here is an example from book\n\n## This part is preparing the data set part\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#calculate conditional probability \npi.cond <- pi.table/rowSums(pi.table)\npi.cond\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Y\nX     1   2   3\n  1 0.4 0.4 0.2\n  2 0.6 0.2 0.2\n```\n\n\n:::\n\n```{.r .cell-code}\n#simulate based on the conditional probability\n#notice the sample size is different from c.table1\n#suppose that we sampled 400 and 600.  This is just what was \n#observed or we selected this\nset.seed(8111) \nsave1 <- rmultinom(n = 1, size = 400, prob = pi.cond[1,]) \nsave2 <- rmultinom(n = 1, size = 600, prob = pi.cond[2,])\n\nc.table2 <- array(data = c(save1[1], save2[1], save1[2],\nsave2[2], save1[3], save2 [3]), dim = c(2,3), dimnames = list(X = 1:2, Y = 1:3))\n\nc.table2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Y\nX     1   2   3\n  1 162 159  79\n  2 351 126 123\n```\n\n\n:::\n:::\n\n\n\n\n-   `Can we estimate the joint from this table?` is the question.\n\n    -   Answer is, `yes`\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n   Y\nX       1      2      3\n  1 0.405 0.3975 0.1975\n  2 0.585 0.2100 0.2050\n```\n\n\n:::\n:::\n\n\n\n\n-   Now, we have joint and marginal, can we test independence?\n\n# Testing for independence\n\n$$H_0: P(X=x_i,Y=y_j) = P(x_i)\\cdot P(y_j)$$ $$H_a: P(X=x_i,Y=y_j) \\not= P(x_i)\\cdot P(y_j)$$ Using the book notation in p147,\n\n$$H_0: \\pi_{ij}=\\pi_{i+}\\pi_{+j} \\text{  for each i,j}$$ $$H_0: \\pi_{ij}\\not=\\pi_{i+}\\pi_{+j} \\text{  for some i,j}$$\n\n### Compare test statistic and $\\chi^2$\n\n-   This is called `Pearson Chi-square test`\n-   Under product multinomial model (see page 147), this is how you compute the test statistics\n\n$$\\text{X}^2 = \\frac{\\text{difference between joint and product of marginal}}{\\text{product of marginal}}$$\n\n-   Above statistic has an approximate $\\chi^2$ distribution with certain degrees of freedom\n\n-   After you set your type I error, $\\alpha$, you get your crtical value and compare that critical value with your observed test statistic $\\text{X}^2$\n\n### LR test\n\n-   This is called LRT test\n-   The likelihood ratio test can also be conducted.\n    -   The joint probability can be computed in two different ways\n        -   From contingency table, just calculate it.\n        -   And using the marginal probabilities, multiply them.\n    -   Then, using equation 3.2 in page 143, you can calculate the likelihood of both and form `the likelihood ratio`\n\n$$\\lambda = \\frac{\\text{the MLF estimated using joint probabilites}}{\\text{the MLF estimated using joint probabilities estimated as the product of marginal}}$$\n\nAnd $-2\\text{log}(\\lambda)$, also follows $\\chi^2$ distribution with certain degrees of freedom. (this sounds like a magic.. to me..)\n\n### Note\n\n-   Both LRT and the Pearson chi-square test generally give similiar result in large samples. (see page 147)\n    -   common criteria $\\frac{\\text{product of marginal counts}}{\\text{total count}} > 1$ or $>5$ for all cells of the contingency table\n\n# Example of Testing Independnece \n\n-   We would like to determine if `bloating severity` is related to the `type of fiber`.\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n      bloat\nfiber  none low medium high\n  none    6   4      2    0\n  bran    7   4      1    0\n  gum     2   2      3    5\n  both    2   5      3    2\n```\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in chisq.test(x = diet.table, correct = FALSE): Chi-squared\napproximation may be incorrect\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test\n\ndata:  diet.table\nX-squared = 16.943, df = 9, p-value = 0.04962\n```\n\n\n:::\n:::\n\n\n\n\n-   Note that we include the `correct = FALSE` argument value in `chisq.test()` to prevent a continuity correction from being applied.\n    -   Please see Section 1.2.3 for why we avoid these corrections.\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n      bloat\nfiber  none  low medium high\n  none 4.25 3.75   2.25 1.75\n  bran 4.25 3.75   2.25 1.75\n  gum  4.25 3.75   2.25 1.75\n  both 4.25 3.75   2.25 1.75\n```\n\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}