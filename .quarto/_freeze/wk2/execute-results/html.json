{
  "hash": "4d0a0373f06c8c68a82b1a1270525d01",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"logit\"\nformat:\n  html:\n    toc: true\n    html-math-method: katex\n    css: style.css  \n    theme:\n      light: cosmo\n      dark: [cosmo, theme-dark.scss]\nexecute: \n   echo: false\neditor_options: \n  chunk_output_type: console\neditor: \n  markdown: \n    wrap: 72\n---\n\n```{=html}\n<style>\n.table-hover > tbody > tr:hover { \n  background-color: #f4f442;\n}\n</style>\n```\n\n\n\n\n\n\n\nReading \nChristopher R. Bilder and Thomas M. Loughin. Analysis of Categorical Data with R. CRC Press. 2015.\n  - CH 2.1, 2.2.1-2.2.4 (page 61- 94)\n  - Ch. 2.2.5 – 2.2.8, 2.3 (page 94)\n  \n  \n-   In CH1, we first focused on estimating $\\pi$\n-   Then, $\\pi_1$ and $\\pi_2$ and independent groups\n    -   Briefly talked about `matched pair` case where $\\pi_1$ and\n        $\\pi_2$ are dependent.\n-   Now we start talking about many different possible probabilities of\n    success to estimate and perform inference upon.\n    \n- Use `odd` to estimate parameters and use `odd ratio` to evaluate the estimated parameters. \n\n# Generalized linear models (GLM)\n\n- GLMs have three different parts (see page 121:\n  \n  1. Random Component:\n      - $Y$, for logistic regression, $Y$ has a Bernoulli distribution\n  \n  2. Systematic Component:\n      - This specifies a linear combination of the regression parameters with `features`, and this part is referred to as the `linear predictor`\n      - $\\beta_0 + \\beta_1x_1 + .... + \\beta_px_p$\n      \n  3. LINK FUNCTION:\n      - Specifies how the expected value of the `random component` $E[Y]$ is linked to the `sysmatic` component.\n      - $\\text{logit}(\\pi) = \\beta_0 + \\beta_1x_1 + .... + \\beta_px_p$\n      - where $E[Y] = \\pi$\n\n\n## Link function \n\n$$\\text{logit}(\\pi_i)=\\text{log}(\\frac{\\pi_i}{1-\\pi_i}) = \\beta_0 + \\beta_1x_{i,1}+...++ \\beta_px_{i,p}$$\n\n-   Unfortunately, there are only a few simple cases where these\n    parameter estimates have `closed-form solutions`; i.e., we cannot\n    generally write out the parameter estimates in terms of the observed\n    data like we could for the single probability estimate $\\pi$ in\n    Section 1.1.2.\n\n-   Instead, we use iterative numerical procedures, as described in\n    Appendix B.3.2, to successively find estimates of the regression\n    parameters that increase the log-likelihood function\n\n# Parameter estimation in R\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#get data\nplacekick <- read.table(here(\"data\",\"Placekick.csv\"),header = T, sep = \",\")\n\nplacekick %>% head %>% kable(\"html\") %>% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> week </th>\n   <th style=\"text-align:right;\"> distance </th>\n   <th style=\"text-align:right;\"> change </th>\n   <th style=\"text-align:right;\"> elap30 </th>\n   <th style=\"text-align:right;\"> PAT </th>\n   <th style=\"text-align:right;\"> type </th>\n   <th style=\"text-align:right;\"> field </th>\n   <th style=\"text-align:right;\"> wind </th>\n   <th style=\"text-align:right;\"> good </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 21 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 24.7167 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 21 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 15.8500 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 20 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.4500 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 28 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 13.5500 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 20 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 21.8667 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 25 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 17.6833 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n\n```{.r .cell-code}\n#fit the model\nmod.fit <- glm(formula = good ~ distance , family = binomial(link = logit), data = placekick)\n\nmod.fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:  glm(formula = good ~ distance, family = binomial(link = logit), \n    data = placekick)\n\nCoefficients:\n(Intercept)     distance  \n      5.812       -0.115  \n\nDegrees of Freedom: 1424 Total (i.e. Null);  1423 Residual\nNull Deviance:\t    1013 \nResidual Deviance: 775.7 \tAIC: 779.7\n```\n\n\n:::\n\n```{.r .cell-code}\n# latex_equation <- extract_eq(mod.fit)\n# print(latex_equation)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n\n## Mod.fit()\n\n$$\n\\log\\left[ \\frac { P( \\operatorname{good} = \\operatorname{1} ) }{ 1 - P( \\operatorname{good} = \\operatorname{1} ) } \\right] = 5.812 +- 0.115\\cdot(\\operatorname{distance})\n$$\n\n-   You can get confidence interval this way\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(mod.fit)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWaiting for profiling to be done...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                 2.5 %      97.5 %\n(Intercept)  5.1958841  6.47715198\ndistance    -0.1318144 -0.09907103\n```\n\n\n:::\n:::\n\n\n\n\n\n-   also this way\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(mod.fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = good ~ distance, family = binomial(link = logit), \n    data = placekick)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.7441   0.2425   0.2425   0.3801   1.6092  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  5.812080   0.326277   17.81   <2e-16 ***\ndistance    -0.115027   0.008339  -13.79   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1013.43  on 1424  degrees of freedom\nResidual deviance:  775.75  on 1423  degrees of freedom\nAIC: 779.75\n\nNumber of Fisher Scoring iterations: 6\n```\n\n\n:::\n:::\n\n\n\n\n\npage74\n\n### Hypothesis tests for regression parameters\n\n-   (see page 56)\n-   Wald test, which involves calculating $\\Z_0$ and using standard\n    normal distribution often suffer from bias.\n-   LRT typically performs better than the Wald Test\n\n$$H_0: \\text{logit}(\\pi) =\n\\log\\left[ \\frac { P( \\operatorname{good} = \\operatorname{1} ) }{ 1 - P( \\operatorname{good} = \\operatorname{1} ) } \\right] = \\alpha + \\beta_{1}\\cdot\\operatorname{distance}\n$$\n\n\n\n$$H_A: \\text{logit}(\\pi) =\n\\log\\left[ \\frac { P( \\operatorname{good} = \\operatorname{1} ) }{ 1 - P( \\operatorname{good} = \\operatorname{1} ) } \\right] = \\alpha + \\beta_{1}\\cdot\\operatorname{distance} + \\beta_2\\cdot\\text{another_feature}\n$$\n\n### LRT\n\n![](image/LRT.png){fig-align=\"center\" width=\"400\"}\n\n$$\\Lambda = \\frac{\\text{ML under }H_0 }{\\text{ML under }H_A}$$\n\n### Transformed LRT\n\n![](image/transformed%20LRT.webp){fig-align=\"center\" width=\"400\"}\n\n$$-2\\text{log}{(\\Lambda)} \\approx \\chi^2$$\n\n# Deviance\n\n-   `deviance` refers to the amount that a particular model deviates\n    from another model as measured by $-2\\text{log}(\\Lambda)$.\n\n## Saturated model\n\n-   `saturated model`, which has a different coefficent for each data\n    point, leading to perfect prediction, a likelihood of one, and a log\n    likelihood of zero.\n\n## Null deviance\n\n-   The `null deviance` measures the performance of the worst model\n    using only `an intercept`, providing a benchmark.\n\n$$\n\\text{Null Deviance}= -2 \\text{log}(L(\\hat{\\beta_0}|y_1,..., y_n))\n$$\n\n## Residual deviance\n\n-   The `residual deviance` is the deviance of our fitted model.\n-   It is always greater than zero unless it is the saturated model\n    which explains the data perfectly.\n\n$$\n\\text{Residual Deviance}= -2 \\text{log}(L(\\hat{\\beta}|y_1,..., y_n))\n$$\n\n-   Therefore, how much better (smaller) our `residual deviance` is\n    compared to the `null deviance` and how close it is to zero is a\n    measure of model fit.\n\n-   Sometimes people will compute an $R^2$ for logistic regression using\n    $1-\\frac{\\text{Residual Deviance}}{\\text{Null Deviance}}$ since it\n    is bounded between 0 (residual deviance = null deviance) and 1\n    (residual deviance = saturated model = 0).\n\n-   Note that we can compute deviance of two separate models by\n    substracting the null model residual deviance and the alternative\n    model residual deviance from separate logistic regression fits.\n\n-   See page 81 for more information\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod.fit2 <- glm(formula = good ~ change + distance , family =\nbinomial(link = logit), data = placekick)\n\n#one at a time test\nAnova(mod.fit2, test = \"LR\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Deviance Table (Type II tests)\n\nResponse: good\n         LR Chisq Df Pr(>Chisq)    \nchange      5.246  1      0.022 *  \ndistance  218.650  1     <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\nanova(mod.fit, mod.fit2, test = \"LR\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Deviance Table\n\nModel 1: good ~ distance\nModel 2: good ~ change + distance\n  Resid. Df Resid. Dev Df Deviance Pr(>Chi)  \n1      1423     775.75                       \n2      1422     770.50  1   5.2455    0.022 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\n\n# Probability \n\n$$\\pi_i = \\frac{\\text{exp}(\\beta_0 + \\beta_1x_{i,1}+...++ \\beta_px_{i,p})}{1+\\text{exp}(\\beta_0 + \\beta_1x_{i,1}+...++ \\beta_px_{i,p})}$$\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear.pred <- mod.fit$coefficients[1] +\nmod.fit$coefficients[2] * 20\n\n#value of the logit function based on given value of distance at 20\nlinear.pred\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept) \n   3.511547 \n```\n\n\n:::\n\n```{.r .cell-code}\nnew_data <- data.frame(distance = 20)\npredict(mod.fit, newdata = new_data, type = \"link\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       1 \n3.511547 \n```\n\n\n:::\n\n```{.r .cell-code}\npredict(mod.fit, newdata = new_data, type = \"response\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        1 \n0.9710145 \n```\n\n\n:::\n:::\n\n\n\n\n\n## confidence interval of the $\\pi$ estimate\n\n-   see page 90\n-   We can get the $\\hat{\\pi}$\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(mod.fit, newdata = new_data, type = \"response\", se = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$fit\n        1 \n0.9710145 \n\n$se.fit\n         1 \n0.00487676 \n\n$residual.scale\n[1] 1\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#create a function to get the confidence interval of pi\nci.pi <- function(newdata , mod.fit.obj , alpha){\n  linear.pred <- predict(object = mod.fit.obj , \n                         newdata =newdata , \n                         type = \"link\", se = TRUE)\n  \n  CI.lin.pred.lower <- linear.pred$fit - qnorm(p =1-alpha/2)*linear.pred$se\n  CI.lin.pred.upper <- linear.pred$fit + qnorm(p =1-alpha/2)*linear.pred$se\n  \n  #get pi\n  CI.pi.lower <- exp(CI.lin.pred.lower) / (1 +exp(CI.lin.pred.lower))\n  CI.pi.upper <- exp(CI.lin.pred.upper) / (1 +exp(CI.lin.pred.upper))\n\n  list(lower = CI.pi.lower , upper = CI.pi.upper)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nci.pi(newdata = data.frame(distance = 20), mod.fit.obj = mod.fit , alpha = 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$lower\n        1 \n0.9597647 \n\n$upper\n        1 \n0.9791871 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- 5:70\n\npredicted <- predict(object = mod.fit , newdata =\ndata.frame(distance = x), type = \"response\")\n\nlower <- ci.pi(newdata = data.frame(distance = x),\nmod.fit.obj = mod.fit , alpha = 0.05)$lower\n\nupper <- ci.pi(newdata = data.frame(distance = x),\nmod.fit.obj = mod.fit , alpha = 0.05)$upper\n\ndf <- as.data.frame(cbind(x,lower,upper,predicted)) %>% pivot_longer(-x)\n\np <- df %>% ggplot(aes(x=x,y = value, color = name)) + geom_line()\n\nggplotly(p)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"plotly html-widget html-fill-item\" id=\"htmlwidget-5032b7e8cf46d17d7783\" style=\"width:100%;height:464px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-5032b7e8cf46d17d7783\">{\"x\":{\"data\":[{\"x\":[5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70],\"y\":[0.99076287056679824,0.98980527080417846,0.98874891017055655,0.9875837255068477,0.98629865529381888,0.98488154614048595,0.98331905153940302,0.98159652240436146,0.97969788888477272,0.97760553292153718,0.9753001509664313,0.97276060622434057,0.96996376968701137,0.9668843490997604,0.96349470483162336,0.95976465140340228,0.95566124317995993,0.95114854249548153,0.94618736835201489,0.94073502401091047,0.93474500265006699,0.92816667241964224,0.92094494672081495,0.91301995390521717,0.90432673497316629,0.89479502074258488,0.88434917348985276,0.87290842222887177,0.86038757052066084,0.84669839729408458,0.83175198058524036,0.81546212163710163,0.7977499116942901,0.77854927558363285,0.75781309680955733,0.73551935705253935,0.71167667017920722,0.68632866189035024,0.65955679000714529,0.63148135216913692,0.60226055163484848,0.57208759206088011,0.54118587261964346,0.50980247354334052,0.47820025766114577,0.44664904640628789,0.41541643169533643,0.38475883336420769,0.35491339155009927,0.32609119494507477,0.29847220323094137,0.27220204870550924,0.24739072507036874,0.22411301500582637,0.20241038999266253,0.18229404430746607,0.16374869994603319,0.14673683345881222,0.13120301851871302,0.11707813764547434,0.10428328214896239,0.092733222671969831,0.082339388179894057,0.073012335979471377,0.064663728594730338,0.05720785580024465],\"text\":[\"x:  5<br />value: 0.99076287<br />name: lower\",\"x:  6<br />value: 0.98980527<br />name: lower\",\"x:  7<br />value: 0.98874891<br />name: lower\",\"x:  8<br />value: 0.98758373<br />name: lower\",\"x:  9<br />value: 0.98629866<br />name: lower\",\"x: 10<br />value: 0.98488155<br />name: lower\",\"x: 11<br />value: 0.98331905<br />name: lower\",\"x: 12<br />value: 0.98159652<br />name: lower\",\"x: 13<br />value: 0.97969789<br />name: lower\",\"x: 14<br />value: 0.97760553<br />name: lower\",\"x: 15<br />value: 0.97530015<br />name: lower\",\"x: 16<br />value: 0.97276061<br />name: lower\",\"x: 17<br />value: 0.96996377<br />name: lower\",\"x: 18<br />value: 0.96688435<br />name: lower\",\"x: 19<br />value: 0.96349470<br />name: lower\",\"x: 20<br />value: 0.95976465<br />name: lower\",\"x: 21<br />value: 0.95566124<br />name: lower\",\"x: 22<br />value: 0.95114854<br />name: lower\",\"x: 23<br />value: 0.94618737<br />name: lower\",\"x: 24<br />value: 0.94073502<br />name: lower\",\"x: 25<br />value: 0.93474500<br />name: lower\",\"x: 26<br />value: 0.92816667<br />name: lower\",\"x: 27<br />value: 0.92094495<br />name: lower\",\"x: 28<br />value: 0.91301995<br />name: lower\",\"x: 29<br />value: 0.90432673<br />name: lower\",\"x: 30<br />value: 0.89479502<br />name: lower\",\"x: 31<br />value: 0.88434917<br />name: lower\",\"x: 32<br />value: 0.87290842<br />name: lower\",\"x: 33<br />value: 0.86038757<br />name: lower\",\"x: 34<br />value: 0.84669840<br />name: lower\",\"x: 35<br />value: 0.83175198<br />name: lower\",\"x: 36<br />value: 0.81546212<br />name: lower\",\"x: 37<br />value: 0.79774991<br />name: lower\",\"x: 38<br />value: 0.77854928<br />name: lower\",\"x: 39<br />value: 0.75781310<br />name: lower\",\"x: 40<br />value: 0.73551936<br />name: lower\",\"x: 41<br />value: 0.71167667<br />name: lower\",\"x: 42<br />value: 0.68632866<br />name: lower\",\"x: 43<br />value: 0.65955679<br />name: lower\",\"x: 44<br />value: 0.63148135<br />name: lower\",\"x: 45<br />value: 0.60226055<br />name: lower\",\"x: 46<br />value: 0.57208759<br />name: lower\",\"x: 47<br />value: 0.54118587<br />name: lower\",\"x: 48<br />value: 0.50980247<br />name: lower\",\"x: 49<br />value: 0.47820026<br />name: lower\",\"x: 50<br />value: 0.44664905<br />name: lower\",\"x: 51<br />value: 0.41541643<br />name: lower\",\"x: 52<br />value: 0.38475883<br />name: lower\",\"x: 53<br />value: 0.35491339<br />name: lower\",\"x: 54<br />value: 0.32609119<br />name: lower\",\"x: 55<br />value: 0.29847220<br />name: lower\",\"x: 56<br />value: 0.27220205<br />name: lower\",\"x: 57<br />value: 0.24739073<br />name: lower\",\"x: 58<br />value: 0.22411302<br />name: lower\",\"x: 59<br />value: 0.20241039<br />name: lower\",\"x: 60<br />value: 0.18229404<br />name: lower\",\"x: 61<br />value: 0.16374870<br />name: lower\",\"x: 62<br />value: 0.14673683<br />name: lower\",\"x: 63<br />value: 0.13120302<br />name: lower\",\"x: 64<br />value: 0.11707814<br />name: lower\",\"x: 65<br />value: 0.10428328<br />name: lower\",\"x: 66<br />value: 0.09273322<br />name: lower\",\"x: 67<br />value: 0.08233939<br />name: lower\",\"x: 68<br />value: 0.07301234<br />name: lower\",\"x: 69<br />value: 0.06466373<br />name: lower\",\"x: 70<br />value: 0.05720786<br />name: lower\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.8897637795275593,\"color\":\"rgba(248,118,109,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"name\":\"lower\",\"legendgroup\":\"lower\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70],\"y\":[0.99471164922510558,0.99407080393077707,0.99335281959757493,0.99254854407940707,0.99164777347560074,0.99063914000822284,0.98950999021366781,0.98824625322616932,0.98683229912338943,0.98525078756606232,0.98348250731102105,0.98150620762850904,0.97929842323163585,0.9768332950514933,0.97408239009063047,0.97101452468547556,0.96759559682855767,0.96378843476334353,0.9595526708791865,0.95484465199844026,0.94961739943837575,0.94382063469313415,0.93740088912046671,0.93030171848583332,0.92246404539741722,0.91382665426612253,0.90432686406426566,0.89390140336750712,0.88248750940281417,0.87002426750147321,0.85645419889026708,0.84172509265984763,0.82579206175169761,0.80861978298310211,0.79018485807496552,0.77047820761041574,0.74950738483790358,0.72729867399869752,0.70389882176684149,0.67937624407260666,0.65382155742140746,0.62734730628766056,0.60008679710568447,0.5721920034362874,0.54383057219393638,0.51518203116592087,0.4864333655687097,0.45777418767860129,0.42939176115278849,0.40146615530677976,0.37416579240088593,0.34764361464345289,0.32203404219534421,0.29745082635647091,0.27398583170825602,0.2517087152119668,0.23066741544772587,0.21088932532778137,0.19238299822803112,0.17514022981580582,0.15913836351362801,0.14434268317769253,0.13070877855189603,0.11818479399595772,0.10671349608226184,0.096234118861999562],\"text\":[\"x:  5<br />value: 0.99471165<br />name: predicted\",\"x:  6<br />value: 0.99407080<br />name: predicted\",\"x:  7<br />value: 0.99335282<br />name: predicted\",\"x:  8<br />value: 0.99254854<br />name: predicted\",\"x:  9<br />value: 0.99164777<br />name: predicted\",\"x: 10<br />value: 0.99063914<br />name: predicted\",\"x: 11<br />value: 0.98950999<br />name: predicted\",\"x: 12<br />value: 0.98824625<br />name: predicted\",\"x: 13<br />value: 0.98683230<br />name: predicted\",\"x: 14<br />value: 0.98525079<br />name: predicted\",\"x: 15<br />value: 0.98348251<br />name: predicted\",\"x: 16<br />value: 0.98150621<br />name: predicted\",\"x: 17<br />value: 0.97929842<br />name: predicted\",\"x: 18<br />value: 0.97683330<br />name: predicted\",\"x: 19<br />value: 0.97408239<br />name: predicted\",\"x: 20<br />value: 0.97101452<br />name: predicted\",\"x: 21<br />value: 0.96759560<br />name: predicted\",\"x: 22<br />value: 0.96378843<br />name: predicted\",\"x: 23<br />value: 0.95955267<br />name: predicted\",\"x: 24<br />value: 0.95484465<br />name: predicted\",\"x: 25<br />value: 0.94961740<br />name: predicted\",\"x: 26<br />value: 0.94382063<br />name: predicted\",\"x: 27<br />value: 0.93740089<br />name: predicted\",\"x: 28<br />value: 0.93030172<br />name: predicted\",\"x: 29<br />value: 0.92246405<br />name: predicted\",\"x: 30<br />value: 0.91382665<br />name: predicted\",\"x: 31<br />value: 0.90432686<br />name: predicted\",\"x: 32<br />value: 0.89390140<br />name: predicted\",\"x: 33<br />value: 0.88248751<br />name: predicted\",\"x: 34<br />value: 0.87002427<br />name: predicted\",\"x: 35<br />value: 0.85645420<br />name: predicted\",\"x: 36<br />value: 0.84172509<br />name: predicted\",\"x: 37<br />value: 0.82579206<br />name: predicted\",\"x: 38<br />value: 0.80861978<br />name: predicted\",\"x: 39<br />value: 0.79018486<br />name: predicted\",\"x: 40<br />value: 0.77047821<br />name: predicted\",\"x: 41<br />value: 0.74950738<br />name: predicted\",\"x: 42<br />value: 0.72729867<br />name: predicted\",\"x: 43<br />value: 0.70389882<br />name: predicted\",\"x: 44<br />value: 0.67937624<br />name: predicted\",\"x: 45<br />value: 0.65382156<br />name: predicted\",\"x: 46<br />value: 0.62734731<br />name: predicted\",\"x: 47<br />value: 0.60008680<br />name: predicted\",\"x: 48<br />value: 0.57219200<br />name: predicted\",\"x: 49<br />value: 0.54383057<br />name: predicted\",\"x: 50<br />value: 0.51518203<br />name: predicted\",\"x: 51<br />value: 0.48643337<br />name: predicted\",\"x: 52<br />value: 0.45777419<br />name: predicted\",\"x: 53<br />value: 0.42939176<br />name: predicted\",\"x: 54<br />value: 0.40146616<br />name: predicted\",\"x: 55<br />value: 0.37416579<br />name: predicted\",\"x: 56<br />value: 0.34764361<br />name: predicted\",\"x: 57<br />value: 0.32203404<br />name: predicted\",\"x: 58<br />value: 0.29745083<br />name: predicted\",\"x: 59<br />value: 0.27398583<br />name: predicted\",\"x: 60<br />value: 0.25170872<br />name: predicted\",\"x: 61<br />value: 0.23066742<br />name: predicted\",\"x: 62<br />value: 0.21088933<br />name: predicted\",\"x: 63<br />value: 0.19238300<br />name: predicted\",\"x: 64<br />value: 0.17514023<br />name: predicted\",\"x: 65<br />value: 0.15913836<br />name: predicted\",\"x: 66<br />value: 0.14434268<br />name: predicted\",\"x: 67<br />value: 0.13070878<br />name: predicted\",\"x: 68<br />value: 0.11818479<br />name: predicted\",\"x: 69<br />value: 0.10671350<br />name: predicted\",\"x: 70<br />value: 0.09623412<br />name: predicted\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.8897637795275593,\"color\":\"rgba(0,186,56,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"name\":\"predicted\",\"legendgroup\":\"predicted\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70],\"y\":[0.99697751497811637,0.99655782016333616,0.99608029261845721,0.99553708288528242,0.99491930932454908,0.99421693185550253,0.99341861304427281,0.99251156615126968,0.99148139006607494,0.9903118915169099,0.98898489558205793,0.98748004640298492,0.98577460116323412,0.98384322192255369,0.98165777185807435,0.97918712494090843,0.97639700113449646,0.97324984287711236,0.96970475287645241,0.96571751794850402,0.96124074841649165,0.95622416675607635,0.9506150815430866,0.94435908147536129,0.93740097667403255,0.929685997366411,0.92116123024125107,0.91177722886612211,0.90148967993170881,0.89026095347116541,0.87806133456061231,0.86486975421993895,0.85067392785077234,0.83546996110979344,0.81926164555633285,0.80205976559714276,0.78388172007681645,0.76475163186214179,0.74470094012293953,0.72376932012614681,0.70200570006028462,0.67946914425995775,0.65622941980129956,0.63236712799535622,0.60797334365944133,0.58314875519643294,0.55800233703680502,0.53264961517837794,0.50721060851319111,0.48180754437492834,0.45656245633720033,0.43159477539126523,0.40701902199134343,0.38294269630002858,0.35946444812599204,0.33667258791963994,0.31464397757517998,0.29344331663154444,0.27312281762595536,0.25372224537857774,0.23526927996609109,0.21778015265141532,0.20126049815269687,0.18570636501311788,0.17110532782567897,0.1574376498489968],\"text\":[\"x:  5<br />value: 0.99697751<br />name: upper\",\"x:  6<br />value: 0.99655782<br />name: upper\",\"x:  7<br />value: 0.99608029<br />name: upper\",\"x:  8<br />value: 0.99553708<br />name: upper\",\"x:  9<br />value: 0.99491931<br />name: upper\",\"x: 10<br />value: 0.99421693<br />name: upper\",\"x: 11<br />value: 0.99341861<br />name: upper\",\"x: 12<br />value: 0.99251157<br />name: upper\",\"x: 13<br />value: 0.99148139<br />name: upper\",\"x: 14<br />value: 0.99031189<br />name: upper\",\"x: 15<br />value: 0.98898490<br />name: upper\",\"x: 16<br />value: 0.98748005<br />name: upper\",\"x: 17<br />value: 0.98577460<br />name: upper\",\"x: 18<br />value: 0.98384322<br />name: upper\",\"x: 19<br />value: 0.98165777<br />name: upper\",\"x: 20<br />value: 0.97918712<br />name: upper\",\"x: 21<br />value: 0.97639700<br />name: upper\",\"x: 22<br />value: 0.97324984<br />name: upper\",\"x: 23<br />value: 0.96970475<br />name: upper\",\"x: 24<br />value: 0.96571752<br />name: upper\",\"x: 25<br />value: 0.96124075<br />name: upper\",\"x: 26<br />value: 0.95622417<br />name: upper\",\"x: 27<br />value: 0.95061508<br />name: upper\",\"x: 28<br />value: 0.94435908<br />name: upper\",\"x: 29<br />value: 0.93740098<br />name: upper\",\"x: 30<br />value: 0.92968600<br />name: upper\",\"x: 31<br />value: 0.92116123<br />name: upper\",\"x: 32<br />value: 0.91177723<br />name: upper\",\"x: 33<br />value: 0.90148968<br />name: upper\",\"x: 34<br />value: 0.89026095<br />name: upper\",\"x: 35<br />value: 0.87806133<br />name: upper\",\"x: 36<br />value: 0.86486975<br />name: upper\",\"x: 37<br />value: 0.85067393<br />name: upper\",\"x: 38<br />value: 0.83546996<br />name: upper\",\"x: 39<br />value: 0.81926165<br />name: upper\",\"x: 40<br />value: 0.80205977<br />name: upper\",\"x: 41<br />value: 0.78388172<br />name: upper\",\"x: 42<br />value: 0.76475163<br />name: upper\",\"x: 43<br />value: 0.74470094<br />name: upper\",\"x: 44<br />value: 0.72376932<br />name: upper\",\"x: 45<br />value: 0.70200570<br />name: upper\",\"x: 46<br />value: 0.67946914<br />name: upper\",\"x: 47<br />value: 0.65622942<br />name: upper\",\"x: 48<br />value: 0.63236713<br />name: upper\",\"x: 49<br />value: 0.60797334<br />name: upper\",\"x: 50<br />value: 0.58314876<br />name: upper\",\"x: 51<br />value: 0.55800234<br />name: upper\",\"x: 52<br />value: 0.53264962<br />name: upper\",\"x: 53<br />value: 0.50721061<br />name: upper\",\"x: 54<br />value: 0.48180754<br />name: upper\",\"x: 55<br />value: 0.45656246<br />name: upper\",\"x: 56<br />value: 0.43159478<br />name: upper\",\"x: 57<br />value: 0.40701902<br />name: upper\",\"x: 58<br />value: 0.38294270<br />name: upper\",\"x: 59<br />value: 0.35946445<br />name: upper\",\"x: 60<br />value: 0.33667259<br />name: upper\",\"x: 61<br />value: 0.31464398<br />name: upper\",\"x: 62<br />value: 0.29344332<br />name: upper\",\"x: 63<br />value: 0.27312282<br />name: upper\",\"x: 64<br />value: 0.25372225<br />name: upper\",\"x: 65<br />value: 0.23526928<br />name: upper\",\"x: 66<br />value: 0.21778015<br />name: upper\",\"x: 67<br />value: 0.20126050<br />name: upper\",\"x: 68<br />value: 0.18570637<br />name: upper\",\"x: 69<br />value: 0.17110533<br />name: upper\",\"x: 70<br />value: 0.15743765<br />name: upper\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.8897637795275593,\"color\":\"rgba(97,156,255,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"name\":\"upper\",\"legendgroup\":\"upper\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":26.228310502283104,\"r\":7.3059360730593621,\"b\":40.182648401826491,\"l\":48.949771689497723},\"plot_bgcolor\":\"rgba(235,235,235,1)\",\"paper_bgcolor\":\"rgba(255,255,255,1)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.611872146118724},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[1.75,73.25],\"tickmode\":\"array\",\"ticktext\":[\"20\",\"40\",\"60\"],\"tickvals\":[20,40,60],\"categoryorder\":\"array\",\"categoryarray\":[\"20\",\"40\",\"60\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.6529680365296811,\"tickwidth\":0.66417600664176002,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.68949771689498},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(255,255,255,1)\",\"gridwidth\":0.66417600664176002,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\"x\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.611872146118724}},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[0.010219372841351063,1.0439659979370099],\"tickmode\":\"array\",\"ticktext\":[\"0.25\",\"0.50\",\"0.75\",\"1.00\"],\"tickvals\":[0.25,0.5,0.75,1],\"categoryorder\":\"array\",\"categoryarray\":[\"0.25\",\"0.50\",\"0.75\",\"1.00\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.6529680365296811,\"tickwidth\":0.66417600664176002,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.68949771689498},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(255,255,255,1)\",\"gridwidth\":0.66417600664176002,\"zeroline\":false,\"anchor\":\"x\",\"title\":{\"text\":\"value\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.611872146118724}},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":true,\"legend\":{\"bgcolor\":\"rgba(255,255,255,1)\",\"bordercolor\":\"transparent\",\"borderwidth\":1.8897637795275593,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":11.68949771689498},\"title\":{\"text\":\"name\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.611872146118724}}},\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"modeBarButtonsToAdd\":[\"hoverclosest\",\"hovercompare\"],\"showSendToCloud\":false},\"source\":\"A\",\"attrs\":{\"2fe42348712\":{\"x\":{},\"y\":{},\"colour\":{},\"type\":\"scatter\"}},\"cur_data\":\"2fe42348712\",\"visdat\":{\"2fe42348712\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.20000000000000001,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n\n\n\n# ODDS\n\n-   see page 83, the odd of success at a particular value of `x`\n\n-   The following model can be used to estimate the odd at specific\n    value of `x`\n\n$$\\text{Odds}_x = \\text{exp}(\\beta_0 + \\beta_1x)$$\n\n-   The following model can be used to estimate the odd at specific\n    value of `x + c`\n\n$$\\text{Odds}_{x+c} = \\text{exp}(\\beta_0 + \\beta_1(x+c)$$ \\\n\n## ODD Ratio\n\n$$\\text{OR} = \\frac{\\text{Odds}_{x+c}}{\\text{Odds}_{x}}= \\text{exp}(c\\beta_1)$$ -\n\n\n- Increase in odd is `OR` \\> 1, \n- decrease in odd if `OR` \\< 1,\n\n$$\\hat{\\text{OR}} = \\text{exp}(c\\hat{\\beta}_1)$$ - So, it will have\nestimated variance. - As you have guessed by now, Wald CI does not work\nwell when sample size small.\n\n-   use `transformed LRT` statistic.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#finds LR confidence interval\nmod.fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:  glm(formula = good ~ distance, family = binomial(link = logit), \n    data = placekick)\n\nCoefficients:\n(Intercept)     distance  \n      5.812       -0.115  \n\nDegrees of Freedom: 1424 Total (i.e. Null);  1423 Residual\nNull Deviance:\t    1013 \nResidual Deviance: 775.7 \tAIC: 779.7\n```\n\n\n:::\n\n```{.r .cell-code}\nbeta.ci <- confint(mod.fit, parm = \"distance\", level = 0.95)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWaiting for profiling to be done...\n```\n\n\n:::\n\n```{.r .cell-code}\nbeta.ci\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      2.5 %      97.5 % \n-0.13181435 -0.09907103 \n```\n\n\n:::\n:::\n\n\n\n\n\n### Understanding the changes in odd\n\n-   `10-yard` decrease in `distance` increases the odds of a successful\n    placekick.\n\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n  97.5 %    2.5 % \n2.693147 3.736478 \n```\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.682701 3.719946\n```\n\n\n:::\n:::\n\n\n\n\n\n-   Has similar interval due to large sample size\n\n\n### Example of magin coin\n\n-   You can convert log odd to probability \n-   Your aunt offers a service in which she weights coins to make them\n    unfair.\n\n-   You give her a coin and tell her how much you want the `log-odds` to\n    change. \n      - She returns the modified coin.\n\n-   For each of the following orders, use your function to compute the\n    resulting probability of heads:\n\n    -   fair coin, increase log-odds by 1.\n    -   fair coin, increase log-odds by 2.\n    -   fair coin, increase log-odds by 10.\n    -   fair coin, decrease log-odds by 1.\n    -   fair coin, decrease log-odds by 2.\n    -   fair coin, decrease log-odds by 10.\n\n-   Write an R function that computes the `probability of heads`, given\n    log-odds.\n\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5\n```\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> log_odds </th>\n   <th style=\"text-align:right;\"> probability </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 10 </td>\n   <td style=\"text-align:right;\"> 1.000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.881 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0.731 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.500 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> -1 </td>\n   <td style=\"text-align:right;\"> 0.269 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> -2 </td>\n   <td style=\"text-align:right;\"> 0.119 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> -10 </td>\n   <td style=\"text-align:right;\"> 0.000 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\n\n-   In you own words, describe how changes in log-odds translate to\n    changes in probability\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](wk2_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n\n\n\n-   You can see in this plot, As log-odds increase, the probability of\n    success increases relative to the probability of failure, and it\n    approaches one. As log-odds decrease probability of success decrease\n    and converges to zero.\n\n-   If you get log-odds values that are very very small like -10 the\n    probability of success is almost zero, and if you get log-odds\n    values that are very big like 10 or the probability of success is\n    almost one.\n\n-   The relationship between log-odd and probability is not linear, but\n    of s-curve type, and log odds ratios ranging from -5 to +5 create\n    probabilities that range from just above 0 to very close to 1.\n\n# Case Study: South African Heart Disease\n\n## Background\n\n-   Target: `Probability of getting coronary heart disease`, `chd`\n-   Feature:\n    -   High blood pressure, high LDL cholesterol, diabetes, smoking,\n        secondhand smoke exposure, obesity, an unhealthy diet, and\n        physical inactivity are among the leading risk factors for heart\n        disease.\n-   Data Source: Source: Rousseauw, J., du Plessis, J., Benade, A.,\n    Jordaan, P., Kotze, J. and Ferreira, J. (1983). Coronary risk factor\n    screening in three rural communities, South African Medical Journal\n    64: 430–436.\n\n- `Data Description`\n  -   sbp: systolic blood pressure\n  -   tobacco: cumulative tobacco use (kg)\n  -   ldl: low density lipoprotein cholesterol ('bad' cholestrol)\n  -   adiposity: Body adiposity index determines body fat\n      percentage(calculated as (HC / (HM)1.5) - 18, where HC = Hip\n      Circumference in Centimetres and HM = Height in meters)\n  -   famhist: family history of heart disease\n  -   typea: A personality type that could raise one's chances of\n      developing coronary heart disease\n  -   obesity: Body Mass Index (BMI) ($kg/m^2$)\n  -   alcohol: current alcohol consumption\n  -   age: age at onset\n  -   `chd`: coronary heart disease  (`target`)\n\n\n## Task\n\n- Using `blood pressure`, `smoking`, `cholesterol`,and `age`.\n\n-   Load the data and answer the following questions:\n\n    -   `What are the number of variables and number of observations?`\n    -   `What is the type of each variable? Do we need to change it?`\n    -   `Are there any missing values (in each of the variables)?`\n    -   `Are there any abnormal values in each of the variables in the\n        raw data?`\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> tobacco </th>\n   <th style=\"text-align:right;\"> ldl </th>\n   <th style=\"text-align:right;\"> sbp </th>\n   <th style=\"text-align:right;\"> age </th>\n   <th style=\"text-align:right;\"> chd </th>\n   <th style=\"text-align:right;\"> obesity </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 12.00 </td>\n   <td style=\"text-align:right;\"> 5.73 </td>\n   <td style=\"text-align:right;\"> 160 </td>\n   <td style=\"text-align:right;\"> 52 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 25.30 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 0.01 </td>\n   <td style=\"text-align:right;\"> 4.41 </td>\n   <td style=\"text-align:right;\"> 144 </td>\n   <td style=\"text-align:right;\"> 63 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 28.87 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 0.08 </td>\n   <td style=\"text-align:right;\"> 3.48 </td>\n   <td style=\"text-align:right;\"> 118 </td>\n   <td style=\"text-align:right;\"> 46 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 29.14 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 7.50 </td>\n   <td style=\"text-align:right;\"> 6.41 </td>\n   <td style=\"text-align:right;\"> 170 </td>\n   <td style=\"text-align:right;\"> 58 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 31.99 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 13.60 </td>\n   <td style=\"text-align:right;\"> 3.50 </td>\n   <td style=\"text-align:right;\"> 134 </td>\n   <td style=\"text-align:right;\"> 49 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 25.99 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 6.20 </td>\n   <td style=\"text-align:right;\"> 6.47 </td>\n   <td style=\"text-align:right;\"> 132 </td>\n   <td style=\"text-align:right;\"> 45 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 30.77 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\n\n## EDA\n\n- Univariate analysis \n\n  -   The response (or dependent) variable of interest, Heart disease, is\n      a binary variable taking the type factor.\n  \n  -   Use a bar chart to explore the distribution of the response variable\n      (`chd`). What do you learn?\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> Heart disease </th>\n   <th style=\"text-align:right;\"> N </th>\n   <th style=\"text-align:right;\"> Proportion </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 302 </td>\n   <td style=\"text-align:right;\"> 0.65 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 160 </td>\n   <td style=\"text-align:right;\"> 0.35 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n\n::: {.cell-output-display}\n![](wk2_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n\n\n\nFor metric variables, a density plot or histogram allows us to determine\nthe shape of the distribution and look for outliers.\n\n-   Use a density plot to explore the distribution of explanatory\n    variables. What do you discover?\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](wk2_files/figure-html/unnamed-chunk-20-1.png){width=960}\n:::\n:::\n\n\n\n\n\n- `Bivariate Analysis`\n\n  -   Prior to moving on to the fully specified model, it is advisable to\n      first examine the simple associations between the response and each\n      explanatory variable.\n  \n  -   `Box plots` are useful for exploring the association between a\n      categorical variable and a variable measured on an interval scale.\n  \n  -   Use a `boxplot` to examine how the explanatory variables are\n      correlated with the response variable (chd)?\n  \n      -   The `coord_flip()` function is used to keep the dependent\n          variable on the y-axis.\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](wk2_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n\n\n\n-   Use the convenient summary_factorlist() function from the finalfit\n    package to tabulate data.\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Dependent: chd </th>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:left;\"> 0 </th>\n   <th style=\"text-align:left;\"> 1 </th>\n   <th style=\"text-align:left;\"> p </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> ldl </td>\n   <td style=\"text-align:left;\"> Mean (SD) </td>\n   <td style=\"text-align:left;\"> 4.3 (1.9) </td>\n   <td style=\"text-align:left;\"> 5.5 (2.2) </td>\n   <td style=\"text-align:left;\"> &lt;0.001 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> sbp </td>\n   <td style=\"text-align:left;\"> Mean (SD) </td>\n   <td style=\"text-align:left;\"> 135.5 (18.0) </td>\n   <td style=\"text-align:left;\"> 143.7 (23.7) </td>\n   <td style=\"text-align:left;\"> &lt;0.001 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> tobacco </td>\n   <td style=\"text-align:left;\"> Mean (SD) </td>\n   <td style=\"text-align:left;\"> 2.6 (3.6) </td>\n   <td style=\"text-align:left;\"> 5.5 (5.6) </td>\n   <td style=\"text-align:left;\"> &lt;0.001 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> age </td>\n   <td style=\"text-align:left;\"> Mean (SD) </td>\n   <td style=\"text-align:left;\"> 38.9 (14.9) </td>\n   <td style=\"text-align:left;\"> 50.3 (10.6) </td>\n   <td style=\"text-align:left;\"> &lt;0.001 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\n\n-   According to the plots and the tables, What variable is most\n    important for explaining heart disease? How is that variable\n    correlated with heart disease?\n\n\n\n## MRL\n\n\n-   Is the linear probability model an appropriate choice to study the\n    relationship between heart disease and risk factors?\n\n-   Estimate the following linear probability model and interpret the\n    model results.\n\n$$\\text{chd} = \\beta_0 + \\beta_1 \\cdot \\text{ldl} + \\beta_2 \\cdot \\text{sbp} + + \\beta_3 \\cdot \\text{tobacco} + \\beta_4 \\cdot \\text{age} + \\epsilon$$\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n$$\n\\operatorname{\\hat{chd}} = -0.35 + 0.036\\cdot\\operatorname{ldl} + 0.00097\\cdot\\operatorname{sbp} + 0.0165 \\cdot\\operatorname{tobacco} + 0.007 \\cdot \\operatorname{age} + \\hat{\\epsilon}\n$$\n\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = chd ~ ldl + sbp + tobacco + age, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.8439 -0.3405 -0.1250  0.4365  1.0172 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.3493578  0.1405912  -2.485 0.013315 *  \nldl          0.0362419  0.0102322   3.542 0.000438 ***\nsbp          0.0009739  0.0010670   0.913 0.361839    \ntobacco      0.0165577  0.0049101   3.372 0.000809 ***\nage          0.0076831  0.0016886   4.550 6.89e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4318 on 457 degrees of freedom\nMultiple R-squared:  0.1853,\tAdjusted R-squared:  0.1781 \nF-statistic: 25.98 on 4 and 457 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n\n-   What are the advantages and disadvantages of the linear probability\n    model?\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](wk2_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\\newpage\n\n## GLM\n\n- Generalized linear model\n- Estimate the following logistic regression model and interpret the model results.\n\n$$ \\text{logit}(\\pi_i) =\\beta_0 + \\beta_1 \\cdot \\text{ldl} + \\beta_2 \\cdot \\text{sbp} + + \\beta_3 \\cdot \\text{tobacco} + \\beta_4 \\cdot \\text{age} +  \\epsilon$$\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n$$\n\\log\\left[ \\frac { P( \\operatorname{chd} = \\operatorname{1} ) }{ 1 - P( \\operatorname{chd} = \\operatorname{1} ) } \\right] = -4.54 + 0.018\\cdot \\operatorname{ldl} + 0.004\\cdot \\operatorname{sbp} + 0.075\\cdot \\operatorname{tobacco} + 0.04\\cdot \\operatorname{age}\n$$\n\n\n\n### Odd ratio \n\n-   Do the `raw` coefficient estimates `directionally make sense`?\n\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = chd ~ ldl + sbp + tobacco + age, family = binomial(link = logit), \n    data = df)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.9457  -0.8595  -0.4999   1.0238   2.3906  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -4.535524   0.781360  -5.805 6.45e-09 ***\nldl          0.185131   0.054121   3.421 0.000625 ***\nsbp          0.004307   0.005394   0.798 0.424623    \ntobacco      0.075982   0.025616   2.966 0.003016 ** \nage          0.046264   0.009852   4.696 2.66e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 596.11  on 461  degrees of freedom\nResidual deviance: 502.19  on 457  degrees of freedom\nAIC: 512.19\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n:::\n\n\n\n\n\n-   \n\n    > **Again, all of the explanatory variables except blood pressure\n    > are statistically significant and positively correlated with the\n    > probability of heart disease, same as the linear probability\n    > model.**\n\n-   Recall that (page 83)\n\n$$\n\\text{OR} = \\frac{\\text{Odds}_{x_k+c}}{\\text{Odds}_{x_k}}=exp(c \\beta_k)\n$$ \n\n- The odd of a success change by exp(c$\\beta_k$) `times` for every\nc-unit increase in x\n\n-   Compute and interpret the estimated odds ratio for a 10-unit\n    increase in each explanatory variable.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Replace with your code\nround(exp(10*coef(mod.logit.h0)),2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)         ldl         sbp     tobacco         age \n       0.00        6.37        1.04        2.14        1.59 \n```\n\n\n:::\n:::\n\n\n\n\n\n> **The estimated odds of success or having a heart disease change by\n> 6.37 times for every 10-unit increase in LDL or 'bad' cholesterol.**\n\n> **Interestingly, the odds of having a heart disease is almost 1 for\n> every 10-unit increase in blood pressure, which means an increase in\n> blood pressure doesn't change the odds of having heart disease, and\n> it's consistent with its insignificant coefficient.**\n\n### Hypothesis Test\n\n-   Using the likelihood ratio test (LRT) for hypothesis testing is a\n    common practice in a logistic regression model.\n\n-   Use LRT to test whether ($obesity$) is associated with heart\n    disease.\n\n    -   $H_0: \\beta_{obesity} = 0$\n\n    -   $H_a: \\beta_{obesity} \\ne 0$\n\nUse both *Anova()* or *anova()* functions.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#mod.logit.ha <- # uncomment and replace with your code\nmod.logit.ha <- glm(chd ~ ldl + sbp +tobacco + age + obesity, family = binomial(link = logit), data = df)\n\n#anova()\nanova(mod.logit.h0, mod.logit.ha, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Deviance Table\n\nModel 1: chd ~ ldl + sbp + tobacco + age\nModel 2: chd ~ ldl + sbp + tobacco + age + obesity\n  Resid. Df Resid. Dev Df Deviance Pr(>Chi)\n1       457     502.19                     \n2       456     501.07  1   1.1191   0.2901\n```\n\n\n:::\n\n```{.r .cell-code}\n#Anova()\nAnova(mod.logit.ha, test = \"LR\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Deviance Table (Type II tests)\n\nResponse: chd\n        LR Chisq Df Pr(>Chisq)    \nldl      13.3932  1  0.0002525 ***\nsbp       0.8640  1  0.3526279    \ntobacco   9.4670  1  0.0020920 ** \nage      24.3447  1  8.055e-07 ***\nobesity   1.1191  1  0.2901078    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\n\n-   `deviance` refers to the amount that a particular model deviates\n    from another model as measured by $-2\\text{log}(\\Lambda)$.\n\n-   What are the null deviance and residual deviance in the model\n    summary?\n\n    -   For `null` and `residual deviance`, the alternative model we use\n        is the `saturated model`, which has a different coefficent for\n        each data point, leading to perfect prediction, a likelihood of\n        one, and a log likelihood of zero.\n\n-   The `null deviance` measures the performance of the worst model\n    using only `an intercept`, providing a benchmark.\n\n$$\n\\text{Null Deviance}= -2 \\text{log}(L(\\hat{\\beta_0}|y_1,..., y_n))\n$$\n\n-   The `residual deviance` is the deviance of our fitted model.\n    -   It is always greater than zero unless it is the saturated model\n        / explains the data perfectly.\n\n$$\n\\text{Residual Deviance}= -2 \\text{log}(L(\\hat{\\beta}|y_1,..., y_n))\n$$\n\n-   Therefore, how much better (smaller) our residual deviance is\n    compared to the null deviance and how close it is to zero is a\n    measure of model fit.\n\n-   Sometimes people will compute an $R^2$ for logistic regression using\n    $1-\\frac{\\text{Residual Deviance}}{\\text{Null Deviance}}$ since it\n    is bounded between 0 (residual deviance = null deviance) and 1\n    (residual deviance = saturated model = 0).\n\n-   Note that we can compute deviance of two separate models by\n    substracting the null model residual deviance and the alternative\n    model residual deviance from separate logistic regression fits. (Why\n    is this?)\n\n-   Using deviance, test whether ($obesity$) is associated with heart\n    disease.\n\n    -   $H_0: \\beta_{obesity} = 0$\n\n    -   $H_a: \\beta_{obesity} \\ne 0$\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n> **We get a p-value of 0.29, the same as what we got from both anova()\n> and Anova() functions, and again we fail to reject the null hypothesis\n> that obesity is not correlated with heart disease given this data\n> set.**\n\n### Confidence Interval\n\n- Confidence Interval for `odds ratio`\n\n**Wald Confidence:**\n\n$$\nc*\\hat{\\beta_k} \\pm c*Z_{1-\\alpha/2} \\sqrt{\\widehat{Var}(\\hat{\\beta}_k)}\n$$\n\n$$\nexp \\left(c*\\hat{\\beta_k} \\pm c*Z_{1-\\alpha/2} \\sqrt{\\widehat{Var}(\\hat{\\beta}_k)} \\right)\n$$\n\n-   Calculate Wald CI for odds ratio of 10-unit increase in LDL\n    cholesterol based on the above formula:\n\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n             (Intercept)           ldl           sbp       tobacco\n(Intercept)  0.610523787 -9.955527e-03 -3.315082e-03  1.122271e-03\nldl         -0.009955527  2.929029e-03 -1.336470e-05 -4.923675e-06\nsbp         -0.003315082 -1.336470e-05  2.909849e-05 -1.506782e-06\ntobacco      0.001122271 -4.923675e-06 -1.506782e-06  6.562050e-04\nage         -0.001828353 -6.178475e-05 -1.503794e-05 -7.699037e-05\n                      age\n(Intercept) -1.828353e-03\nldl         -6.178475e-05\nsbp         -1.503794e-05\ntobacco     -7.699037e-05\nage          9.706566e-05\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  2.20 18.39\n```\n\n\n:::\n:::\n\n\n\n\n\n> **With 95% confidence, the odds of having a heart disease change\n> between 2.20 to 18.4 times for every 10-unit increase in LDL or 'bad'\n> cholesterol.**\n\n-   What is the main concern with Wald CI?\n\n> **Wald confidence interval has a true confidence level close to the\n> 95% only when we have large samples. When the sample size is not\n> large, profile LR confidence intervals generally perform better.**\n\n-   Now calculate the *profile likelihood ratio (LR)* confidence\n    interval using the confint function.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Replace with your code\nbeta_ci <- confint(mod.logit.h0)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWaiting for profiling to be done...\n```\n\n\n:::\n\n```{.r .cell-code}\nodds_ci <- exp(10*beta_ci)\n\nround(cbind(odds_ci ),2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            2.5 % 97.5 %\n(Intercept)  0.00   0.00\nldl          2.24  18.84\nsbp          0.94   1.16\ntobacco      1.31   3.59\nage          1.31   1.93\n```\n\n\n:::\n:::\n\n\n\n\n\n> **Since we have a large sample, 462 observations, the profile\n> likelihood ratio (LR) confidence interval is pretty close to the Wald\n> CI.**\n\n- `Confidence Interval for the Probability of Success`\n\n-   Recall that the estimated probability of success is $$\n    \\hat{\\pi} = \\frac{exp \\left( \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\dots + \\hat{\\beta}_K x_k \\right)}{1+exp \\left(\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\dots + \\hat{\\beta}_K x_k \\right)}\n    $$\n\nWhile backing out the estimated probability of success is\nstraightforward, obtaining its confidence interval is not, as it\ninvolves many parameters.\n\n**Wald Confidence Interval**\n\n$$\n\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\dots + \\hat{\\beta}_K x_K \\pm Z_{1-\\alpha/2} \\sqrt{\\widehat{Var}(\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\dots + \\hat{\\beta}_K x_K)} \n$$\n\nwhere\n\n$$\n\\widehat{Var}(\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\dots + \\hat{\\beta}_K x_K) = \\sum_{i=0}^K x_i^2 \\widehat{Var}(\\hat{\\beta_i}) + 2 \\sum_{i=0}^{K-1} \\sum_{j=i+1}^{K} x_i x_j \\widehat{Cov}(\\hat{\\beta}_i,\\hat{\\beta}_j)\n$$\n\nSo, the Wald Interval for $\\pi$ is\n\n$$\n\\frac{exp \\left( \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\dots + \\hat{\\beta}_K x_k \\pm \\sqrt{\\sum_{i=0}^K x_i^2 \\widehat{Var}(\\hat{\\beta_i}) + 2 \\sum_{i=0}^{K-1} \\sum_{j=i+1}^{K} x_i x_j \\widehat{Cov}(\\hat{\\beta}_i,\\hat{\\beta}_j)}  \\right)}{1+exp \\left(\\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\dots + \\hat{\\beta}_K x_k \\right)  \\pm \\sqrt{\\sum_{i=0}^K x_i^2 \\widehat{Var}(\\hat{\\beta_i}) + 2 \\sum_{i=0}^{K-1} \\sum_{j=i+1}^{K} x_i x_j \\widehat{Cov}(\\hat{\\beta}_i,\\hat{\\beta}_j)}}\n$$\n\n-   For an average value of all explanatory variables, compute the\n    Confidence Interval for the Probability of Success given the formula\n    above\n\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n  pi.hat  lower  upper\n1 0.3089 0.2925 0.3259\n```\n\n\n:::\n:::\n\n\n\n\n\n## Final Visualization\n\n-   Using both the linear probability and logistic regression models,\n    plot the estimated probability of heart disease for different values\n    of cholesterol, holding other variables constant at their average\n    level.\n\n-   Discuss which one can better explain this relationship.\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](wk2_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Final Report\n\n-   Display both estimated linear and logistic models in a regression\n    table. Is there any significant difference between their results?\n\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\nTable 1: The estimated relationship between heart disease and risk factors\n==================================================\n                         Dependent variable:      \n                    ------------------------------\n                                 chd              \n                           OLS          logistic  \n                           (1)             (2)    \n--------------------------------------------------\nldl                      0.036***       0.185***  \n                         (0.010)         (0.054)  \n                                                  \nsbp                       0.001           0.004   \n                         (0.001)         (0.005)  \n                                                  \ntobacco                  0.017***        0.076**  \n                         (0.005)         (0.026)  \n                                                  \nage                      0.008***       0.046***  \n                         (0.002)         (0.010)  \n                                                  \nConstant                 -0.349*        -4.536*** \n                         (0.141)         (0.781)  \n                                                  \n--------------------------------------------------\nObservations               462             462    \nR2                        0.185                   \nAdjusted R2               0.178                   \nLog Likelihood                          -251.093  \nAkaike Inf. Crit.                        512.187  \nResidual Std. Error  0.432 (df = 457)             \n==================================================\nNote:                *p<0.05; **p<0.01; ***p<0.001\n```\n\n\n:::\n:::\n\n\n\n\n\n> **In both models, all the coefficients except blood pressure are\n> statistically significant and positively associated with the\n> probability of having heart disease. Also, LDL is the most correlated\n> variable with the probability of heart disease in both models.**\n\n# Case Study: osteoporosis\n\n## Background\n\n-   In `osteoporosis`, bones become weak and brittle, so weak that even\n    bending over or coughing can fracture them. Hip, wrist, and spine\n    fractures are the most common osteoporosis-related fractures.\n\n-   All races of people are at risk for osteoporosis.\n\n-   However, white and Asian women, particularly those that are post\n    menopause, are at the greatest risk.\n\n-   A healthy diet, weight-bearing exercises, and medications can\n    strengthen weak bones or prevent their loss. (Mayo Clinic)\n\nHere, Our goal is description of the data:\n\n-   **How factors such as age and weight are related to the fracture\n    rates among older women?**\n\n\nThis sample comes from the Global Longitudinal Study of Osteoporosis in\nWomen (GLOW).\n\nThe data set includes information on 500 subjects enrolled in this\nstudy.\n\nInstall and load the aplore3 library to use the glow500 dataset and\nunderstand the structure dataset.\n\nWe summarize some of the variables that we will use:\n\n-   PRIORFRAC: History of prior fracture\n-   AGE: Age at enrollment\n-   WEIGHT: Weight at enrollment (Kilograms)\n-   HEIGHT: Height at enrollment (Centimeters)\n-   BMI: Body mass index ($kg/m^2$)\n-   PREMENO: Menopause before age 45\n-   `FRACTURE`: Any fracture in first year of follow up (`target`)\n-   RATERISK: Self-reported risk of fracture\n-   SMOKE: Former or current smoker\n\n## Task\n\n- Predict the probability of have a `fracture`\n\n## EDA\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- glow500 %>%\n  dplyr::select(fracture, age, priorfrac, premeno, raterisk, smoke, bmi)\n\n\ndf %>% count(fracture) %>%\n  mutate(prop = round(prop.table(n),2)) %>%\n  kable(format = \"html\",col.names = c('Fracture', 'N', \"Proportion\")) %>% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Fracture </th>\n   <th style=\"text-align:right;\"> N </th>\n   <th style=\"text-align:right;\"> Proportion </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> No </td>\n   <td style=\"text-align:right;\"> 375 </td>\n   <td style=\"text-align:right;\"> 0.75 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Yes </td>\n   <td style=\"text-align:right;\"> 125 </td>\n   <td style=\"text-align:right;\"> 0.25 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](wk2_files/figure-html/unnamed-chunk-37-1.png){width=672}\n:::\n:::\n\n\n\n\n\n**Age has a higher age in women with fractures than women without\nfractures. BMI distributions have almost the same mean and same variance\nin both groups with and without fracture, so probably BMI is not a\nuseful variable to classify these two groups**\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](wk2_files/figure-html/unnamed-chunk-38-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](wk2_files/figure-html/unnamed-chunk-39-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](wk2_files/figure-html/unnamed-chunk-40-1.png){width=768}\n:::\n:::\n\n\n\n\n\n\n**From these box plots, we can see the women who suffered from a fracture are older, but both groups have the same distribution of BMI.**\n\n**From the plots above, we see that the women with a history of prior fracture, and a high self-reported risk of fracture, have a higher probability of having a fracture in the first year of study. But, smokers and no smokers and women with or without menopause before 45 have the same probability of having a fracture. so smokers and menopause do not help classify these two groups, and we're not going to use them for modeling**\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Dependent: fracture </th>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:left;\"> No </th>\n   <th style=\"text-align:left;\"> Yes </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> bmi </td>\n   <td style=\"text-align:left;\"> Mean (SD) </td>\n   <td style=\"text-align:left;\"> 27.5 (6.0) </td>\n   <td style=\"text-align:left;\"> 27.7 (5.9) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> age </td>\n   <td style=\"text-align:left;\"> Mean (SD) </td>\n   <td style=\"text-align:left;\"> 67.5 (8.7) </td>\n   <td style=\"text-align:left;\"> 71.8 (9.1) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> priorfrac </td>\n   <td style=\"text-align:left;\"> No </td>\n   <td style=\"text-align:left;\"> 301 (80.3) </td>\n   <td style=\"text-align:left;\"> 73 (58.4) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\"> Yes </td>\n   <td style=\"text-align:left;\"> 74 (19.7) </td>\n   <td style=\"text-align:left;\"> 52 (41.6) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> premeno </td>\n   <td style=\"text-align:left;\"> No </td>\n   <td style=\"text-align:left;\"> 303 (80.8) </td>\n   <td style=\"text-align:left;\"> 100 (80.0) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\"> Yes </td>\n   <td style=\"text-align:left;\"> 72 (19.2) </td>\n   <td style=\"text-align:left;\"> 25 (20.0) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> raterisk </td>\n   <td style=\"text-align:left;\"> Less </td>\n   <td style=\"text-align:left;\"> 139 (37.1) </td>\n   <td style=\"text-align:left;\"> 28 (22.4) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\"> Same </td>\n   <td style=\"text-align:left;\"> 138 (36.8) </td>\n   <td style=\"text-align:left;\"> 48 (38.4) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\"> Greater </td>\n   <td style=\"text-align:left;\"> 98 (26.1) </td>\n   <td style=\"text-align:left;\"> 49 (39.2) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> smoke </td>\n   <td style=\"text-align:left;\"> No </td>\n   <td style=\"text-align:left;\"> 347 (92.5) </td>\n   <td style=\"text-align:left;\"> 118 (94.4) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\"> Yes </td>\n   <td style=\"text-align:left;\"> 28 (7.5) </td>\n   <td style=\"text-align:left;\"> 7 (5.6) </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\n\n## GLM\n\n### simple model \n\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = fracture ~ bmi + age, family = binomial(link = logit), \n    data = df)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-1.21426  -0.77408  -0.62995  -0.07905   2.02854  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -5.83441    1.10792  -5.266 1.39e-07 ***\nbmi          0.02692    0.01817   1.482    0.138    \nage          0.05736    0.01211   4.735 2.20e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 562.34  on 499  degrees of freedom\nResidual deviance: 538.89  on 497  degrees of freedom\nAIC: 544.89\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n:::\n\n\n\n\n\n$$\n\\log\\left[ \\frac { P( \\operatorname{fracture} = \\operatorname{Yes} ) }{ 1 - P( \\operatorname{fracture} = \\operatorname{Yes} ) } \\right] = \\alpha + \\beta_{1}\\cdot\\operatorname{bmi} + \\beta_{2}\\cdot \\operatorname{age}\n$$\n\n- Recall:\n\n$$\nOR = \\frac{Odds_{x_k+c}}{Odds_{x_k}}=exp(c \\beta_k)\n$$\n\n- Find and interpret the estimated odds ratios for a 10-unit increase in age. \n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n    [,1]\nage 1.77\n```\n\n\n:::\n:::\n\n\n\n\n\n**The estimated odds of having a fracture change by 1.77 times for every 10-year increase in age, or it's 77% higher**\n\n### model with categorical feature\n\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"No\"  \"Yes\"\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Less\"    \"Same\"    \"Greater\"\n```\n\n\n:::\n:::\n\n\n\n\n\n$$\n\\log\\left[ \\frac { P( \\operatorname{fracture} = \\operatorname{Yes} ) }{ 1 - P( \\operatorname{fracture} = \\operatorname{Yes} ) } \\right] = \\alpha + \\beta_{1}\\cdot \\operatorname{bmi} + \\beta_{2}\\cdot \\operatorname{age} + \\beta_{3}\\cdot \\operatorname{priorfrac}_{\\operatorname{Yes}} + \\beta_{4}\\cdot \\operatorname{raterisk}_{\\operatorname{Same}} + \\beta_{5}\\cdot \\operatorname{raterisk}_{\\operatorname{Greater}}\n$$",
    "supporting": [
      "wk2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n<link href=\"site_libs/htmltools-fill-0.5.8.1/fill.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\n<script src=\"site_libs/plotly-binding-4.10.4/plotly.js\"></script>\n<script src=\"site_libs/typedarray-0.1/typedarray.min.js\"></script>\n<script src=\"site_libs/jquery-3.5.1/jquery.min.js\"></script>\n<link href=\"site_libs/crosstalk-1.2.1/css/crosstalk.min.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/crosstalk-1.2.1/js/crosstalk.min.js\"></script>\n<link href=\"site_libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/plotly-main-2.11.1/plotly-latest.min.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}