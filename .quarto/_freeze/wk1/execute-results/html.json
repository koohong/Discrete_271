{
  "hash": "0751574f8ce0d7b959678497ddf020f4",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"One and Two binary groups \"\nformat:\n  html:\n    toc: true\n    html-math-method: katex\n    css: style.css  \n    theme:\n      light: cosmo\n      dark: [cosmo, theme-dark.scss]\nexecute: \n   echo: false\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n[Source](https://quarto.org/docs/get-started/hello/rstudio.html)\n\n\n\n\n```{=html}\n<style>\n.table-hover > tbody > tr:hover { \n  background-color: #f4f442;\n}\n</style>\n```\n\n\n\n\n\n\n# Distributions\n\n- Reading `CB` CH1 (skip 1.2.6 and 1.2.7)\n\n## Bernoulli distribution\n\n### parameters\n\n-   Y is 0 or 1. Has one parameter $\\pi$\n-   $E[Y] = \\pi$, $\\text{V}[Y]=\\pi(1-\\pi)$\n\n### PMF\n\n$$P(Y=y) = \\pi^y(1-\\pi)^{1-y}$$ \\\n\n## Binomial distribution\n\n### parameters\n\n-   Multiple bernoulli trials. Suppose we have `n` trials\n-   $E[Y] = n\\pi$, $\\text{V}[Y]= n\\pi(1-\\pi)$\n\n### PMF\n\n$$P(W=w) = {n \\choose w} \\pi^w(1-\\pi)^{1-w}$$\n\n### Likelihood Function\n\n- What is Likelihood function?\n\n$$\\begin{align}\nL(\\pi|y_1,y_2,....y_n) &= P(Y_1 = y_1)\\cdot ... P(Y_n=y_n)\\\\\n&=\\pi^w(1-\\pi)^{n-w}\n\\end{align}$$\n\n### MLE\n\n-   Maximum Likelihood Estimation\n\n-   Suppose your aunt sends you an `unfair coin`, but you forgot what your order was.\n\n    -   To figure out the probability of success, you flip the coin three times and collect the following data (we are defining heads as success here):\n\n> HTH\n\n-   For a hypothesized Bernoulli parameter $\\pi$, what is the likelihood of the data? Your answer should be a function of $\\pi$.\n\n-   likelihood function is:\n\n$$\n\\begin{aligned}\n  L(\\pi|x_1,x_2,x_3) &= P(X_1 = x_1,X_1 = x_2, X_3 =  x_{3}) \\\\\n       &= \\prod_{i=1}^{3} P({X=x_i}) \\\\\n       &= \\prod_{i=1}^{3} \\pi^{x_i}(1-\\pi)^{1-x_i} \\\\\n       &= \\pi^{\\sum_{i=1}^{3} x_i} (1-\\pi)^{\\sum_{i=1}^{3}(1- x_i)} \\\\\n\\end{aligned}\n$$\n\n-   log of the likelihood function\n\n$$\n\\begin{aligned}\n  Log[L(\\pi|x_1,x_2,x_3)] &= \\\\\n       & \\left( {\\sum_{i=1}^{3} x_i} \\right)log(\\pi) + \\left({\\sum_{i=1}^{3}(1- x_i)} \\right) log(1-\\pi)\\\\\n\\end{aligned}\n$$\n\n-   What is the natural log of the likelihood of the data? Write an R function that computes the log likelihood.\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n-   Graph your function and visually estimate what the maximum likelihood estimate for $\\pi$ is.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](wk1_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n\n-   We know that MLE of $\\pi$ is: $$\\hat{\\pi} =\\frac{\\sum x_i}{N} = \\frac{2}{3} $$\n\n-   and in this question, it's:\n\n$$\\hat{\\pi} = \\frac{2}{3} $$\n\n-   In the plot, we can see that log-likelihood has a single peak at 2/3.\n\n# One binary group\n\n## Interval vs Hypothesis test\n\n-   Given observation, knowing distribution, we are estimating the parameter of the function.\n\n-   Since this is an estimator, it will change each time we collect sample.\n\n-   We talked about Wald confidence interval, similar to what we talked about in W203.\n\n    -   It rely on the underlying normal distribution approximation for the maximum likelihood estimator. (see page 11)\n\n-   When only one simple parameter is of interest, such as $\\pi$ here, we generally prefer confidence intervals over hypothesis tests, because the interval gives a range of possible parameter values.\n\n-   We can typically infer that a hypothesized value for a parameter can be rejected if it does not lie within the confidence interval for the parameter.\n\n-   However, there are situations where a `fixed known value` of $\\pi$, say $\\pi_0$ , is of special interest, leading to a formal hypothesis test of\n\n$$H_0 : \\pi = \\pi_0$$ $$H_a : \\pi  \\not= \\pi_0$$\n\n## Interval test\n\n- See page 11 and 12\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](wk1_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Hypothesis test\n\n-   Situations where a `fixed known value` of $\\pi$, say $\\pi_0$ , is of special interest, leading to a formal hypothesis test of\n\n$$H_0 : \\pi = \\pi_0$$ $$H_a : \\pi  \\not= \\pi_0$$\n\n### Hypothesis testing using Wilson interval, score test statistic\n\n![](image/wilson.PNG){fig-align=\"center\" width=\"400\"}\n\nThis statistic is called the `score test statistic`\n\n$$Z_0 = \\frac{\\hat{\\pi}-\\pi_0}{\\sqrt{\\pi_0(1-\\pi_0)/n}}$$ \n\n- When $H_0$ is true, $Z_0$ should have approximately standard normal distribution.\n\n-   The book recommend using the `score test` when performing a test for $\\pi$ (see page 17)\n\n\n\n### Hypothesis testing using likelihood ratio test (LRT)\n\n![](image/LRT.png){fig-align=\"center\" width=\"400\"}\n\n-   LRT statistics look like this.\n\n$$\\Lambda = \\frac{\\text{Maximum of likelihood function udner } H_0}{\\text{Maximum of likelihood function udner } H_A}$$\n\n- LRT is used to calculate confidence intervals in some more complicated contexts where better intervals are not available. \n\n- This interval is better than the wald interval in most problems. (see page 10)\n\n### Hypothesis testing using Transformed LRT\n\n![](image/transformed LRT.webp){fig-align=\"center\" width=\"400\"}\n\n$$-2\\text{log}(\\Lambda)$$\n\n- Above statistic is called `transformed LRT` statistisc and have an approximate $\\chi^2$ distribution.\n\n# Two binary groups\n\n- Consider Bernoulli trial is measured on units that can be classified into groups.\n  - Female and male\n  - Fresh and Salt-Water fish\n  - Larry birds free throw [LINK](https://docs.google.com/spreadsheets/d/1YyO_sX1hIn7siATwF7L-mE8OfHktGX2w/edit?gid=50379254#gid=50379254)\n    - Basketball fans and commentators often speculate that the result of a second free throw might depend on the result of the first.\n  - Salk vaccine clinical trial. [LINK](https://docs.google.com/spreadsheets/d/1YyO_sX1hIn7siATwF7L-mE8OfHktGX2w/edit?gid=50379254#gid=50379254)\n    - Randomized experiment. \n    - 57 out of 200,745 developed polio during the study period\n    - 142 out of 201,220 developed polio during the placebo group.\n    - `Does the vaccine help to prevent polio?`\n\n- We have two random variables $Y_1$ and $Y_2$ and their outcome is independent of each other.\n\n- We have `TWO BINARY VARIABLES`\n\n## Interval test\n\n- Wald Confidence interval using normal approximation.\n- Book recommend using the Agresti-Caffo method\n\n### Larry Birdâ€™s free throw shooting\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc.table <- array(data = c(251, 48, 34, 5), dim = c(2,2),\ndimnames = list(First = c(\"made\", \"missed\"), Second = c(\"made\", \"missed\")))\nc.table\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        Second\nFirst    made missed\n  made    251     34\n  missed   48      5\n```\n\n\n:::\n\n```{.r .cell-code}\n#conditional probabilities\npi.hat.table <- c.table/rowSums(c.table)\n\n#get the pi estimates\npi.hat1 <- pi.hat.table[1,1] \npi.hat2 <- pi.hat.table[2,1]\n\n#set type I error\nalpha <- 0.05\n```\n:::\n\n\n\n\n- `Wald Confidence interval`\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.11218742  0.06227017\n```\n\n\n:::\n:::\n\n\n\n\n- `Agresti-Caffo confidence interval`\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.10353254  0.07781192\n```\n\n\n:::\n:::\n\n\n\n\n- Because these interval contains 0, we cannot reject $H_0$\n\n\n## Hypothesis test\n\n$$H_0: \\pi_1 - \\pi_2 = 0$$\n$$H_0: \\pi_1 - \\pi_2 \\not= 0$$\n\n\n### Person chi-square test \n\n- So, this is the test for TWO BINARY VARIABLES.\n\n- Create a statistic comparing the difference between what was observed and what was predicted under $H_0$ that there is no difference.\n\n- This statistic following $\\Chi^2$ with $n_1$ and $n_2$ degress of freedom.\n\n$$\\chi^2 = \\sum_{j=1}^2\\frac{(w_j-n_j\\bar{\\pi})^2}{n_j\\bar{\\pi}(1-\\bar{\\pi})}$$\n\n### LRT test\n\n$$-2\\text{log}({\\Lambda})= ... $$\n- if $-2\\text{log}({\\Lambda}) > \\chi^2_{..}^{..}$, Reject $H_0$\n\n### Score test\n\n- Use score statistic to perform test.  Score test performs the best when the same size is small.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprop.test(x = c.table , conf.level = 0.95, correct = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\t2-sample test for equality of proportions without continuity correction\n\ndata:  c.table\nX-squared = 0.27274, df = 1, p-value = 0.6015\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.11218742  0.06227017\nsample estimates:\n   prop 1    prop 2 \n0.8807018 0.9056604 \n```\n\n\n:::\n:::\n\n\n\n\n- `correct = FALSE` argument value guarantees that the test statistic is calculated as shown $Z_0$.\n\n-   The `prevalence of a disease` is the proportion of a population that is afflicted with that disease\n\n\n## Relative Risk\n\n- The problem with basing inferences on $\\pi_1 - \\pi_2$ is that it measures a quantity whose meaning changes depending on the value of $\\pi_1,\\pi_2$\n\n1. $\\pi_1 = 0.51 \\text{ and }  \\pi_2 = 0.5$\n2. $\\pi_1 = 0.011 \\text{ and }  \\pi_2 = 0.001$\n\n- In both cases, 1. $\\pi_1 - \\pi_2 = 0.01$. \n- In case (1), this change is small compare to 1. $\\pi_1 \\text{ and }  \\pi_2 $\n- In case (2), this change is 11 times the chance of $\\pi_2 $.  Suppose $\\pi_2$ is the chance of nonsmoking population getting a disease, then, the chance is 11 times the chance of nonsmoking population getting a disease.\n\n- To capture this information, we use another statistic called `relative risk`\n\n$$\\text{RR} = \\frac{\\pi_1}{\\pi_2}=\\frac{0.011}{0.001}$$\n- Smokers are 11 times `as likely` to have the disease than nonsmokers\n- Smokers are 10 times `more likely` to have that disease than nonsmokers. (see page 38)\n\n- $\\hat{RR}$ is MLE and using normal approximation is rather poor for MLE and not recommended.  Use normal approximation on $\\text{log}(\\hat{RR})$\n\n\n### RR of Salk vaccine clinical trial\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc.table <- array(data = c(57, 142, 200688, 201087), dim =\nc(2,2), dimnames = list(Treatment = c(\"vaccine\", \"placebo\"), Result = c(\"polio\", \"polio free\")))\n\nc.table\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         Result\nTreatment polio polio free\n  vaccine    57     200688\n  placebo   142     201087\n```\n\n\n:::\n\n```{.r .cell-code}\n#calculate conditional probability\npi.hat.table <- c.table/rowSums(c.table) \npi.hat.table\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         Result\nTreatment        polio polio free\n  vaccine 0.0002839423  0.9997161\n  placebo 0.0007056637  0.9992943\n```\n\n\n:::\n\n```{.r .cell-code}\n#estimated parameters to be compared\npi.hat1 <- pi.hat.table[1,1] \npi.hat2 <- pi.hat.table[2,1]\n\nRR <- pi.hat1/pi.hat2\nRR\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4023763\n```\n\n\n:::\n:::\n\n\n\n\n- The `estimated` probability of contracting polio is 0.4 times `as likely` for the vaccine group that for teh placebo group.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#set type 1 error\nalpha <- 0.05\nn1 <- sum(c.table[1,]) \nn2 <- sum(c.table[2,])\n\nvar.log.rr <- (1-pi.hat1)/(n1*pi.hat1) + (1-pi.hat2)/(n2*pi.hat2)\n\nci <- exp(log(pi.hat1/pi.hat2) + qnorm(p = c(alpha/2,\n1-alpha/2)) * sqrt(var.log.rr))\n\n\nci\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2959316 0.5471084\n```\n\n\n:::\n:::\n\n\n\n\n# Odds \n\n## Odds\n\n- Odds can also be used as a similar measure as relative risk.\n- Odds are defined as the probability of a success divided by the probability of a failure.\n\n$$\\text{Odds} = \\frac{\\pi}{1-\\pi} = \\frac{0.1}{1-0.1}= \\frac{0.1}{0.9}$$\n- This will be referred to as `9-to-1` odds against.\n\n- Odds have no upper limit unlike probabilities.\n- Like RR, odds are estimated with MLE.\n\n\n## Odds ratios\n\n- Determining whether or not an `odds ratio` is equal to 1, greater than 1 or less than 1 is often of interest. \n\n$$\\text{OR} = \\frac{\\text{Odds}_1}{\\text{Odds}_2}$$\n\n- The estimated odds of a success is $\\hat{\\text{OR}}$ times as larag as `in group 1` than in `group 2`\n\n- Since OR is a statistic, each time you get a sample and estimate, you will get difference value. You can calculate CI of this estimate.\n\n\n\n\n\n\n\n::: {#tbl-penguins-top10 .cell tbl-cap='First 10 Penguins'}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Odd ratio </th>\n   <th style=\"text-align:left;\"> Meanining (page 40) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 1 </td>\n   <td style=\"text-align:left;\"> Odds are not dependent on the group (i.e., the odds of success are independent of the group designation) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> &gt;1 </td>\n   <td style=\"text-align:left;\"> The odds of a success are higher for group 1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 1&lt; </td>\n   <td style=\"text-align:left;\"> The odds of a success are higher for group 2 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\n### OR of Salk vaccine clinical trial\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4022\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4.972972e-06\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.30 0.55\n```\n\n\n:::\n:::\n\n\n\n\n\n# Matched pair data\n\n- $w_1$ and $w_2$ were independent random variables in the examples shown before. [LINK](https://docs.google.com/spreadsheets/d/1YyO_sX1hIn7siATwF7L-mE8OfHktGX2w/edit?gid=50379254#gid=50379254)\n\n- There are other situations where the two probabilities being compared are `dependent` random variables.\n\n- This occurs with `matched pair` data, where two binary response observations, $X$ and $Y$ are made on each `sample unit`\n\n- The desired comparision between success probabilities for X and Y involves two correlated statistics.\n\n## Prostate cancer diagnosis procedures\n\n- (P44) Zhou and Qin (2005) discuss a study that was used to compare the diagnostic accuracy of magnetic resonance imaging (`MRI`) and `ultrasound` in patients who had been established as having `localized prostate cancer` by a gold standard test.\n\n### Hypothesis testing \n\n- This is comparing marginal probabilities\n\n$$H_0: \\pi_{+1}=\\pi_{1+} $$\n$$H_a: \\pi_{+1}\\not= \\pi_{1+} $$\n\n- We can use `wald test statistic` (the book uses $Z_0$ notation which was used for score statistic) or `McNemar's test statistic`, `M` which has approximately $\\chi_^2$ distribution for large samples. \n  - Reject $H_0$ when `M` > $\\chi^2_{..}$\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#get marginal distribution \nn <- sum(c.table) \npi.hat.plus1 <- sum(c.table[,1])/n \npi.hat.1plus <- sum(c.table[1,])/n\n\n#evaluate the difference by \n#subtracting the sample statistics\ndata.frame(pi.hat.plus1 , pi.hat.1plus , \n           diff = pi.hat.plus1 - pi.hat.1plus)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  pi.hat.plus1 pi.hat.1plus       diff\n1 0.0004950569     0.499398 -0.4989029\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nc.table <- array(data = c(4, 3, 6, 3), dim = c(2,2), dimnames =\nlist(MRI = c(\"Localized\", \"Advanced\"), Ultrasound =\nc(\"Localized\", \"Advanced\"))) \n\nc.table\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           Ultrasound\nMRI         Localized Advanced\n  Localized         4        6\n  Advanced          3        3\n```\n\n\n:::\n\n```{.r .cell-code}\nmcnemar.test(x = c.table , correct = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tMcNemar's Chi-squared test\n\ndata:  c.table\nMcNemar's chi-squared = 1, df = 1, p-value = 0.3173\n```\n\n\n:::\n:::\n\n\n\n\n\n### Interval \n\n- `Wald confidence interval`\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\n\ndata:  \n\n95 percent confidence interval:\n -0.5433238  0.1683238\nsample estimates:\n[1] -0.1875\n```\n\n\n:::\n:::\n\n\n\n\n- `Agresti-Min confidence interval`\n  - You also get difference sample esitmate.\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\n\ndata:  \n\n95 percent confidence interval:\n -0.5022786  0.1689453\nsample estimates:\n[1] -0.1666667\n```\n\n\n:::\n:::\n\n\n\n\n\n# Larger contingency tables\n\n- There are many instances where more than two groups exist (i.e., there are more than two rows within a contingency table)\n\n- Once more rows are added to a contingency table, it is often easier to perform the analysis with a binary regression models as covered in Chapter 2. \n\n",
    "supporting": [
      "wk1_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}