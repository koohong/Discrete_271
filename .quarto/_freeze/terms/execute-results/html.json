{
  "hash": "a4a3543c76bdb11ca98e2d1ff4e44a80",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"terms\"\nauthor: \"2024 FALL\"\ndate: \"2025-05-16\"\noutput: html_document\n---\n\n\n\n\n\n<style>\n.table-hover > tbody > tr:hover { \n  background-color: #f4f442;\n}\n</style>\n\n# Terms\n\n\n\n\n\n::: {#tbl-penguins-top10 .cell tbl-cap='First 10 Penguins'}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> wk </th>\n   <th style=\"text-align:left;\"> Term </th>\n   <th style=\"text-align:left;\"> Description </th>\n   <th style=\"text-align:left;\"> $$\\text{equals to}$$ </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> wk1 </td>\n   <td style=\"text-align:left;\"> odd </td>\n   <td style=\"text-align:left;\"> For a Bernoulli random variable with parameter $p$, the odds are defined as the ratio of the probability of success to the probability of failure, </td>\n   <td style=\"text-align:left;\"> $$\\text{Odd}=\\frac{p}{1-p}$$ </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> wk1 </td>\n   <td style=\"text-align:left;\"> logit </td>\n   <td style=\"text-align:left;\"> This is log of odd </td>\n   <td style=\"text-align:left;\"> $$\\text{logit}(p)=\\text{log}\\frac{p}{1-p}$$ </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> wk1 </td>\n   <td style=\"text-align:left;\"> odd </td>\n   <td style=\"text-align:left;\"> from logit, you can get odd.  This expression will be used a lot later. </td>\n   <td style=\"text-align:left;\"> $$\\text{exp}(x) =\\frac{p}{1-p}$$ </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> wk1 </td>\n   <td style=\"text-align:left;\"> probability </td>\n   <td style=\"text-align:left;\"> this is probability of success in binary respose case </td>\n   <td style=\"text-align:left;\"> $$p = \\frac{exp(x)}{1+exp(x)}$$ </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> wk2 </td>\n   <td style=\"text-align:left;\"> saturated model </td>\n   <td style=\"text-align:left;\"> This model is frequently referred as the `saturated model` because the number of parameters is equal to the number of observations, so that no additional parameters can be estimated. It refers to observed proportions of success. See p81 </td>\n   <td style=\"text-align:left;\"> $$\\text{logit}(\\pi_i) = \\gamma_i$$ </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> wk2 </td>\n   <td style=\"text-align:left;\"> likelihood </td>\n   <td style=\"text-align:left;\"> Also known as likelihood function.  It measures how well a statistical model explains observed data by calculating the probability of seeing that data under different parameter values of the model. </td>\n   <td style=\"text-align:left;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> wk2 </td>\n   <td style=\"text-align:left;\"> likelihood ratio test </td>\n   <td style=\"text-align:left;\"> Likelihood ratio test (LRT) is a statistic. P17.  Note that this is $\\Lambda$ </td>\n   <td style=\"text-align:left;\"> $$\\Lambda = \\frac{\\text{MLR under} H_0}{\\text{MLR under} H_0 \\text{ or } H_a}$$ </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> wk2 </td>\n   <td style=\"text-align:left;\"> deviance </td>\n   <td style=\"text-align:left;\"> the amount that a particular model deviates from another model measured by the transformed LRT,$-2log(\\Lambda)$, see p 81 </td>\n   <td style=\"text-align:left;\"> $$-2\\text{log}(\\Lambda)$$ </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> wk2 </td>\n   <td style=\"text-align:left;\"> null deviance </td>\n   <td style=\"text-align:left;\"> The null deviance denotes how much the probabilities estimated from the model logit$(\\pi_i)=\\beta_0$ for all observation.  Measured in transformed LRT, $-2log(\\Lambda)$ </td>\n   <td style=\"text-align:left;\"> The $\\pi_i$ is estaimted to be the same value for this particular model. See p81 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> wk2 </td>\n   <td style=\"text-align:left;\"> residual deviance </td>\n   <td style=\"text-align:left;\"> measures how much probabilities estimated from a model of interest deviatesfrom the `observed proportions of success`. Residual deviance statistics are often calculated as an intermediate step for performing a LRT to compare two models.  Measured in transformed LRT, $-2log(\\Lambda)$ </td>\n   <td style=\"text-align:left;\"> It is a measure of overall goodness of fit for a model.  When you have two different models $H_0$ and $H_a$, you can estimate `the transformed LRT` of $H_0$ by comparing the logit($\\pi^{(0)}$ with `saturated model` and also that of $H_a$.  When you subtract  the transformed  LRT of $H_0$ from that of $H_a$, it measures the probability of success under the $H_0$ and $H_a$. p77 and 81 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> wk2 </td>\n   <td style=\"text-align:left;\"> transformed likelihood ratio test statistic </td>\n   <td style=\"text-align:left;\"> transformed likelihood ratio test (LRT) statistic follows chi-square degress of freedom.  Recommend using it. Better than the Wald Interval </td>\n   <td style=\"text-align:left;\"> $$\n-2log(\\Lambda) = -2log( \\frac{L(\\hat{{\\beta}}^{(0)} | y_1,..., y_n)}{L(\\hat{{\\beta}}^{(a)} | y_1,..., y_n)})\n= -2\\sum y_i log( \\frac{\\hat{\\pi}_i^{(0)}}{\\hat{\\pi}_i^{(a)}}) + (1 - y_i ) log( \\frac{1- \\hat{\\pi}_i^{(0)}}{1- \\hat{\\pi}_i^{(a)}})\n$$ </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> wk2 </td>\n   <td style=\"text-align:left;\"> $\\text{Odds}_x$ </td>\n   <td style=\"text-align:left;\"> The odds of a success at a particular value of x. p83 </td>\n   <td style=\"text-align:left;\"> $$\\text{Odds}_x = exp(\\beta_0 + \\beta_1x) $$ </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> wk2 </td>\n   <td style=\"text-align:left;\"> $\\text{Odds}_{x+c}$ </td>\n   <td style=\"text-align:left;\"> The odds of a success when x is changed by c. p83 </td>\n   <td style=\"text-align:left;\"> $$\\text{Odds}_{x+c} = exp(\\beta_0 + \\beta_1(x+c)) $$ </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> wk2 </td>\n   <td style=\"text-align:left;\"> odd ratio </td>\n   <td style=\"text-align:left;\"> OR is operator. See the bottom of page 82.  It should be E[X].  The odds of a success change by exp($c\\beta_1$ ) times for every c-unit increase in x. </td>\n   <td style=\"text-align:left;\"> $$\\frac{\\text{Odds}_{x+c}}{\\text{Odds}_x}=\\frac{exp(\\beta_0 + \\beta_1(x+c))}{exp(\\beta_0 + \\beta_1x)}=exp(c\\beta_1)$$ </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> wk2 </td>\n   <td style=\"text-align:left;\"> $\\hat{\\text{OR}}$ </td>\n   <td style=\"text-align:left;\"> the estimated odd ratio, and its confidence interval can be found by `confint()` </td>\n   <td style=\"text-align:left;\"> $$\\hat{\\text{OR}} =\\text{exp}(c\\hat{\\beta})$$ </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> wk3 </td>\n   <td style=\"text-align:left;\"> $\\hat{\\pi}$ </td>\n   <td style=\"text-align:left;\"> probability of succuss in logistic regression. P86 </td>\n   <td style=\"text-align:left;\">  </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}