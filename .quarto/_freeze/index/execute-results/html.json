{
  "hash": "6f49bcbe8fc6319e791c5a338606cd7b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Wk1\"\nformat:\n  html:\n    toc: true\n    html-math-method: katex\n    css: style.css  \n    theme:\n      light: cosmo\n      dark: [cosmo, theme-dark.scss]\nexecute: \n   echo: false\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n[Source](https://quarto.org/docs/get-started/hello/rstudio.html)\n\n\n\n\n```{=html}\n<style>\n.table-hover > tbody > tr:hover { \n  background-color: #f4f442;\n}\n</style>\n```\n\n\n\n\n\n\n# Log odds to probability \n\n- Your aunt offers a service in which she weights coins to make them unfair. \n\n- You give her a coin and tell her how much you want the `log-odds` to change. She returns the modified coin.\n\n- For each of the following orders, use your function to compute the resulting probability of heads:\n\n  - fair coin, increase log-odds by 1.\n  - fair coin, increase log-odds by 2.\n  - fair coin, increase log-odds by 10.\n  - fair coin, decrease log-odds by 1.\n  - fair coin, decrease log-odds by 2.\n  - fair coin, decrease log-odds by 10.\n  \n\n- Write an R function that computes the `probability of heads`, given log-odds.\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5\n```\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> log_odds </th>\n   <th style=\"text-align:right;\"> probability </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 10 </td>\n   <td style=\"text-align:right;\"> 1.000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.881 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0.731 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.500 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> -1 </td>\n   <td style=\"text-align:right;\"> 0.269 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> -2 </td>\n   <td style=\"text-align:right;\"> 0.119 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> -10 </td>\n   <td style=\"text-align:right;\"> 0.000 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\n- In you own words, describe how changes in log-odds translate to changes in probability\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\n- You can see in this plot, As log-odds increase, the probability of success increases relative to the probability of failure, and it approaches one. As log-odds decrease probability of success decrease and converges to zero.\n\n        \n- If you get log-odds values that are very very small like -10 the probability of success is almost zero, and if you get log-odds values that are very big like 10 or the probability of success is almost one.\n\n- The relationship between log-odd and probability is not linear, but of s-curve type, and  log odds ratios ranging from -5 to +5 create probabilities that range from just above 0 to very close to 1. \n\n# MLE\n\n- Maximum Likelihood Estimation\n\n- Suppose your aunt sends you an `unfair coin`, but you forgot what your order was. \n  - To figure out the probability of success, you flip the coin three times and collect the following data (we are defining heads as success here):\n\n> HTH\n\n- For a hypothesized Bernoulli parameter $\\pi$, what is the likelihood of the data? Your answer should be a function of $\\pi$.\n\n- likelihood function is:\n\n$$\n\\begin{aligned}\n  L(\\pi|x_1,x_2,x_3) &= P(X_1 = x_1,X_1 = x_2, X_3 =  x_{3}) \\\\\n       &= \\prod_{i=1}^{3} P({X=x_i}) \\\\\n       &= \\prod_{i=1}^{3} \\pi^{x_i}(1-\\pi)^{1-x_i} \\\\\n       &= \\pi^{\\sum_{i=1}^{3} x_i} (1-\\pi)^{\\sum_{i=1}^{3}(1- x_i)} \\\\\n\\end{aligned}\n$$\n\n- log of the likelihood function\n\n$$\n\\begin{aligned}\n  Log[L(\\pi|x_1,x_2,x_3)] &= \\\\\n       & \\left( {\\sum_{i=1}^{3} x_i} \\right)log(\\pi) + \\left({\\sum_{i=1}^{3}(1- x_i)} \\right) log(1-\\pi)\\\\\n\\end{aligned}\n$$\n\n- What is the natural log of the likelihood of the data? Write an R function that computes the log likelihood.\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n- Graph your function and visually estimate what the maximum likelihood estimate for $\\pi$ is.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n- We know that MLE of $\\pi$ is:\n$$\\hat{\\pi} =\\frac{\\sum x_i}{N} = \\frac{2}{3} $$\n\n- and in this question, it's:\n\n$$\\hat{\\pi} = \\frac{2}{3} $$\n\n- In the plot, we can see that log-likelihood has a single peak at 2/3.\n\n\n\n# Terms\n\n\n\n\n\n\n::: {#tbl-penguins-top10 .cell tbl-cap='First 10 Penguins'}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> wk </th>\n   <th style=\"text-align:left;\"> Term </th>\n   <th style=\"text-align:left;\"> Notation </th>\n   <th style=\"text-align:left;\"> Description </th>\n   <th style=\"text-align:left;\"> note </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> wk1 </td>\n   <td style=\"text-align:left;\"> odds </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\"> For a Bernoulli random variable with parameter $p$, the odds are defined as the ratio of the probability of success to the probability of failure, $\\frac{p}{1-p}$ </td>\n   <td style=\"text-align:left;\"> $p$ probability of success </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> wk1 </td>\n   <td style=\"text-align:left;\"> logit(p) </td>\n   <td style=\"text-align:left;\"> x </td>\n   <td style=\"text-align:left;\"> log-odds, $\\text{log}\\frac{p}{1-p}$ </td>\n   <td style=\"text-align:left;\"> $p$ probability of success </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> wk1 </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\"> $\\text{exp}(x)$ </td>\n   <td style=\"text-align:left;\"> $\\frac{p}{1-p}$ </td>\n   <td style=\"text-align:left;\"> $p$ probability of success </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> wk1 </td>\n   <td style=\"text-align:left;\"> probability </td>\n   <td style=\"text-align:left;\"> p </td>\n   <td style=\"text-align:left;\"> $\\frac{exp(x)}{1+exp(x)}$ </td>\n   <td style=\"text-align:left;\"> $p$ probability of success </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> wk2 </td>\n   <td style=\"text-align:left;\"> observed proportions of success </td>\n   <td style=\"text-align:left;\"> logit$(\\pi_i) = \\gamma_i$ </td>\n   <td style=\"text-align:left;\"> This model is frequently referred as the `saturated model` because the number of parameters is equal to the number of observations, so that no additional parameters can be estimated. </td>\n   <td style=\"text-align:left;\"> see page 81 of CB </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> wk2 </td>\n   <td style=\"text-align:left;\"> likelihood function, aka `likelihood` </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\"> measures how well a statistical model explains observed data by calculating the probability of seeing that data under different parameter values of the model. </td>\n   <td style=\"text-align:left;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> wk2 </td>\n   <td style=\"text-align:left;\"> deviance </td>\n   <td style=\"text-align:left;\"> dev </td>\n   <td style=\"text-align:left;\"> the amount that a particular model deviates from another model measured by the transformed LRT, $-2\\text{log}(\\lambda)$, see page 81 </td>\n   <td style=\"text-align:left;\"> see page 81 of CB </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> wk2 </td>\n   <td style=\"text-align:left;\"> null deviance </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\"> The null deviance denotes how much the probabilities estimated from the model logit$(\\pi_i)=\\beta_0$ for all observation.  The $\\pi_i$ is estaimted to be the same value for this particular model. </td>\n   <td style=\"text-align:left;\"> page 81 of CB </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> wk2 </td>\n   <td style=\"text-align:left;\"> likelihood ratio test (LRT) is a statistics </td>\n   <td style=\"text-align:left;\"> $\\Lambda$ </td>\n   <td style=\"text-align:left;\"> $\\frac{\\text{MLR under} H_0}{\\text{MLR under} H_0 \\text{ or } H_a}$ </td>\n   <td style=\"text-align:left;\"> see page 17, CB </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> wk2 </td>\n   <td style=\"text-align:left;\"> residual deviance </td>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\"> measures how much probabilities estimated from a model of interest deviatesfrom the `observed proportions of success`. It is a measure of overall goodness of fit for a model.  When you have two different models $H_0$ and $H_a$, you can estimate `the transformed LRT` of $H_0$ by comparing the logit($\\pi^{(0)}$ with `saturated model` and also that of $H_a$.  When you subtract  the transformed  LRT of $H_0$ from that of $H_a$, it measures the probability of success under the $H_0$ and $H_a$ </td>\n   <td style=\"text-align:left;\"> see page 77 and  81 of CB </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> wk2 </td>\n   <td style=\"text-align:left;\"> the transformed likelihood ratio test (LRT) statistic </td>\n   <td style=\"text-align:left;\"> $-2\\text{log}(\\lambda)$ </td>\n   <td style=\"text-align:left;\"> $$\n-2log(\\Lambda) = -2log( \\frac{L(\\hat{{\\beta}}^{(0)} | y_1,..., y_n)}{L(\\hat{{\\beta}}^{(a)} | y_1,..., y_n)})\n= -2\\sum y_i log( \\frac{\\hat{\\pi}_i^{(0)}}{\\hat{\\pi}_i^{(a)}}) + (1 - y_i ) log( \\frac{1- \\hat{\\pi}_i^{(0)}}{1- \\hat{\\pi}_i^{(a)}})\n$$ </td>\n   <td style=\"text-align:left;\"> This follows $\\chi_1^2$ distribution. Recommend using it. Better than the Wald Interval </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}